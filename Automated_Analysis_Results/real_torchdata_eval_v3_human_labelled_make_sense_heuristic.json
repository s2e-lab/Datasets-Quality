[{"nl": {"id": "TorchDataEval/0", "comment": "# Please use the following APIs to solve the task:\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/0", "comment": "# cycle(*args, **kwds): Cycles the specified input in perpetuity by default, or for the specified number of times.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/0", "comment": "# How to augument the datapipe by repeating it six times.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/1", "comment": "# Please use the following APIs to solve the task:\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/1", "comment": "# enumerate(*args, **kwds): Adds an index to an existing DataPipe through enumeration, with the index starting from 0 by default.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/1", "comment": "# Assign indexs to the datepipe object.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/2", "comment": "# How to get one training data from the batch_dp\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/4", "comment": "# Please use the following APIs to solve the task:\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/4", "comment": "# demux(datapipe: torch.utils.data.dataset.IterDataPipe, num_instances: int, classifier_fn: Callable[[+T_co], Union[int, NoneType]], drop_none: bool = False, buffer_size: int = 1000): Splits the input DataPipe into multiple child DataPipes, using the given classification function.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/4", "comment": "# Split into 2 sub-datapipes by the odd_or_even function\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/5", "comment": "# Please use the following APIs to solve the task:\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/5", "comment": "# fork(datapipe: torch.utils.data.dataset.IterDataPipe, num_instances: int, buffer_size: int = 1000): Creates multiple instances of the same Iterable DataPipe.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/5", "comment": "# Clone the source datapipe two times\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/6", "comment": "# Please use the following APIs to solve the task:\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/6", "comment": "# zip_with_iter(source_datapipe: IterDataPipe, ref_datapipe: IterDataPipe, key_fn: Callable, ref_key_fn: Optional[Callable] = None, keep_key: bool = False, buffer_size: int = 10000, merge_fn: Optional[Callable] = None): Zips two IterDataPipes together based on the matching key.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/6", "comment": "# Putting two IterDataPipes together based on their key.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/7", "comment": "# Please use the following APIs to solve the task:\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/7", "comment": "# zip_with_map(source_iterdatapipe: IterDataPipe, map_datapipe: MapDataPipe, key_fn: Callable, merge_fn: Optional[Callable] = None): Joins the items from the source IterDataPipe with items from a MapDataPipe.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/7", "comment": "# Attach the elements in the source IterDataPipe to the elements in the MapDataPipe.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/9", "comment": "# Please use the following APIs to solve the task:\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/9", "comment": "# SampleMultiplexer(*args, **kwds): Takes a `Dict` of (IterDataPipe, Weight), and yields items by sampling from these DataPipes with respect to their weights.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/9", "comment": "# Take samples from these DataPipes based on their weights with random seed 0, until all of them are exhausted.\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/10", "comment": "# Please use the following APIs to solve the task:\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/10", "comment": "# unzip(source_datapipe: torch.utils.data.dataset.IterDataPipe[typing.Sequence[~T]], sequence_length: int, buffer_size: int = 1000, columns_to_skip: Union[Sequence[int], NoneType] = None): Takes in a DataPipe of Sequences, unpacks each Sequence, and return the elements in separate DataPipes based on their position in the Sequence.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/10", "comment": "# Unzip the three tuples, and return these elements in separate DataPipes, depending on their location.\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/11", "comment": "# Please use the following APIs to solve the task:\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/11", "comment": "# batch(datapipe: IterDataPipe, batch_size: int, drop_last: bool = False, wrapper_class=List): Creates mini-batches of data.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/11", "comment": "# bucketbatch(datapipe: torch.utils.data.dataset.IterDataPipe[+T_co], batch_size: int, drop_last: bool = False, batch_num: int = 100, bucket_num: int = 1, sort_key: Union[Callable, NoneType] = None, in_batch_shuffle: bool = True): Creates mini-batches of data from sorted bucket.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/11", "comment": "# Divide datapipes into 3 batches and discard if the last batch is not reached.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/12", "comment": "# Please use the following APIs to solve the task:\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/12", "comment": "# bucketbatch(datapipe: torch.utils.data.dataset.IterDataPipe[+T_co], batch_size: int, drop_last: bool = False, batch_num: int = 100, bucket_num: int = 1, sort_key: Union[Callable, NoneType] = None, in_batch_shuffle: bool = True): Creates mini-batches of data from sorted bucket.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/12", "comment": "# Create batch datapipe with batch size 3, batch num is 100, and drop the last batch if it is not full.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/12", "comment": "# Also, useing the sort_bucket function to sort the bucket, where the bucket_num is 1.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/14", "comment": "# Please use the following APIs to solve the task:\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/14", "comment": "# groupby(datapipe: IterDataPipe[torch.utils.data.datapipes.iter.grouping.T_co], group_key_fn: Callable, *, buffer_size: int = 10000, group_size: Optional[int] = None, guaranteed_group_size: Optional[int] = None, drop_remaining: bool = False): Groups data from input IterDataPipe by keys which are generated from ``group_key_fn``, and yields a ``DataChunk`` with batch size up to ``group_size`` if defined.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/14", "comment": "# Group by file name (except extension), we set the buffer size and group size to 3, and the guaranteed group size to 2.\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/16", "comment": "# Please use the following APIs to solve the task:\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/16", "comment": "# HttpReader(source_datapipe: IterDataPipe[str], timeout: Optional[float] = None): Takes file URLs (HTTP URLs pointing to files), and yields tuples of file URL and IO stream.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/16", "comment": "# Using IterableWrapper to the file url and HttpReader to read the file\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/17", "comment": "# Please use the following APIs to solve the task:\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/17", "comment": "# flatmap(*args, **kwds): Applies a function over each item from the source DataPipe, then flattens the outputs to a single, unnested IterDataPipe.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/17", "comment": "# Each item in the source_dp is applied mutiple_fn function and the output is then tiled to a single, unnested one.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/18", "comment": "# Please use the following APIs to solve the task:\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/18", "comment": "# map(datapipe: IterDataPipe, fn: Callable, input_col=None, output_col=None): Applies a function over each item from the source DataPipe.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/18", "comment": "# Method 1\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/18", "comment": "# Invocation via functional form is preferred\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/18", "comment": "# Method 2\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/18", "comment": "# We discourage the usage of `lambda` functions as they are not serializable with `pickle`\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/18", "comment": "# Using `lambda` to implement add_two rather than add_one that is mentioned in above.\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/19", "comment": "# Please use the following APIs to solve the task:\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/19", "comment": "# filter(datapipe: IterDataPipe, filter_fn: Callable, drop_empty_batches: bool = True): Filters out elements from the source datapipe according to input ``filter_fn``.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/19", "comment": "# Filtering by the above function\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/20", "comment": "# Please use the following APIs to solve the task:\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/20", "comment": "# header(source_datapipe: IterDataPipe[torchdata.datapipes.iter.util.header.T_co], limit: int = 10): Yields elements from the source DataPipe from the start, up to the specfied limit.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/20", "comment": "# How to get the first three elements of a datapipe?\n"}, "Heuristic": ["H1", "H7", "H12"]}, {"nl": {"id": "TorchDataEval/21", "comment": "# Each element in a batch is a `Dict`\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/21", "comment": "# Takes an input DataPipe with batches of data, processes the batches one and produces a Dict for each batch.\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/21", "comment": "# We only need the column 'a' from each batch.\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/23", "comment": "# Please use the following APIs to solve the task:\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/23", "comment": "# batch(datapipe: MapDataPipe[T], batch_size: int, drop_last: bool = False, wrapper_class=DataChunk): Create mini-batches of data.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/23", "comment": "# Using functional form (recommended)\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/23", "comment": "# Using class constructor\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/23", "comment": "# Get the mapper datapipe (map_dp_1) batch datas with the batch size of 2.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/24", "comment": "# Please use the following APIs to solve the task:\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/24", "comment": "# HttpReader(source_datapipe: IterDataPipe[str], timeout: Optional[float] = None): Takes file URLs (HTTP URLs pointing to files), and yields tuples of file URL and IO stream.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/24", "comment": "# Read the URL using the HTTP protocol and process the csv file.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/25", "comment": "# Please use the following APIs to solve the task:\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/25", "comment": "# map(datapipe: IterDataPipe, fn: Callable, input_col=None, output_col=None): Applies a function over each item from the source DataPipe.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/25", "comment": "# HttpReader(source_datapipe: IterDataPipe[str], timeout: Optional[float] = None): Takes file URLs (HTTP URLs pointing to files), and yields tuples of file URL and IO stream.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/25", "comment": "# Read the URL using the HTTP protocol and process the csv file.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/25", "comment": "# Then, we map the datapipe using lambda_func_ to get what we want.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/26", "comment": "# Read the URL using the HTTP protocol and process the csv file.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/26", "comment": "# Then, we map the datapipe using lambda_func_ to get what we want.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/26", "comment": "# How to get all batches from a datapipe with batch size 2?\n"}, "Heuristic": ["H1", "H7", "H12"]}, {"nl": {"id": "TorchDataEval/26", "comment": "# Furthermore, the batches should be mapped using lambda_batch.\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/27", "comment": "# Augument the datapipe with repeat three times and sample the data.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/28", "comment": "# Please use the following APIs to solve the task:\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/28", "comment": "# concat(*args, **kwds): Concatenates multiple Iterable DataPipes.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/28", "comment": "# First we concatenate two datapipes and then repeat the concatenated datapipe three times.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/29", "comment": "# According to the merge_fn, we zip the above two datapipes and keep the key True.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/29", "comment": "# Whatsmore, cycle the zipped datapipe three times.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/30", "comment": "# Please use the following APIs to solve the task:\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/30", "comment": "# zip_with_iter(source_datapipe: IterDataPipe, ref_datapipe: IterDataPipe, key_fn: Callable, ref_key_fn: Optional[Callable] = None, keep_key: bool = False, buffer_size: int = 10000, merge_fn: Optional[Callable] = None): Zips two IterDataPipes together based on the matching key.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/30", "comment": "# We zipp the above two data pipes and set keep_key to True according to merge_fn.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/30", "comment": "# Also, enumerating the zipped datapipe.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/31", "comment": "# Please use the following APIs to solve the task:\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/31", "comment": "# zip_with_iter(source_datapipe: IterDataPipe, ref_datapipe: IterDataPipe, key_fn: Callable, ref_key_fn: Optional[Callable] = None, keep_key: bool = False, buffer_size: int = 10000, merge_fn: Optional[Callable] = None): Zips two IterDataPipes together based on the matching key.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/31", "comment": "# Zipping the above two data pipes and set keep_key to True according to merge_fn. \n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/31", "comment": "# Moreover, transform its type to List and get the first element.\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/32", "comment": "# Please use the following APIs to solve the task:\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/32", "comment": "# zip_with_map(source_iterdatapipe: IterDataPipe, map_datapipe: MapDataPipe, key_fn: Callable, merge_fn: Optional[Callable] = None): Joins the items from the source IterDataPipe with items from a MapDataPipe.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/32", "comment": "# Using merge_fn to zip the two data pipes.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/32", "comment": "# Repeating three times to argument the zipped data pipe.\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/33", "comment": "# Using merge_fn to zip the two data pipes, and repeating three times to argument the zipped data pipe.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/33", "comment": "# Finally, we convert the result type to a list and take the second element of each tuple.\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/34", "comment": "# Using merge_fn to zip the two data pipes, and repeating three times to argument the zipped data pipe, and then sampling the result.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/34", "comment": "# Finally, we convert the result type to a list and take the third element of each tuple.\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/35", "comment": "# Please use the following APIs to solve the task:\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/35", "comment": "# groupby(datapipe: IterDataPipe[torch.utils.data.datapipes.iter.grouping.T_co], group_key_fn: Callable, *, buffer_size: int = 10000, group_size: Optional[int] = None, guaranteed_group_size: Optional[int] = None, drop_remaining: bool = False): Groups data from input IterDataPipe by keys which are generated from ``group_key_fn``, and yields a ``DataChunk`` with batch size up to ``group_size`` if defined.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/35", "comment": "# Group the files by their file name using the group_fn function.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/35", "comment": "# Then, reserving the length of group result greater than 1.\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/37", "comment": "# Please use the following APIs to solve the task:\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/37", "comment": "# collate(datapipe: IterDataPipe, collate_fn: Callable = <function default_collate>): Collates samples from DataPipe to Tensor(s) by a custom collate function.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/37", "comment": "# map(datapipe: IterDataPipe, fn: Callable, input_col=None, output_col=None): Applies a function over each item from the source DataPipe.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/37", "comment": "# First get the head 2 elements\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/37", "comment": "# Second make the datapipe tensor-like by using `collate_fn`\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/38", "comment": "# Please use the following APIs to solve the task:\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/38", "comment": "# collate(datapipe: IterDataPipe, collate_fn: Callable = <function default_collate>): Collates samples from DataPipe to Tensor(s) by a custom collate function.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/38", "comment": "# filter(datapipe: IterDataPipe, filter_fn: Callable, drop_empty_batches: bool = True): Filters out elements from the source datapipe according to input ``filter_fn``.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/38", "comment": "# Filter the value smaller than 5\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/38", "comment": "# Second make the datapipe tensor-like by using `collate_fn`\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/40", "comment": "# Please use the following APIs to solve the task:\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/40", "comment": "# demux(datapipe: torch.utils.data.dataset.IterDataPipe, num_instances: int, classifier_fn: Callable[[+T_co], Union[int, NoneType]], drop_none: bool = False, buffer_size: int = 1000): Splits the input DataPipe into multiple child DataPipes, using the given classification function.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/40", "comment": "# Split the source datapipe into two datapipes by applying the function `great_than_5`\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/41", "comment": "# Given the weight, how to sample from two datapipes?\n"}, "Heuristic": ["H1", "H7", "H12"]}, {"nl": {"id": "TorchDataEval/41", "comment": "# Note that the sample seed is set to 1 for reproducibility\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/42", "comment": "# Please use the following APIs to solve the task:\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/42", "comment": "# map(datapipe: IterDataPipe, fn: Callable, input_col=None, output_col=None): Applies a function over each item from the source DataPipe.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/42", "comment": "# unzip(source_datapipe: torch.utils.data.dataset.IterDataPipe[typing.Sequence[~T]], sequence_length: int, buffer_size: int = 1000, columns_to_skip: Union[Sequence[int], NoneType] = None): Takes in a DataPipe of Sequences, unpacks each Sequence, and return the elements in separate DataPipes based on their position in the Sequence.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/42", "comment": "# I would like assgin dp1 to be a datapipe that contains the first column of raw_dp\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/42", "comment": "# and dp2 to be a datapipe that contains the second column of raw_dp\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/42", "comment": "# and dp3 to be a datapipe that contains the third column of raw_dp\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/42", "comment": "# How to do this?\n"}, "Heuristic": ["H7", "H12"]}, {"nl": {"id": "TorchDataEval/43", "comment": "# Please use the following APIs to solve the task:\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/43", "comment": "# batch(datapipe: IterDataPipe, batch_size: int, drop_last: bool = False, wrapper_class=List): Creates mini-batches of data.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/43", "comment": "# Make a batch operation on the datapipe `dp` of size 3 with droping last batch if it is not full.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/43", "comment": "# And then get the first two batches.\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/44", "comment": "# Batch on data pipe `dp1` of size 4 and discard the last batch if they are not filled, and then obtain the first two batches.\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/44", "comment": "# Then the above result is concatenated with the datapipe `dp2`.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/45", "comment": "# Please use the following APIs to solve the task:\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/45", "comment": "# concat(*args, **kwds): Concatenates multiple Iterable DataPipes.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/45", "comment": "# add_index(*args, **kwds): Adds an index to an existing Iterable DataPipe with.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/45", "comment": "# Concatenate two datapipes and add corresponding indices with the name `Ids`.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/46", "comment": "# Join the two data pipes and add an index with the name `Ids`.\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/46", "comment": "# Then create three copies of the datapipe.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/47", "comment": "# Join the three data pipes and obtain the enumerated datapipe.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/48", "comment": "# Please use the following APIs to solve the task:\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/48", "comment": "# flatmap(*args, **kwds): Applies a function over each item from the source DataPipe, then flattens the outputs to a single, unnested IterDataPipe.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/48", "comment": "# I want to augment the source datapipe with the above function, which will return nine elements.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/48", "comment": "# Then we flatten the nine elements into a single datapipe.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/49", "comment": "# Please use the following APIs to solve the task:\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/49", "comment": "# HttpReader(source_datapipe: IterDataPipe[str], timeout: Optional[float] = None): Takes file URLs (HTTP URLs pointing to files), and yields tuples of file URL and IO stream.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/49", "comment": "# Read the URL using the HTTP protocol and parse the csv file as a dictionary.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/3", "comment": "# Please use the following APIs to solve the task:\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/3", "comment": "# concat(*args, **kwds): Concatenate multiple Map DataPipes.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/3", "comment": "# concat two datapipes\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/8", "comment": "# Please use the following APIs to solve the task:\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/8", "comment": "# mux(*datapipes): Yields one element at a time from each of the input Iterable DataPipes.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/8", "comment": "# One element is generated from each input Iterable DataPipes in turn, until the end when the shortest input DataPipe is used up.\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/13", "comment": "# convert integer to float Tensor using `int2tensor`.\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/15", "comment": "# Please use the following APIs to solve the task:\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/15", "comment": "# unbatch(datapipe: IterDataPipe, unbatch_level: int = 1): Undoes batching of data.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/15", "comment": "# Does the unbatch processing of data, the level is setted by default to 1.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/22", "comment": "# generating bytes where the chunk is set to one.\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/39", "comment": "# Put the above DataPipes into one list obj, and remove the last number from each element (e.g., \"1\" in \"dog1\")\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/36", "comment": "# Please use the following APIs to solve the task:\n"}, "Heuristic": ["H7"]}, {"nl": {"id": "TorchDataEval/36", "comment": "# groupby(datapipe: IterDataPipe[torch.utils.data.datapipes.iter.grouping.T_co], group_key_fn: Callable, *, buffer_size: int = 10000, group_size: Optional[int] = None, guaranteed_group_size: Optional[int] = None, drop_remaining: bool = False): Groups data from input IterDataPipe by keys which are generated from ``group_key_fn``, and yields a ``DataChunk`` with batch size up to ``group_size`` if defined.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/36", "comment": "# header(source_datapipe: IterDataPipe[torchdata.datapipes.iter.util.header.T_co], limit: int = 10): Yields elements from the source DataPipe from the start, up to the specfied limit.\n"}, "Heuristic": ["H1", "H7"]}, {"nl": {"id": "TorchDataEval/36", "comment": "# group by source_dp using the ``group_fn`` function and obtain the header groups by default, assign the result to the new variable ``header_groups``.\n"}, "Heuristic": ["H1", "H7"]}]