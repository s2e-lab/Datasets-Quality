{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6777"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"datasets_study.json\", \"r\") as f:\n",
    "    data = json.load(f) \n",
    "\n",
    "analysis_data = []\n",
    "\n",
    "\n",
    "for item in data:\n",
    "    if item['model']=='datasets_study.analysis':\n",
    "        analysis_data.append(item)\n",
    "\n",
    "len(analysis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'Spelling and grammatical errors', 2: 'Very short sentence.', 3: 'Partial or incomplete sentence.', 4: 'Self-Admitted Technical Debt (ex: TODO / FIXME / hack / XXX, etc)', 5: 'Automatically generated code/comments (ex:: // TODO auto-generated method stub).', 6: \"Method name mismatches with the function/method's intent.\", 7: 'Not using standard JavaDoc (Java) or docstring (Python)', 8: 'If one sample contains an example of I/O, are all the samples containing the same number of examples\\r\\nDifferent JavaDoc/Docstring style.\\r\\nGiving type hint in some samples in Python, not for all.', 9: 'URL or reference in the comment.', 12: 'Check whether the question mark is indeed asking a question about what the function does instead of being just part of the description of the code.', 13: 'Check the JavaDoc quality.', 14: 'Comment includes PII(author name, email address, etc.)', 15: 'The prompt description is unclear.', 16: 'Incorrect input/output pair example.'}\n"
     ]
    }
   ],
   "source": [
    "problem_list = {}\n",
    "for item in data:\n",
    "    if item['model']=='datasets_study.problem':\n",
    "       problem_list[item['pk']] = item['fields']['description']\n",
    "print(problem_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3395\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "users = defaultdict(list)\n",
    "analysis = defaultdict(list)\n",
    "problems = defaultdict(int)\n",
    "\n",
    "\n",
    "for item in analysis_data:\n",
    "    if item['fields']['user_id'] == 1:\n",
    "        continue\n",
    "    \n",
    "    users[item['fields']['user_id']].append(item['fields']['prompt'])\n",
    "    analysis[item['fields']['prompt']].append(item['fields'])\n",
    "    for problem in item['fields']['problems']:\n",
    "        problems[problem] += 1\n",
    "print(len(users))\n",
    "print(len(analysis))\n",
    "print(len(problems))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 797), (2, 1), (3, 206), (4, 1), (6, 2636), (7, 872), (8, 598), (9, 36), (12, 292), (13, 2023), (15, 200), (16, 42)]\n"
     ]
    }
   ],
   "source": [
    "problems = sorted(problems.items(), key=lambda x: x[0])\n",
    "print(problems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Spelling and grammatical errors: 797\n",
      "2. Very short sentence.: 1\n",
      "3. Partial or incomplete sentence.: 206\n",
      "4. Self-Admitted Technical Debt (ex: TODO / FIXME / hack / XXX, etc): 1\n",
      "6. Method name mismatches with the function/method's intent.: 2636\n",
      "7. Not using standard JavaDoc (Java) or docstring (Python): 872\n",
      "8. If one sample contains an example of I/O, are all the samples containing the same number of examples\n",
      "Different JavaDoc/Docstring style.\n",
      "Giving type hint in some samples in Python, not for all.: 598\n",
      "9. URL or reference in the comment.: 36\n",
      "12. Check whether the question mark is indeed asking a question about what the function does instead of being just part of the description of the code.: 292\n",
      "13. Check the JavaDoc quality.: 2023\n",
      "15. The prompt description is unclear.: 200\n",
      "16. Incorrect input/output pair example.: 42\n"
     ]
    }
   ],
   "source": [
    "for item in problems:\n",
    "    key = item[0]\n",
    "    total = item[1]\n",
    "    print(f\"{key}. {problem_list[key]}: {total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3394, 3382)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(users[3])), len(set(users[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra = list(set(users[3])-set(users[4]))\n",
    "extra.extend(set(users[4])-set(users[3]))\n",
    "\n",
    "len(extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"extra.txt\", \"w\") as f:\n",
    "    for id in extra:\n",
    "        if id in users[3]:\n",
    "            f.write(\"https://jdasilv2.pythonanywhere.com/datasets_study/prompts/{},Dristi\\n\".format(id))\n",
    "        else:\n",
    "            f.write(\"https://jdasilv2.pythonanywhere.com/datasets_study/prompts/{},Joy\\n\".format(id))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "731\n"
     ]
    }
   ],
   "source": [
    "mismatch = 0\n",
    "for key in analysis.keys():\n",
    "    current_problems = analysis[key][0]['problems']\n",
    "    for item in analysis[key]:\n",
    "        if item['problems'] != current_problems:\n",
    "            mismatch += 1\n",
    "            break\n",
    "print(mismatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
