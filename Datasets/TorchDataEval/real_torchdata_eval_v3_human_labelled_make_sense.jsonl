{"task_id": "TorchDataEval/0", "prompt": "# Please use the following APIs to solve the task:\n# cycle(*args, **kwds): Cycles the specified input in perpetuity by default, or for the specified number of times.\nfrom torchdata.datapipes.iter import IterableWrapper\ndatapipe = IterableWrapper([1,2,3])\n# How to augument the datapipe by repeating it six times.\nnew_datapipe =", "entry_point": "none", "canonical_solution": [" Cycler(datapipe, 6)", " datapipe.cycle(6)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'Cycler'\n}\n\n\ndef check():\n    assert list(new_datapipe) == [1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3]\n\n"}
{"task_id": "TorchDataEval/1", "prompt": "# Please use the following APIs to solve the task:\n# enumerate(*args, **kwds): Adds an index to an existing DataPipe through enumeration, with the index starting from 0 by default.\nfrom torchdata.datapipes.iter import IterableWrapper\n\ndp = IterableWrapper(['a', 'b', 'c'])\n# Assign indexs to the datepipe object.\nnew_dp =", "entry_point": "none", "canonical_solution": [" dp.enumerate()", " Enumerator(dp)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'Enumerator'\n}\n\n\ndef check():\n    assert list(new_dp) == [(0, 'a'), (1, 'b'), (2, 'c')]\n\n"}
{"task_id": "TorchDataEval/2", "prompt": "from torchdata.datapipes.iter import IterableWrapper, Sampler\nsource_dp = IterableWrapper(range(10))\nbatch_dp = source_dp.batch(batch_size=3, drop_last=True)\n\n# How to get one training data from the batch_dp\nresult =", "entry_point": "none", "canonical_solution": [" Sampler(batch_dp)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'Sampler'\n}\n\n\ndef check():\n    assert list(batch_dp) == [[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n\n"}
{"task_id": "TorchDataEval/4", "prompt": "# Please use the following APIs to solve the task:\n# demux(datapipe: torch.utils.data.dataset.IterDataPipe, num_instances: int, classifier_fn: Callable[[+T_co], Union[int, NoneType]], drop_none: bool = False, buffer_size: int = 1000): Splits the input DataPipe into multiple child DataPipes, using the given classification function.\nfrom torchdata.datapipes.iter import IterableWrapper\ndef odd_or_even(n):\n    return n % 2\nsource_dp = IterableWrapper(range(5))\n# Split into 2 sub-datapipes by the odd_or_even function\ndp1, dp2 =", "entry_point": "none", "canonical_solution": [" Demultiplexer(source_dp, 2, odd_or_even)", " source_dp.demux(2, odd_or_even)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'Demultiplexer'\n}\n\n\ndef check():\n    assert list(dp1) == [0, 2, 4]\n    assert list(dp2) == [1, 3]\n\n\n"}
{"task_id": "TorchDataEval/5", "prompt": "# Please use the following APIs to solve the task:\n# fork(datapipe: torch.utils.data.dataset.IterDataPipe, num_instances: int, buffer_size: int = 1000): Creates multiple instances of the same Iterable DataPipe.\nfrom torchdata.datapipes.iter import IterableWrapper\n\nsource_dp = IterableWrapper(range(5))\n# Clone the source datapipe two times\ndp1, dp2 =", "entry_point": "none", "canonical_solution": [" Forker(source_dp, 2)", " source_dp.fork(num_instances=2)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'Forker'\n}\n\n\ndef check():\n    assert list(dp1) == [0, 1, 2, 3, 4]\n    assert list(dp2) == [0, 1, 2, 3, 4]\n\n\n"}
{"task_id": "TorchDataEval/6", "prompt": "# Please use the following APIs to solve the task:\n# zip_with_iter(source_datapipe: IterDataPipe, ref_datapipe: IterDataPipe, key_fn: Callable, ref_key_fn: Optional[Callable] = None, keep_key: bool = False, buffer_size: int = 10000, merge_fn: Optional[Callable] = None): Zips two IterDataPipes together based on the matching key.\nfrom torchdata.datapipes.iter import IterableWrapper\nfrom operator import itemgetter\n\ndef merge_fn(t1, t2):\n    return t1[1] + t2[1]\ndp1 = IterableWrapper([('a', 100), ('b', 200), ('c', 300)])\ndp2 = IterableWrapper([('a', 1), ('b', 2), ('c', 3), ('d', 4)])\n# Putting two IterDataPipes together based on their key.\nres_dp =", "entry_point": "none", "canonical_solution": [" dp1.zip_with_iter(dp2, key_fn=itemgetter(0), ref_key_fn=itemgetter(0), keep_key=True, merge_fn=merge_fn)", " IterKeyZipper(dp1, dp2, key_fn=itemgetter(0), ref_key_fn=itemgetter(0), keep_key=True, merge_fn=merge_fn)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'IterKeyZipper'\n}\n\n\ndef check():\n    assert list(res_dp) == [('a', 101), ('b', 202), ('c', 303)]\n\n\n"}
{"task_id": "TorchDataEval/7", "prompt": "# Please use the following APIs to solve the task:\n# zip_with_map(source_iterdatapipe: IterDataPipe, map_datapipe: MapDataPipe, key_fn: Callable, merge_fn: Optional[Callable] = None): Joins the items from the source IterDataPipe with items from a MapDataPipe.\nfrom torchdata.datapipes.iter import IterableWrapper\nfrom torchdata.datapipes.map import SequenceWrapper\nfrom operator import itemgetter\n\ndef merge_fn(tuple_from_iter, value_from_map):\n    return tuple_from_iter[0], tuple_from_iter[1] + value_from_map\ndp1 = IterableWrapper([('a', 1), ('b', 2), ('c', 3)])\nmapdp = SequenceWrapper({'a': 100, 'b': 200, 'c': 300, 'd': 400})\n# Attach the elements in the source IterDataPipe to the elements in the MapDataPipe.\nres_dp =", "entry_point": "none", "canonical_solution": [" dp1.zip_with_map(map_datapipe=mapdp, key_fn=itemgetter(0), merge_fn=merge_fn)", " MapKeyZipper(dp1, mapdp, key_fn=itemgetter(0), merge_fn=merge_fn)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'MapKeyZipper'\n}\n\n\ndef check():\n    assert list(res_dp) == [('a', 101), ('b', 202), ('c', 303)]\n\n\n"}
{"task_id": "TorchDataEval/9", "prompt": "# Please use the following APIs to solve the task:\n# SampleMultiplexer(*args, **kwds): Takes a `Dict` of (IterDataPipe, Weight), and yields items by sampling from these DataPipes with respect to their weights.\nfrom torchdata.datapipes.iter import IterableWrapper, SampleMultiplexer\n\nsource_dp1 = IterableWrapper([0] * 10)\nsource_dp2 = IterableWrapper([5] * 10)\nweitghts = {source_dp1.cycle(2) : 0.2, source_dp2: 0.1}\n# Take samples from these DataPipes based on their weights with random seed 0, until all of them are exhausted.\nsample_mul_dp =", "entry_point": "none", "canonical_solution": [" SampleMultiplexer(pipes_to_weights_dict=weitghts, seed=0)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'SampleMultiplexer'\n}\n\n\ndef check():\n    assert list(sample_mul_dp) == [5, 5, 0, 0, 0, 0, 5, 0, 0, 0, 5, 0, 0, 5, 0, 0, 5, 5, 5, 5, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0]"}
{"task_id": "TorchDataEval/10", "prompt": "# Please use the following APIs to solve the task:\n# unzip(source_datapipe: torch.utils.data.dataset.IterDataPipe[typing.Sequence[~T]], sequence_length: int, buffer_size: int = 1000, columns_to_skip: Union[Sequence[int], NoneType] = None): Takes in a DataPipe of Sequences, unpacks each Sequence, and return the elements in separate DataPipes based on their position in the Sequence.\nfrom torchdata.datapipes.iter import IterableWrapper\n\nsource_dp = IterableWrapper([(i, i + 10, i + 20) for i in range(3)])\n# Unzip the three tuples, and return these elements in separate DataPipes, depending on their location.\ndp1, dp2, dp3 =", "entry_point": "none", "canonical_solution": [" source_dp.unzip(sequence_length=3)", " UnZipper(source_dp, sequence_length=3)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'SampleMultiplexer'\n}\n\n\ndef check():\n    assert list(dp1) == [0, 1, 2]\n    assert list(dp2) == [10, 11, 12]\n    assert list(dp3) == [20, 21, 22]"}
{"task_id": "TorchDataEval/11", "prompt": "# Please use the following APIs to solve the task:\n# batch(datapipe: IterDataPipe, batch_size: int, drop_last: bool = False, wrapper_class=List): Creates mini-batches of data.\n# bucketbatch(datapipe: torch.utils.data.dataset.IterDataPipe[+T_co], batch_size: int, drop_last: bool = False, batch_num: int = 100, bucket_num: int = 1, sort_key: Union[Callable, NoneType] = None, in_batch_shuffle: bool = True): Creates mini-batches of data from sorted bucket.\nfrom torchdata.datapipes.iter import IterableWrapper\n\ndp = IterableWrapper(range(10))\n# Divide datapipes into 3 batches and discard if the last batch is not reached.\ndp =", "entry_point": "none", "canonical_solution": [" dp.batch(batch_size=3, drop_last=True)", " Batcher(dp, batch_size=3, drop_last=True)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'Batcher'\n}\n\n\ndef check():\n    assert list(dp) == [[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n\n\n"}
{"task_id": "TorchDataEval/12", "prompt": "# Please use the following APIs to solve the task:\n# bucketbatch(datapipe: torch.utils.data.dataset.IterDataPipe[+T_co], batch_size: int, drop_last: bool = False, batch_num: int = 100, bucket_num: int = 1, sort_key: Union[Callable, NoneType] = None, in_batch_shuffle: bool = True): Creates mini-batches of data from sorted bucket.\nfrom torchdata.datapipes.iter import IterableWrapper\nsource_dp = IterableWrapper([3,2,1,6,0,5,4,9,8,7])\n\ndef sort_bucket(bucket):\n    return sorted(bucket)\n\n# Create batch datapipe with batch size 3, batch num is 100, and drop the last batch if it is not full.\n# Also, useing the sort_bucket function to sort the bucket, where the bucket_num is 1.\nbatch_dp =", "entry_point": "none", "canonical_solution": [" source_dp.bucketbatch(\n    batch_size=3, drop_last=True, batch_num=100, bucket_num=1, sort_key=sort_bucket\n)", " BucketBatcher(source_dp, batch_size=3, drop_last=True, batch_num=100, bucket_num=1, sort_key=sort_bucket)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'BucketBatcher'\n}\n\n\ndef check():\n    assert list(batch_dp) == [[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n\n\n"}
{"task_id": "TorchDataEval/14", "prompt": "# Please use the following APIs to solve the task:\n# groupby(datapipe: IterDataPipe[torch.utils.data.datapipes.iter.grouping.T_co], group_key_fn: Callable, *, buffer_size: int = 10000, group_size: Optional[int] = None, guaranteed_group_size: Optional[int] = None, drop_remaining: bool = False): Groups data from input IterDataPipe by keys which are generated from ``group_key_fn``, and yields a ``DataChunk`` with batch size up to ``group_size`` if defined.\nimport os\nfrom torchdata.datapipes.iter import IterableWrapper\n\ndef group_fn(file):\n   return os.path.basename(file).split(\".\")[0]\n\nsource_dp = IterableWrapper([\"a.png\", \"b.png\", \"a.json\", \"b.json\", \"a.jpg\", \"c.json\"])\n\n# Group by file name (except extension), we set the buffer size and group size to 3, and the guaranteed group size to 2.\ndp2 =", "entry_point": "none", "canonical_solution": [" source_dp.groupby(\n    group_key_fn=group_fn, \n    buffer_size=3, \n    group_size=3, \n    guaranteed_group_size=2\n)", " Grouper(\n    source_dp,\n    group_key_fn=group_fn, \n    buffer_size=3, \n    group_size=3, \n    guaranteed_group_size=2\n)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'Collator'\n}\n\n\ndef check():\n    assert list(collated_ds) == [torch.tensor(3.), torch.tensor(4.), torch.tensor(5.), torch.tensor(6.)]\n\n\n"}
{"task_id": "TorchDataEval/16", "prompt": "# Please use the following APIs to solve the task:\n# HttpReader(source_datapipe: IterDataPipe[str], timeout: Optional[float] = None): Takes file URLs (HTTP URLs pointing to files), and yields tuples of file URL and IO stream.\nfrom torchdata.datapipes.iter import IterableWrapper, HttpReader\nfile_url = \"https://raw.githubusercontent.com/pytorch/data/main/LICENSE\"\n\n# Using IterableWrapper to the file url and HttpReader to read the file\nhttp_reader_dp =", "entry_point": "none", "canonical_solution": [" HttpReader(IterableWrapper([file_url]))"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'IterableWrapper_HttpReader'\n}\n\n\ndef check():\n    assert list(http_reader_dp.readlines()) == list(HttpReader(IterableWrapper([file_url])).readlines())\n\n\n"}
{"task_id": "TorchDataEval/17", "prompt": "# Please use the following APIs to solve the task:\n# flatmap(*args, **kwds): Applies a function over each item from the source DataPipe, then flattens the outputs to a single, unnested IterDataPipe.\nfrom torchdata.datapipes.iter import IterableWrapper\n\ndef mutiple_fn(e):\n    return [e, e * 10]\n\nsource_dp = IterableWrapper(list(range(5)))\n# Each item in the source_dp is applied mutiple_fn function and the output is then tiled to a single, unnested one.\nnew_dp =", "entry_point": "none", "canonical_solution": [" source_dp.flatmap(mutiple_fn)", " FlatMapper(source_dp, mutiple_fn)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'FlatMapper'\n}\n\n\ndef check():\n    assert list(new_dp) == [0, 0, 1, 10, 2, 20, 3, 30, 4, 40]\n\n\n"}
{"task_id": "TorchDataEval/18", "prompt": "# Please use the following APIs to solve the task:\n# map(datapipe: IterDataPipe, fn: Callable, input_col=None, output_col=None): Applies a function over each item from the source DataPipe.\nfrom torchdata.datapipes.iter import IterableWrapper\n\ndef add_one(x):\n    return x + 1\ndp = IterableWrapper(range(10))\n# Method 1\nmap_dp_1 = dp.map(add_one)  # Invocation via functional form is preferred\n\n# Method 2\n# We discourage the usage of `lambda` functions as they are not serializable with `pickle`\n# Using `lambda` to implement add_two rather than add_one that is mentioned in above.\nnew_dp_2 =", "entry_point": "none", "canonical_solution": [" Mapper(dp, lambda x: x + 2)", " dp.map(lambda x: x + 2)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'Mapper'\n}\n\n\ndef check():\n    assert list(new_dp_2) == [2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n\n\n"}
{"task_id": "TorchDataEval/19", "prompt": "# Please use the following APIs to solve the task:\n# filter(datapipe: IterDataPipe, filter_fn: Callable, drop_empty_batches: bool = True): Filters out elements from the source datapipe according to input ``filter_fn``.\nfrom torchdata.datapipes.iter import IterableWrapper\n\ndef is_even(n):\n    return n % 2 == 0\ndp = IterableWrapper(range(5))\n# Filtering by the above function\nnew_dp =", "entry_point": "none", "canonical_solution": [" dp.filter(filter_fn=is_even)", " Filter(dp, filter_fn=is_even)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'Filter'\n}\n\n\ndef check():\n    assert list(new_dp) == [0, 2, 4]\n\n\n"}
{"task_id": "TorchDataEval/20", "prompt": "# Please use the following APIs to solve the task:\n# header(source_datapipe: IterDataPipe[torchdata.datapipes.iter.util.header.T_co], limit: int = 10): Yields elements from the source DataPipe from the start, up to the specfied limit.\nfrom torchdata.datapipes.iter import IterableWrapper\n\ndp = IterableWrapper(range(10))\n# How to get the first three elements of a datapipe?\nnew_dp =", "entry_point": "none", "canonical_solution": [" dp.header(3)", " Header(dp, 3)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'Header'\n}\n\n\ndef check():\n    assert list(new_dp) == [0, 1, 2]\n\n\n"}
{"task_id": "TorchDataEval/21", "prompt": "# Each element in a batch is a `Dict`\nfrom torchdata.datapipes.iter import IterableWrapper\ndp = IterableWrapper([[{'a': 1}, {'b': 2, 'a': 1}], [{'a': 2, 'b': 200}, {'b': 2, 'c': 3, 'a': 100}]])\n\n# Takes an input DataPipe with batches of data, processes the batches one and produces a Dict for each batch.\n# We only need the column 'a' from each batch.\nnew_dp =", "entry_point": "none", "canonical_solution": [" dp.rows2columnar(column_names=['a'])", " Rows2Columnar(dp, column_names=['a'])"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'Rows2Columnar'\n}\n\n\ndef check():\n    assert list(new_dp) == list(dp.rows2columnar(column_names=['a']))\n\n\n"}
{"task_id": "TorchDataEval/23", "prompt": "# Please use the following APIs to solve the task:\n# batch(datapipe: MapDataPipe[T], batch_size: int, drop_last: bool = False, wrapper_class=DataChunk): Create mini-batches of data.\nfrom torchdata.datapipes.map import SequenceWrapper, Mapper\ndp = SequenceWrapper(range(10))\nmap_dp_1 = dp.map(lambda x: x + 1)  # Using functional form (recommended)\nmap_dp_2 = Mapper(dp, lambda x: x + 1)  # Using class constructor\n\n# Get the mapper datapipe (map_dp_1) batch datas with the batch size of 2.\nnew_dp =", "entry_point": "none", "canonical_solution": [" map_dp_1.batch(batch_size=2)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'Batcher'\n}\n\n\ndef check():\n    assert list(new_dp) == [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]\n\n\n"}
{"task_id": "TorchDataEval/24", "prompt": "# Please use the following APIs to solve the task:\n# HttpReader(source_datapipe: IterDataPipe[str], timeout: Optional[float] = None): Takes file URLs (HTTP URLs pointing to files), and yields tuples of file URL and IO stream.\nfrom torchdata.datapipes.iter import HttpReader\nURL = \"https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv\"\n# Read the URL using the HTTP protocol and process the csv file.\nag_news_train =", "entry_point": "none", "canonical_solution": [" HttpReader([URL]).parse_csv()"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'HttpReader_CSVParser'\n}\n\n\ndef check():\n    assert list(ag_news_train) == list(HttpReader([URL]).parse_csv())\n\n\n"}
{"task_id": "TorchDataEval/25", "prompt": "# Please use the following APIs to solve the task:\n# map(datapipe: IterDataPipe, fn: Callable, input_col=None, output_col=None): Applies a function over each item from the source DataPipe.\n# HttpReader(source_datapipe: IterDataPipe[str], timeout: Optional[float] = None): Takes file URLs (HTTP URLs pointing to files), and yields tuples of file URL and IO stream.\nfrom torchdata.datapipes.iter import HttpReader\nURL = \"https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv\"\n# Read the URL using the HTTP protocol and process the csv file.\n# Then, we map the datapipe using lambda_func_ to get what we want.\nlambda_func_ = lambda t: (int(t[0]), \" \".join(t[1:]))\nag_news_train =", "entry_point": "none", "canonical_solution": [" HttpReader([URL]).parse_csv().map(lambda_func_)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'HttpReader_CSVParser_Mapper'\n}\n\n\ndef check():\n    assert list(ag_news_train) == list(HttpReader([URL]).parse_csv().map(lambda_func_))\n\n\n"}
{"task_id": "TorchDataEval/26", "prompt": "from torchdata.datapipes.iter import HttpReader\n\nURL = \"https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv\"\n# Read the URL using the HTTP protocol and process the csv file.\n# Then, we map the datapipe using lambda_func_ to get what we want.\nlambda_func_ = lambda t: (int(t[0]), \" \".join(t[1:]))\nag_news_train = HttpReader([URL]).parse_csv().map(lambda_func_)\nlambda_batch = lambda batch: {'labels': [sample[0] for sample in batch],\\\n                                      'text': [sample[1].split() for sample in batch]}\n# How to get all batches from a datapipe with batch size 2?\n# Furthermore, the batches should be mapped using lambda_batch.\nagn_batches =", "entry_point": "none", "canonical_solution": [" ag_news_train.batch(2).map(lambda_batch)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'HttpReader_CSVParser_Mapper_Batcher'\n}\n\n\ndef check():\n    assert list(agn_batches) == list(ag_news_train.batch(2).map(lambda_batch))\n\n\n"}
{"task_id": "TorchDataEval/27", "prompt": "from torchdata.datapipes.iter import IterableWrapper, Sampler\ndp = IterableWrapper(range(3))\n# Augument the datapipe with repeat three times and sample the data.\ndp =", "entry_point": "none", "canonical_solution": [" Sampler(dp.cycle(5))"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'Sampler_Cycler'\n}\n\n\ndef check():\n    assert list(dp) == [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n\n\n"}
{"task_id": "TorchDataEval/28", "prompt": "# Please use the following APIs to solve the task:\n# concat(*args, **kwds): Concatenates multiple Iterable DataPipes.\nfrom torchdata.datapipes.iter import IterableWrapper\ndp1 = IterableWrapper(range(3))\ndp2 = IterableWrapper(range(5))\n\n# First we concatenate two datapipes and then repeat the concatenated datapipe three times.\ndp =", "entry_point": "none", "canonical_solution": [" Concater(dp1, dp2).cycle(3)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'Concater_Cycler'\n}\n\n\ndef check():\n    assert list(dp) == [0, 1, 2, 0, 1, 2, 3, 4, 0, 1, 2, 0, 1, 2, 3, 4, 0, 1, 2, 0, 1, 2, 3, 4]\n\n\n"}
{"task_id": "TorchDataEval/29", "prompt": "from torchdata.datapipes.iter import IterableWrapper\nfrom operator import itemgetter\n\ndef merge_fn(t1, t2):\n    return t1[1] + t2[1]\n\ndp1 = IterableWrapper([('a', 100), ('b', 200), ('c', 300)])\ndp2 = IterableWrapper([('a', 1), ('b', 2), ('c', 3), ('d', 4)])\n# According to the merge_fn, we zip the above two datapipes and keep the key True.\n# Whatsmore, cycle the zipped datapipe three times.\nres_dp =", "entry_point": "none", "canonical_solution": [" dp1.zip_with_iter(dp2, key_fn=itemgetter(0),\n                           ref_key_fn=itemgetter(0), keep_key=True, merge_fn=merge_fn).cycle(3)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'IterKeyZipper_Cycler'\n}\n\n\ndef check():\n    assert list(res_dp) == [('a', 101), ('b', 202), ('c', 303), ('a', 101), ('b', 202), ('c', 303), ('a', 101), ('b', 202), ('c', 303)]\n\n\n"}
{"task_id": "TorchDataEval/30", "prompt": "# Please use the following APIs to solve the task:\n# zip_with_iter(source_datapipe: IterDataPipe, ref_datapipe: IterDataPipe, key_fn: Callable, ref_key_fn: Optional[Callable] = None, keep_key: bool = False, buffer_size: int = 10000, merge_fn: Optional[Callable] = None): Zips two IterDataPipes together based on the matching key.\nfrom torchdata.datapipes.iter import IterableWrapper\nfrom operator import itemgetter\n\ndef merge_fn(t1, t2):\n    return t1[1] * t2[1]\n\ndp1 = IterableWrapper([('a', 100), ('b', 200), ('c', 300)])\ndp2 = IterableWrapper([('a', 1), ('b', 2), ('c', 3), ('d', 4)])\n# We zipp the above two data pipes and set keep_key to True according to merge_fn.\n# Also, enumerating the zipped datapipe.\nres_dp =", "entry_point": "none", "canonical_solution": [" dp1.zip_with_iter(dp2, key_fn=itemgetter(0),\n                           ref_key_fn=itemgetter(0), keep_key=True, merge_fn=merge_fn).enumerate()"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'IterKeyZipper_Enumerator'\n}\n\n\ndef check():\n    assert list(res_dp) == [(0, ('a', 100)), (1, ('b', 400)), (2, ('c', 900))]\n\n\n"}
{"task_id": "TorchDataEval/31", "prompt": "# Please use the following APIs to solve the task:\n# zip_with_iter(source_datapipe: IterDataPipe, ref_datapipe: IterDataPipe, key_fn: Callable, ref_key_fn: Optional[Callable] = None, keep_key: bool = False, buffer_size: int = 10000, merge_fn: Optional[Callable] = None): Zips two IterDataPipes together based on the matching key.\nfrom torchdata.datapipes.iter import IterableWrapper\nfrom operator import itemgetter\n\ndef merge_fn(t1, t2):\n    return t1[1] * t2[1]\n\ndp1 = IterableWrapper([('a', 100), ('b', 200), ('c', 300)])\ndp2 = IterableWrapper([('a', 1), ('b', 2), ('c', 3), ('d', 4)])\n# Zipping the above two data pipes and set keep_key to True according to merge_fn. \n# Moreover, transform its type to List and get the first element.\nres_dp =", "entry_point": "none", "canonical_solution": [" list(dp1.zip_with_iter(dp2, key_fn=itemgetter(0),\n                           ref_key_fn=itemgetter(0), keep_key=True, merge_fn=merge_fn))[0]"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'IterKeyZipper_List'\n}\n\n\ndef check():\n    assert res_dp == ('a', 100)\n\n\n"}
{"task_id": "TorchDataEval/32", "prompt": "# Please use the following APIs to solve the task:\n# zip_with_map(source_iterdatapipe: IterDataPipe, map_datapipe: MapDataPipe, key_fn: Callable, merge_fn: Optional[Callable] = None): Joins the items from the source IterDataPipe with items from a MapDataPipe.\nfrom torchdata.datapipes.iter import IterableWrapper\nfrom torchdata.datapipes.map import SequenceWrapper\nfrom operator import itemgetter\n\ndef merge_fn(tuple_from_iter, value_from_map):\n    return tuple_from_iter[0], tuple_from_iter[1] + value_from_map\n\ndp1 = IterableWrapper([('a', 1), ('b', 2), ('c', 3)])\nmapdp = SequenceWrapper({'a': 100, 'b': 200, 'c': 300, 'd': 400})\n\n# Using merge_fn to zip the two data pipes.\n# Repeating three times to argument the zipped data pipe.\nres_dp =", "entry_point": "none", "canonical_solution": [" dp1.zip_with_map(map_datapipe=mapdp, key_fn=itemgetter(0), merge_fn=merge_fn).cycle(3)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'MapKeyZipper_Cycler'\n}\n\n\ndef check():\n    assert list(res_dp) == [('a', 101), ('b', 202), ('c', 303), ('a', 101), ('b', 202), ('c', 303), ('a', 101), ('b', 202), ('c', 303)]\n\n\n"}
{"task_id": "TorchDataEval/33", "prompt": "from torchdata.datapipes.iter import IterableWrapper\nfrom torchdata.datapipes.map import SequenceWrapper\nfrom operator import itemgetter\n\ndef merge_fn(tuple_from_iter, value_from_map):\n    return tuple_from_iter[0], tuple_from_iter[1] + value_from_map\n\ndp1 = IterableWrapper([('a', 1), ('b', 2), ('c', 3)])\nmapdp = SequenceWrapper({'a': 100, 'b': 200, 'c': 300, 'd': 400})\n\n# Using merge_fn to zip the two data pipes, and repeating three times to argument the zipped data pipe.\n# Finally, we convert the result type to a list and take the second element of each tuple.\nres_dp =", "entry_point": "none", "canonical_solution": [" list(dp1.zip_with_map(map_datapipe=mapdp, key_fn=itemgetter(0), merge_fn=merge_fn).cycle(3))[1]"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'MapKeyZipper_Cycler_List'\n}\n\n\ndef check():\n    assert res_dp == ('b', 202)\n\n\n"}
{"task_id": "TorchDataEval/34", "prompt": "from torchdata.datapipes.iter import IterableWrapper\nfrom torchdata.datapipes.map import SequenceWrapper\nfrom torchdata.datapipes.iter import Sampler\nfrom operator import itemgetter\n\ndef merge_fn(tuple_from_iter, value_from_map):\n    return tuple_from_iter[0], tuple_from_iter[1] + value_from_map\n\ndp1 = IterableWrapper([('a', 1), ('b', 2), ('c', 3)])\nmapdp = SequenceWrapper({'a': 100, 'b': 200, 'c': 300, 'd': 400})\n\n# Using merge_fn to zip the two data pipes, and repeating three times to argument the zipped data pipe, and then sampling the result.\n# Finally, we convert the result type to a list and take the third element of each tuple.\nres_dp =", "entry_point": "none", "canonical_solution": [" list(Sampler(dp1.zip_with_map(map_datapipe=mapdp, key_fn=itemgetter(0), merge_fn=merge_fn).cycle(3)))[2]"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'MapKeyZipper_Cycler_Sampler_List'\n}\n\n\ndef check():\n    assert res_dp == 2\n\n\n"}
{"task_id": "TorchDataEval/35", "prompt": "# Please use the following APIs to solve the task:\n# groupby(datapipe: IterDataPipe[torch.utils.data.datapipes.iter.grouping.T_co], group_key_fn: Callable, *, buffer_size: int = 10000, group_size: Optional[int] = None, guaranteed_group_size: Optional[int] = None, drop_remaining: bool = False): Groups data from input IterDataPipe by keys which are generated from ``group_key_fn``, and yields a ``DataChunk`` with batch size up to ``group_size`` if defined.\nimport os\nfrom torchdata.datapipes.iter import IterableWrapper\n\ndef group_fn(file):\n   return os.path.basename(file).split(\".\")[0]\n\nsource_dp = IterableWrapper([\"a.png\", \"b.png\", \"a.json\", \"b.json\", \"a.jpg\", \"c.json\"])\n\n# Group the files by their file name using the group_fn function.\n# Then, reserving the length of group result greater than 1.\ndp0 =", "entry_point": "none", "canonical_solution": [" source_dp.groupby(group_key_fn=group_fn).filter(lambda x: len(x) > 1)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'Grouper_Filter'\n}\n\n\ndef check():\n    assert list(dp0) == [['a.png', 'a.json', 'a.jpg'], ['b.png', 'b.json']]\n\n\n"}
{"task_id": "TorchDataEval/37", "prompt": "# Please use the following APIs to solve the task:\n# collate(datapipe: IterDataPipe, collate_fn: Callable = <function default_collate>): Collates samples from DataPipe to Tensor(s) by a custom collate function.\n# map(datapipe: IterDataPipe, fn: Callable, input_col=None, output_col=None): Applies a function over each item from the source DataPipe.\nimport torch\nclass MyIterDataPipe(torch.utils.data.IterDataPipe):\n    def __init__(self, start, end):\n        super(MyIterDataPipe).__init__()\n        assert end > start, \"this example code only works with end >= start\"\n        self.start = start\n        self.end = end\n\n    def __iter__(self):\n        return iter(range(self.start, self.end))\n\n    def __len__(self):\n        return self.end - self.start\nds = MyIterDataPipe(start=3, end=7)\n\ndef collate_fn(batch):\n    return torch.tensor(batch, dtype=torch.float)\n\n# First get the head 2 elements\n# Second make the datapipe tensor-like by using `collate_fn`\ncollated_ds =", "entry_point": "none", "canonical_solution": [" Collator(ds.header(2), collate_fn=collate_fn)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'Collator_Header'\n}\n\n\ndef check():\n    assert list(collated_ds) == [torch.tensor(3.), torch.tensor(4.)]\n\n\n"}
{"task_id": "TorchDataEval/38", "prompt": "# Please use the following APIs to solve the task:\n# collate(datapipe: IterDataPipe, collate_fn: Callable = <function default_collate>): Collates samples from DataPipe to Tensor(s) by a custom collate function.\n# filter(datapipe: IterDataPipe, filter_fn: Callable, drop_empty_batches: bool = True): Filters out elements from the source datapipe according to input ``filter_fn``.\nimport torch\nclass MyIterDataPipe(torch.utils.data.IterDataPipe):\n    def __init__(self, start, end):\n        super(MyIterDataPipe).__init__()\n        assert end > start, \"this example code only works with end >= start\"\n        self.start = start\n        self.end = end\n\n    def __iter__(self):\n        return iter(range(self.start, self.end))\n\n    def __len__(self):\n        return self.end - self.start\nds = MyIterDataPipe(start=3, end=7)\n\ndef collate_fn(batch):\n    return torch.tensor(batch, dtype=torch.float)\n\n# Filter the value smaller than 5\n# Second make the datapipe tensor-like by using `collate_fn`\ncollated_ds =", "entry_point": "none", "canonical_solution": [" Collator(ds.filter(lambda x: x < 5), collate_fn=collate_fn)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'Collator_Filter'\n}\n\n\ndef check():\n    assert list(collated_ds) == [torch.tensor(3.), torch.tensor(4.)]\n\n\n"}
{"task_id": "TorchDataEval/40", "prompt": "# Please use the following APIs to solve the task:\n# demux(datapipe: torch.utils.data.dataset.IterDataPipe, num_instances: int, classifier_fn: Callable[[+T_co], Union[int, NoneType]], drop_none: bool = False, buffer_size: int = 1000): Splits the input DataPipe into multiple child DataPipes, using the given classification function.\nfrom torchdata.datapipes.iter import IterableWrapper\n\ndef great_than_5(x: int):\n    return x > 5\n\nsource_dp = IterableWrapper(range(10))\n# Split the source datapipe into two datapipes by applying the function `great_than_5`\ndp_one, dp_two =", "entry_point": "none", "canonical_solution": [" source_dp.demux(num_instances=2, classifier_fn=great_than_5)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'Demultiplexer'\n}\n\n\ndef check():\n    assert list(dp_one) == [0, 1, 2, 3, 4, 5]\n    assert list(dp_two) == [6, 7, 8, 9]\n\n\n"}
{"task_id": "TorchDataEval/41", "prompt": "from torchdata.datapipes.iter import IterableWrapper\nfrom torchdata.datapipes.iter import SampleMultiplexer\n\ndp1 = IterableWrapper([1, 2, 3, 4, 5, 6])\ndp2 = IterableWrapper([7, 8, 9, 10, 11, 12])\nweight_ = {\n    dp1: 0.8, \n    dp2: 0.2\n}\n\n# Given the weight, how to sample from two datapipes?\n# Note that the sample seed is set to 1 for reproducibility\nresult_dp =", "entry_point": "none", "canonical_solution": [" SampleMultiplexer(pipes_to_weights_dict=weight_, seed=1)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'SampleMultiplexer'\n}\n\n\ndef check():\n    assert list(result_dp) == [1, 7, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12]\n\n\n"}
{"task_id": "TorchDataEval/42", "prompt": "# Please use the following APIs to solve the task:\n# map(datapipe: IterDataPipe, fn: Callable, input_col=None, output_col=None): Applies a function over each item from the source DataPipe.\n# unzip(source_datapipe: torch.utils.data.dataset.IterDataPipe[typing.Sequence[~T]], sequence_length: int, buffer_size: int = 1000, columns_to_skip: Union[Sequence[int], NoneType] = None): Takes in a DataPipe of Sequences, unpacks each Sequence, and return the elements in separate DataPipes based on their position in the Sequence.\nfrom torchdata.datapipes.iter import IterableWrapper\n\nraw_dp = IterableWrapper([(0, 10, 20), (1, 11, 21), (2, 12, 22)])\n# I would like assgin dp1 to be a datapipe that contains the first column of raw_dp\n# and dp2 to be a datapipe that contains the second column of raw_dp\n# and dp3 to be a datapipe that contains the third column of raw_dp\n# How to do this?\ndp1, dp2, dp3 =", "entry_point": "none", "canonical_solution": [" raw_dp.unzip(sequence_length=3)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'UnZipper'\n}\n\n\ndef check():\n    assert list(dp1) == [0, 1, 2]\n\n\n"}
{"task_id": "TorchDataEval/43", "prompt": "# Please use the following APIs to solve the task:\n# batch(datapipe: IterDataPipe, batch_size: int, drop_last: bool = False, wrapper_class=List): Creates mini-batches of data.\nfrom torchdata.datapipes.iter import IterableWrapper\n\ndp = IterableWrapper([\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\"])\n# Make a batch operation on the datapipe `dp` of size 3 with droping last batch if it is not full.\n# And then get the first two batches.\ndp =", "entry_point": "none", "canonical_solution": [" dp.batch(batch_size=3, drop_last=True).header(2)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'UnZipper'\n}\n\n\ndef check():\n    assert list(dp) == [['a', 'b', 'c'], ['d', 'e', 'f']]\n\n\n"}
{"task_id": "TorchDataEval/44", "prompt": "from torchdata.datapipes.iter import IterableWrapper\n\ndp1 = IterableWrapper([\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\"])\ndp2 = IterableWrapper([\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\"])\n# Batch on data pipe `dp1` of size 4 and discard the last batch if they are not filled, and then obtain the first two batches.\n# Then the above result is concatenated with the datapipe `dp2`.\ndp_3 =", "entry_point": "none", "canonical_solution": [" dp1.batch(batch_size=4, drop_last=True).header(2).concat(dp2)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'Batcher_Header_Concater'\n}\n\n\ndef check():\n    assert list(dp_3) == [['a', 'b', 'c', 'd'], ['e', 'f', 'g', 'h'], '1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n\n\n"}
{"task_id": "TorchDataEval/45", "prompt": "# Please use the following APIs to solve the task:\n# concat(*args, **kwds): Concatenates multiple Iterable DataPipes.\n# add_index(*args, **kwds): Adds an index to an existing Iterable DataPipe with.\nfrom torchdata.datapipes.iter import IterableWrapper\ndp_source_1 = IterableWrapper([{'a': 1, 'b': 2}, {'c': 3, 'a': 1}])\ndp_source_2 = IterableWrapper([{'d': 10, 'e': 20}, {'f': 30, 'd': 10}])\n\n# Concatenate two datapipes and add corresponding indices with the name `Ids`.\nindex_dp =", "entry_point": "none", "canonical_solution": [" dp_source_1.concat(dp_source_2).add_index(\"Ids\")"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'IndexAdder_Concater'\n}\n\n\ndef check():\n    assert list(index_dp) == [{'a': 1, 'b': 2, 'Ids': 0}, {'c': 3, 'a': 1, 'Ids': 1}, {'d': 10, 'e': 20, 'Ids': 2}, {'f': 30, 'd': 10, 'Ids': 3}]\n\n\n"}
{"task_id": "TorchDataEval/46", "prompt": "from torchdata.datapipes.iter import IterableWrapper\n\ndp_source_1 = IterableWrapper([{'a': 1, 'b': 2}, {'c': 3, 'a': 1}])\ndp_source_2 = IterableWrapper([{'d': 10, 'e': 20}, {'f': 30, 'd': 10}])\n\n# Join the two data pipes and add an index with the name `Ids`.\n# Then create three copies of the datapipe.\nindex_dp1, index_dp2, index_dp3 =", "entry_point": "none", "canonical_solution": [" dp_source_1.concat(dp_source_2).add_index(\"Ids\").fork(num_instances=3)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'IndexAdder_Concater_Forker'\n}\n\n\ndef check():\n    assert list(index_dp1) == [{'a': 1, 'b': 2, 'Ids': 0}, {'c': 3, 'a': 1, 'Ids': 1}, {'d': 10, 'e': 20, 'Ids': 2}, {'f': 30, 'd': 10, 'Ids': 3}]\n    assert list(index_dp2) == [{'a': 1, 'b': 2, 'Ids': 0}, {'c': 3, 'a': 1, 'Ids': 1}, {'d': 10, 'e': 20, 'Ids': 2}, {'f': 30, 'd': 10, 'Ids': 3}]\n    assert list(index_dp3) == [{'a': 1, 'b': 2, 'Ids': 0}, {'c': 3, 'a': 1, 'Ids': 1}, {'d': 10, 'e': 20, 'Ids': 2}, {'f': 30, 'd': 10, 'Ids': 3}]\n\n\n"}
{"task_id": "TorchDataEval/47", "prompt": "from torchdata.datapipes.iter import IterableWrapper\n\ndp_source_1 = IterableWrapper([{'a': 1, 'b': 2}, {'c': 3, 'a': 1}])\ndp_source_2 = IterableWrapper([{'d': 10, 'e': 20}, {'f': 30, 'd': 10}])\ndp_source_3 = IterableWrapper([{'g': 100, 'h': 200}, {'i': 300, 'g': 100}])\n\n# Join the three data pipes and obtain the enumerated datapipe.\nnew_dp =", "entry_point": "none", "canonical_solution": [" dp_source_1.concat(dp_source_2).enumerate()"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'Concater_Enumerator'\n}\n\n\ndef check():\n    assert list(new_dp) == [(0, {'a': 1, 'b': 2}), (1, {'c': 3, 'a': 1}), (2, {'d': 10, 'e': 20}), (3, {'f': 30, 'd': 10})]\n\n\n"}
{"task_id": "TorchDataEval/48", "prompt": "# Please use the following APIs to solve the task:\n# flatmap(*args, **kwds): Applies a function over each item from the source DataPipe, then flattens the outputs to a single, unnested IterDataPipe.\nfrom torchdata.datapipes.iter import IterableWrapper\n\ndef flatted_func(x):\n    return [x, x+\"_1\", x+\"_2\"]\n\nsource_dp = IterableWrapper([\"a\", \"b\", \"c\"])\n# I want to augment the source datapipe with the above function, which will return nine elements.\n# Then we flatten the nine elements into a single datapipe.\nnew_dp =", "entry_point": "none", "canonical_solution": [" source_dp.flatmap(flatted_func)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'FlatMapper'\n}\n\n\ndef check():\n    assert list(new_dp) == ['a', 'a_1', 'a_2', 'b', 'b_1', 'b_2', 'c', 'c_1', 'c_2']\n\n\n"}
{"task_id": "TorchDataEval/49", "prompt": "# Please use the following APIs to solve the task:\n# HttpReader(source_datapipe: IterDataPipe[str], timeout: Optional[float] = None): Takes file URLs (HTTP URLs pointing to files), and yields tuples of file URL and IO stream.\nfrom torchdata.datapipes.iter import HttpReader\n\nAG_NEWS_CSV_URL = \"https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv\"\n# Read the URL using the HTTP protocol and parse the csv file as a dictionary.\nag_news_train =", "entry_point": "none", "canonical_solution": [" HttpReader([AG_NEWS_CSV_URL]).parse_csv_as_dict()"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'HttpReader_CSVDictParser'\n}\n\n\ndef check():\n    assert list(ag_news_train) == list(HttpReader([AG_NEWS_CSV_URL]).parse_csv_as_dict())\n\n\n"}
{"task_id": "TorchDataEval/3", "prompt": "# Please use the following APIs to solve the task:\n# concat(*args, **kwds): Concatenate multiple Map DataPipes.\nfrom torchdata.datapipes.iter import IterableWrapper\ndp_one, dp_two = IterableWrapper(range(3)), IterableWrapper(range(5))\n\n# concat two datapipes\nnew_dp =", "entry_point": "none", "canonical_solution": [" dp_one.concat(dp_two)", " dp_one + dp_two"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'concat'\n}\n\n\ndef check():\n    assert list(new_dp) == [0, 1, 2, 0, 1, 2, 3, 4]\n\n\n"}
{"task_id": "TorchDataEval/8", "prompt": "# Please use the following APIs to solve the task:\n# mux(*datapipes): Yields one element at a time from each of the input Iterable DataPipes.\nfrom torchdata.datapipes.iter import IterableWrapper\n\ndp1, dp2 = IterableWrapper(range(3)), IterableWrapper(range(10, 15))\n# One element is generated from each input Iterable DataPipes in turn, until the end when the shortest input DataPipe is used up.\nresult =", "entry_point": "none", "canonical_solution": [" dp1.mux(dp2)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'mux'\n}\n\n\ndef check():\n    assert list(result) == [0, 10, 1, 11, 2, 12, 13, 14]\n\n\n"}
{"task_id": "TorchDataEval/13", "prompt": "import torch\nclass MyIterDataPipe(torch.utils.data.IterDataPipe):\n    def __init__(self, start, end):\n        super(MyIterDataPipe).__init__()\n        assert end > start, \"this example code only works with end >= start\"\n        self.start = start\n        self.end = end\n\n    def __iter__(self):\n        return iter(range(self.start, self.end))\n    def __len__(self):\n        return self.end - self.start\nds = MyIterDataPipe(start=3, end=7)\n\ndef int2tensor(batch):\n    return torch.tensor(batch, dtype=torch.float)\n\n# convert integer to float Tensor using `int2tensor`.\ncollated_ds =", "entry_point": "none", "canonical_solution": [" ds.collate(int2tensor)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'collate'\n}\n\n\ndef check():\n    assert list(collated_ds) == [torch.tensor(3.), torch.tensor(4.), torch.tensor(5.), torch.tensor(6.)]\n\n\n"}
{"task_id": "TorchDataEval/15", "prompt": "# Please use the following APIs to solve the task:\n# unbatch(datapipe: IterDataPipe, unbatch_level: int = 1): Undoes batching of data.\nfrom torchdata.datapipes.iter import IterableWrapper\n\nsource_dp = IterableWrapper([[[0, 1], [2]], [[3, 4], [5]], [[6]]])\n# Does the unbatch processing of data, the level is setted by default to 1.\ndp2 =", "entry_point": "none", "canonical_solution": [" source_dp.unbatch(1)", " source_dp.unbatch()"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'unbatch'\n}\n\n\ndef check():\n    assert list(dp2) == [[0, 1], [2], [3, 4], [5], [6]]\n\n\n"}
{"task_id": "TorchDataEval/22", "prompt": "from torchdata.datapipes.iter import IterableWrapper, StreamReader\nfrom io import StringIO\n\ndp = IterableWrapper([(\"alphabet\", StringIO(\"abcde\"))])\n# generating bytes where the chunk is set to one.\nresult_dp =", "entry_point": "none", "canonical_solution": [" StreamReader(dp, chunk=1)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'StreamReader'\n}\n\n\ndef check():\n    assert list(result_dp) == [('alphabet', 'a'), ('alphabet', 'b'), ('alphabet', 'c'), ('alphabet', 'd'), ('alphabet', 'e')]\n\n\n"}
{"task_id": "TorchDataEval/39", "prompt": "import random\nfrom torchdata.datapipes.iter import IterableWrapper\n\ndp_dog = IterableWrapper([\"dog1\", \"dog2\", \"dog3\"])\ndp_cat = IterableWrapper([\"cat1\", \"cat2\", \"cat3\"])\n\ndef remove_final_number(s):\n    return s[:-1]\n\n# Put the above DataPipes into one list obj, and remove the last number from each element (e.g., \"1\" in \"dog1\")\nresult =", "entry_point": "none", "canonical_solution": [" [remove_final_number(s) for s in dp_dog + dp_cat]", " list(dp_dog.concat(dp_cat).map(remove_final_number))"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'concat_map'\n}\n\n\ndef check():\n    assert result == ['dog', 'dog', 'dog', 'cat', 'cat', 'cat']\n\n\n"}
{"task_id": "TorchDataEval/36", "prompt": "# Please use the following APIs to solve the task:\n# groupby(datapipe: IterDataPipe[torch.utils.data.datapipes.iter.grouping.T_co], group_key_fn: Callable, *, buffer_size: int = 10000, group_size: Optional[int] = None, guaranteed_group_size: Optional[int] = None, drop_remaining: bool = False): Groups data from input IterDataPipe by keys which are generated from ``group_key_fn``, and yields a ``DataChunk`` with batch size up to ``group_size`` if defined.\n# header(source_datapipe: IterDataPipe[torchdata.datapipes.iter.util.header.T_co], limit: int = 10): Yields elements from the source DataPipe from the start, up to the specfied limit.\nimport os\nfrom torchdata.datapipes.iter import IterableWrapper\n\ndef group_fn(file):\n   return os.path.basename(file).split(\".\")[0]\n\nsource_dp = IterableWrapper([\"a.png\", \"b.png\", \"a.json\", \"b.json\", \"a.jpg\", \"c.json\"])\n\n# group by source_dp using the ``group_fn`` function and obtain the header groups by default, assign the result to the new variable ``header_groups``.\ndp0 =", "entry_point": "none", "canonical_solution": [" source_dp.groupby(group_fn).header()\nheader_groups = dp0"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'groupby_header'\n}\n\n\ndef check():\n    assert list(header_groups) == [['a.png', 'a.json', 'a.jpg'], ['b.png', 'b.json'], ['c.json']]\n\n\n"}
