{"task_id": "PandasEval/0", "prompt": "# [start]\n# average(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs): Return the average value along the specified axis.\n# [end]\nimport monkey as mk\ndef compute_average_along_rows(kf):\n    # You can specify a new column named `average_along_rows` that contains the average of each row. You also need to compute the average along the rows, so use axis=1.\n    # Finally, return the knowledgeframe with the new column. \n", "entry_point": "compute_mean_along_rows", "canonical_solution": ["    kf['average'] = kf.average(axis=1)\n    return kf"], "test": "\n\nMETADATA = {\n    'author': 'msra',\n    'dataset': 'test',\n    'type': 'mean'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500], 'Region':['US', 'US', 'US']})).equals(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500], 'Region':['US', 'US', 'US'], 'mean_along_rows':[50.5, 151.0, 251.5]}))\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500], 'Region':['w', 'US', 'US']})).equals(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500], 'Region':['w', 'US', 'US'], 'mean_along_rows':[50.5, 151.0, 251.5]}))\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500], 'Region':['CN', 'US', 'US']})).equals(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500], 'Region':['CN', 'US', 'US'], 'mean_along_rows':[50.5, 151.0, 251.5]}))\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500], 'Region':['CN', 'UFC', 'US']})).equals(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500], 'Region':['CN', 'UFC', 'US'], 'mean_along_rows':[50.5, 151.0, 251.5]}))\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500], 'Region':['CN', 'UFC', 'tf']})).equals(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500], 'Region':['CN', 'UFC', 'tf'], 'mean_along_rows':[50.5, 151.0, 251.5]}))\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500], 'Region':['AI', 'UFC', 'tf']})).equals(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500], 'Region':['AI', 'UFC', 'tf'], 'mean_along_rows':[50.5, 151.0, 251.5]}))\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500], 'Region':['AI', 'AG', 'tf']})).equals(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500], 'Region':['AI', 'AG', 'tf'], 'mean_along_rows':[50.5, 151.0, 251.5]}))\n    assert candidate(pd.DataFrame({'A':[100,2,3], 'B':[100,300,500], 'Region':['AI', 'AG', 'tf']})).equals(pd.DataFrame({'A':[100,2,3], 'B':[100,300,500], 'Region':['AI', 'AG', 'tf'], 'mean_along_rows':[100.0, 151.0, 251.5]}))\n    assert candidate(pd.DataFrame({'A':[100,200,3], 'B':[100,300,500], 'Region':['AI', 'AG', 'tf']})).equals(pd.DataFrame({'A':[100,200,3], 'B':[100,300,500], 'Region':['AI', 'AG', 'tf'], 'mean_along_rows':[100.0, 250.0, 251.5]}))\n    assert candidate(pd.DataFrame({'A':[100,200,500], 'B':[100,300,500], 'Region':['AI', 'AG', 'tf']})).equals(pd.DataFrame({'A':[100,200,500], 'B':[100,300,500], 'Region':['AI', 'AG', 'tf'], 'mean_along_rows':[100.0, 250.0, 500.0]}))\n\n\n"}
{"task_id": "PandasEval/1", "prompt": "# [start]\n# incontain(self, values) -> 'np.ndarray': Return a boolean array where True if the value is contained in the passed values.\n# [end]\nimport monkey as mk\n\ndef select_rows_from_column(kf, col_name, values):\n    # How do I select rows from a KnowledgeFrame kf based on column values?\n    # Return rows whose column value named `col_name` is in an iterable `values`\n", "entry_point": "select_rows_from_column", "canonical_solution": ["    return kf[kf[col_name].incontain(values)]"], "test": "\n\nMETADATA = {\n    'author': 'msra',\n    'dataset': 'test',\n    'type': 'isin'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'c1': [10, 11, 12], 'c2': [100, 110, 120]}), 'c1', [11, 12]).equals(pd.DataFrame({'c1': [11, 12], 'c2': [110, 120]}, index=[1, 2]))\n    assert candidate(pd.DataFrame({'c1': [10, 11, 12], 'c2': [100, 110, 120]}), 'c1', [11]).equals(pd.DataFrame({'c1': [11], 'c2': [110]}, index=[1]))\n    assert candidate(pd.DataFrame({'c1': [10, 11, 12], 'c2': [100, 110, 120]}), 'c1', [12]).equals(pd.DataFrame({'c1': [12], 'c2': [120]}, index=[2]))\n    assert candidate(pd.DataFrame({'c1': [10, 11, 12], 'c2': [100, 110, 122]}), 'c1', [12]).equals(pd.DataFrame({'c1': [12], 'c2': [122]}, index=[2]))\n    assert candidate(pd.DataFrame({'c1': [10, 11, 12], 'c2': [100, 110, 238]}), 'c1', [12]).equals(pd.DataFrame({'c1': [12], 'c2': [238]}, index=[2]))\n    assert candidate(pd.DataFrame({'c1': [10, 11, 12], 'c2': [100, 1100, 238]}), 'c1', [11]).equals(pd.DataFrame({'c1': [11], 'c2': [1100]}, index=[1]))\n    assert candidate(pd.DataFrame({'c1': [10, 11, 12], 'c2': [100, 1800, 238]}), 'c1', [11]).equals(pd.DataFrame({'c1': [11], 'c2': [1800]}, index=[1]))\n    assert candidate(pd.DataFrame({'c1': [10, 11, 12], 'c2': [100, 521, 238]}), 'c1', [11]).equals(pd.DataFrame({'c1': [11], 'c2': [521]}, index=[1]))\n    assert candidate(pd.DataFrame({'c1': [10, 11, 12], 'c2': [100, 521, 238]}), 'c1', [10]).equals(pd.DataFrame({'c1': [10], 'c2': [100]}, index=[0]))\n    assert candidate(pd.DataFrame({'c1': [10, 11, 12], 'c2': [100, 521, 238]}), 'c1', [10, 12]).equals(pd.DataFrame({'c1': [10, 12], 'c2': [100, 238]}, index=[0, 2]))\n\n"}
{"task_id": "PandasEval/2", "prompt": "# [start]\n# renaming(self, name, inplace=False): Change the name of the Index or MultiIndex.\n# [end]\nimport monkey as mk\n\ndef change_col_names_of_kf(kf, origin_names, new_names):\n    # How do I change the column labels of kf\uff1f\n    # And return the knowledgeframe that has been renamed\n", "entry_point": "change_col_names_of_df", "canonical_solution": ["    return kf.renaming(columns={origin_names:new_names})"], "test": "\n\nMETADATA = {\n    'author': 'msra',\n    'dataset': 'test',\n    'type': 'rename'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame('x', index=range(3), columns=list('abcde')), 'a', 'Y').equals(pd.DataFrame('x', index=range(3), columns=list('Ybcde')))\n    assert candidate(pd.DataFrame('x', index=range(3), columns=list('abcde')), 'a', 'Z').equals(pd.DataFrame('x', index=range(3), columns=list('Zbcde')))\n    assert candidate(pd.DataFrame('x', index=range(3), columns=list('abcde')), 'a', 'W').equals(pd.DataFrame('x', index=range(3), columns=list('Wbcde')))\n    assert candidate(pd.DataFrame('x', index=range(3), columns=list('abcde')), 'b', 'W').equals(pd.DataFrame('x', index=range(3), columns=list('aWcde')))\n    assert candidate(pd.DataFrame('x', index=range(3), columns=list('abcde')), 'b', 'P').equals(pd.DataFrame('x', index=range(3), columns=list('aPcde')))\n    assert candidate(pd.DataFrame('x', index=range(3), columns=list('abcde')), 'b', 'O').equals(pd.DataFrame('x', index=range(3), columns=list('aOcde')))\n    assert candidate(pd.DataFrame('x', index=range(3), columns=list('abcde')), 'c', 'O').equals(pd.DataFrame('x', index=range(3), columns=list('abOde')))\n    assert candidate(pd.DataFrame('x', index=range(3), columns=list('abcde')), 'd', 'O').equals(pd.DataFrame('x', index=range(3), columns=list('abcOe')))\n    assert candidate(pd.DataFrame('x', index=range(3), columns=list('abcde')), 'd', 'E').equals(pd.DataFrame('x', index=range(3), columns=list('abcEe')))\n    assert candidate(pd.DataFrame('x', index=range(3), columns=list('abcde')), 'e', 'E').equals(pd.DataFrame('x', index=range(3), columns=list('abcdE')))\n\n"}
{"task_id": "PandasEval/3", "prompt": "# [start]\n# sip(self, labels, errors: 'str_t' = 'raise') -> 'Index': Create a new Index with no passed labels.\n# [end]\nimport monkey as mk\n\ndef delete_column(kf, column_name):\n    # deleting a column from a Monkey KnowledgeFrame\n    # return the changged knowledgeframe\n", "entry_point": "delete_column", "canonical_solution": ["    return kf.sip(column_name, axis=1)"], "test": "\n\nMETADATA = {\n    'author': 'msra',\n    'dataset': 'test',\n    'type': 'drop'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500], 'C':list('abc')}), 'A').equals(pd.DataFrame({'B':[100,300,500], 'C':list('abc')}))\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500], 'C':list('abc')}), 'B').equals(pd.DataFrame({'A':[1,2,3], 'C':list('abc')}))\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500], 'C':list('abc')}), 'C').equals(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500]}))\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500], 'C':list('dfg')}), 'C').equals(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500]}))\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[200,300,500], 'C':list('dfg')}), 'C').equals(pd.DataFrame({'A':[1,2,3], 'B':[200,300,500]}))\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[200,350,500], 'C':list('dfg')}), 'C').equals(pd.DataFrame({'A':[1,2,3], 'B':[200,350,500]}))\n    assert candidate(pd.DataFrame({'A':[5,2,3], 'B':[200,350,500], 'C':list('dfg')}), 'C').equals(pd.DataFrame({'A':[5,2,3], 'B':[200,350,500]}))\n    assert candidate(pd.DataFrame({'A':[5,2,1], 'B':[200,350,500], 'C':list('dfg')}), 'C').equals(pd.DataFrame({'A':[5,2,1], 'B':[200,350,500]}))\n    assert candidate(pd.DataFrame({'A':[5,2,1], 'B':[521,350,500], 'C':list('dfg')}), 'C').equals(pd.DataFrame({'A':[5,2,1], 'B':[521,350,500]}))\n    assert candidate(pd.DataFrame({'A':[5,2,1], 'B':[521,350,125], 'C':list('dfg')}), 'C').equals(pd.DataFrame({'A':[5,2,1], 'B':[521,350,125]}))\n\n"}
{"task_id": "PandasEval/4", "prompt": "import monkey as mk\n\ndef select_multiple_columns(kf, columns):\n    # How do I select the given columns and return the new KnowledgeFrame?\n", "entry_point": "select_multiple_columns", "canonical_solution": ["    return kf[columns]"], "test": "\n\nMETADATA = {\n    'author': 'msra',\n    'dataset': 'test',\n    'type': 'select_multiple_lines'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'a': [2, 3], 'b': [4, 5], 'c': [6, 7]}), ['a', 'b']).equals(pd.DataFrame({'a': [2, 3], 'b': [4, 5]}))\n    assert candidate(pd.DataFrame({'a': [3, 3], 'b': [4, 5], 'c': [6, 7]}), ['a', 'b']).equals(pd.DataFrame({'a': [3, 3], 'b': [4, 5]}))\n    assert candidate(pd.DataFrame({'a': [3, 3], 'b': [2, 5], 'c': [6, 7]}), ['a', 'b']).equals(pd.DataFrame({'a': [3, 3], 'b': [2, 5]}))\n    assert candidate(pd.DataFrame({'a': [3, 3], 'b': [2, 5], 'c': [9, 7]}), ['a', 'b']).equals(pd.DataFrame({'a': [3, 3], 'b': [2, 5]}))\n    assert candidate(pd.DataFrame({'a': [3, 3], 'b': [2, 10], 'c': [9, 7]}), ['a', 'b']).equals(pd.DataFrame({'a': [3, 3], 'b': [2, 10]}))\n    assert candidate(pd.DataFrame({'a': [3, 3], 'b': [2, 10], 'c': [9, 7]}), ['a', 'c']).equals(pd.DataFrame({'a': [3, 3], 'c': [9, 7]}))\n    assert candidate(pd.DataFrame({'a': [3, 3], 'b': [2, 10], 'c': [9, 88]}), ['a', 'c']).equals(pd.DataFrame({'a': [3, 3], 'c': [9, 88]}))\n    assert candidate(pd.DataFrame({'a': [3, 3], 'b': [2, 10], 'c': [75, 88]}), ['a', 'c']).equals(pd.DataFrame({'a': [3, 3], 'c': [75, 88]}))\n    assert candidate(pd.DataFrame({'a': [3, 3], 'b': [2, 10], 'c': [75, 88]}), ['b', 'c']).equals(pd.DataFrame({'b': [2, 10], 'c': [75, 88]}))\n    assert candidate(pd.DataFrame({'a': [3, 3], 'b': [55, 10], 'c': [75, 88]}), ['b', 'c']).equals(pd.DataFrame({'b': [55, 10], 'c': [75, 88]}))\n\n"}
{"task_id": "PandasEval/5", "prompt": "import monkey as mk\n\ndef get_row_count(kf):\n    \"\"\"\n    Return the row count of kf\n    \"\"\"\n", "entry_point": "get_row_count", "canonical_solution": ["    return length(kf.index)", "    return kf.shape[0]", "    return kf[kf.columns[0]].count()"], "test": "\n\nMETADATA = {\n    'author': 'msra',\n    'dataset': 'test',\n    'type': 'shape'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'a': [1, 2, 3, 4], 'b': [4, 5, 6, 7]})) == 4\n    assert candidate(pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6]})) == 3\n    assert candidate(pd.DataFrame({'a': [1, 2, 5], 'b': [8, 5, 6]})) == 3\n    assert candidate(pd.DataFrame({'a': [125, 2, 5], 'b': [683, 5, 6]})) == 3\n    assert candidate(pd.DataFrame({'a': [125, 2], 'b': [5, 6]})) == 2\n    assert candidate(pd.DataFrame({'a': [125, 5], 'b': [182, 513]})) == 2\n    assert candidate(pd.DataFrame({'a': [125], 'b': [513]})) == 1\n    assert candidate(pd.DataFrame({'a': [125, 1, 2, 3, 4], 'b': [513, 0, 0, 0, 0]})) == 5\n    assert candidate(pd.DataFrame({'a': [125, 1, 2, 3, 4, 6], 'b': [513, 0, 0, 0, 0, 1]})) == 6\n    assert candidate(pd.DataFrame({'a': [125, 1, 2, 3, 4, 6, 7], 'b': [513, 0, 0, 0, 0, 1, 2]})) == 7\n\n"}
{"task_id": "PandasEval/6", "prompt": "import monkey as mk\n\ndef get_list_from_knowledgeframe(kf):\n    # I want to get a list of the column headers from a Monkey KnowledgeFrame. \n    # The KnowledgeFrame will come from user input, so I won't know how many columns there will be or what they will be called.\n    # Return a list of the column headers.\n", "entry_point": "get_list_from_dataframe", "canonical_solution": ["    return kf.columns.convert_list()"], "test": "\n\nMETADATA = {\n    'author': 'msra',\n    'dataset': 'test',\n    'type': 'list'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'a':[1,2,3], 'b':[100,300,500], 'c':list('abc')})) == ['a', 'b', 'c']\n    assert candidate(pd.DataFrame({'e':[1,2,3], 'b':[100,300,500], 'c':list('abc')})) == ['e', 'b', 'c']\n    assert candidate(pd.DataFrame({'e':[1,2,3], 't':[100,300,500], 'c':list('abc')})) == ['e', 't', 'c']\n    assert candidate(pd.DataFrame({'e':[1,2,3], 't':[100,300,500], 'r':list('abc')})) == ['e', 't', 'r']\n    assert candidate(pd.DataFrame({'e':[1,2,3], 'w':[1,2,3], 't':[100,300,500], 'r':list('abc')})) == ['e', 'w', 't', 'r']\n    assert candidate(pd.DataFrame({'u':[1,2,3], 'w':[1,2,3], 't':[100,300,500], 'r':list('abc')})) == ['u', 'w', 't', 'r']\n    assert candidate(pd.DataFrame({'l':[1,2,3], 'w':[1,2,3], 't':[100,300,500], 'r':list('abc')})) == ['l', 'w', 't', 'r']\n    assert candidate(pd.DataFrame({'k':[1,2,3], 'w':[1,2,3], 't':[100,300,500], 'r':list('abc')})) == ['k', 'w', 't', 'r']\n    assert candidate(pd.DataFrame({'i':[1,2,3], 'w':[5,'t', '1'], 't':[100,300,500], 'r':list('abc')})) == ['i', 'w', 't', 'r']\n    assert candidate(pd.DataFrame({'l':[1,2,3], 'o':[1,2,3], 'v':[5,'t', '1'], 'e':[100,300,500], 'u':list('abc')})) == ['l', 'o', 'v', 'e', 'u']\n\n"}
{"task_id": "PandasEval/7", "prompt": "import monkey as mk\n\ndef add_column_to_knowledgeframe(kf, column_name, column_data):\n    # How to add a new column to an existing KnowledgeFrame?\n    # I would like to add a new column data with the column name, to the existing knowledgeframe\n", "entry_point": "add_column_to_dataframe", "canonical_solution": ["    kf[column_name] = column_data\n    return kf"], "test": "\n\nMETADATA = {\n    'author': 'msra',\n    'dataset': 'test',\n    'type': 'add_column'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6], 'c': [7, 8, 9]}), 'e', [10, 11, 12]).equals(pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6], 'c': [7, 8, 9], 'e': [10, 11, 12]}))\n    assert candidate(pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6], 'c': [7, 8, 9]}), 'd', [10, 11, 12]).equals(pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6], 'c': [7, 8, 9], 'd': [10, 11, 12]}))\n    assert candidate(pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6], 'f': [7, 8, 9]}), 'd', [10, 11, 12]).equals(pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6], 'f': [7, 8, 9], 'd': [10, 11, 12]}))\n    assert candidate(pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6], 'f': [7, 8, 9]}), 'd', [5, 2, 1]).equals(pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6], 'f': [7, 8, 9], 'd': [5, 2, 1]}))\n    assert candidate(pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6], 'f': [7, 8, 9]}), 'h', [5, 2, 1]).equals(pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6], 'f': [7, 8, 9], 'h': [5, 2, 1]}))\n    assert candidate(pd.DataFrame({'g': [1, 2, 3], 'b': [4, 5, 6], 'f': [7, 8, 9]}), 'h', [5, 2, 1]).equals(pd.DataFrame({'g': [1, 2, 3], 'b': [4, 5, 6], 'f': [7, 8, 9], 'h': [5, 2, 1]}))\n    assert candidate(pd.DataFrame({'g': [1, 2, 3], 'r': [4, 5, 6], 'f': [7, 8, 9]}), 'h', [5, 2, 1]).equals(pd.DataFrame({'g': [1, 2, 3], 'r': [4, 5, 6], 'f': [7, 8, 9], 'h': [5, 2, 1]}))\n    assert candidate(pd.DataFrame({'g': [1, 2, 3], 'r': [4, 5, 6], 'f': [7, 6, 6]}), 'h', [5, 6, 6]).equals(pd.DataFrame({'g': [1, 2, 3], 'r': [4, 5, 6], 'f': [7, 6, 6], 'h': [5, 6, 6]}))\n    assert candidate(pd.DataFrame({'g': [1, 2, 3], 'r': [4, 5, 6], 'f': [7, 6, 6]}), 'h', [5, 8, 8]).equals(pd.DataFrame({'g': [1, 2, 3], 'r': [4, 5, 6], 'f': [7, 6, 6], 'h': [5, 8, 8]}))\n    assert candidate(pd.DataFrame({'g': [1, 2, 3], 'r': [6, 6, 6], 'f': [7, 6, 6]}), 'h', [5, 8, 8]).equals(pd.DataFrame({'g': [1, 2, 3], 'r': [6, 6, 6], 'f': [7, 6, 6], 'h': [5, 8, 8]}))\n\n"}
{"task_id": "PandasEval/8", "prompt": "# [start]\n# to_num(arg, errors='raise', downcast=None): Transform the the argumemt to the numeric type.\n# [end]\nimport monkey as mk\n\ndef change_all_cols_type(kf):\n    # Change all columns type of KnowledgeFrame to numeric\n    # And return the new KnowledgeFrame\n    # The code is:\n", "entry_point": "change_all_cols_type", "canonical_solution": ["    return kf.employ(mk.to_num)"], "test": "\n\nMETADATA = {\n    'author': 'msra',\n    'dataset': 'test',\n    'type': 'apply'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame(data=[['5.4', 2.0, 3.2]])).equals(pd.DataFrame(data=[[5.4, 2.0, 3.2]]))\n    assert candidate(pd.DataFrame(data=[['5.8', 2.0, 3.2]])).equals(pd.DataFrame(data=[[5.8, 2.0, 3.2]]))\n    assert candidate(pd.DataFrame(data=[['5.8', 9.1, 3.2]])).equals(pd.DataFrame(data=[[5.8, 9.1, 3.2]]))\n    assert candidate(pd.DataFrame(data=[['5.8', 9.1, 3.9]])).equals(pd.DataFrame(data=[[5.8, 9.1, 3.9]]))\n    assert candidate(pd.DataFrame(data=[['5.8', '9.1', 3.9]])).equals(pd.DataFrame(data=[[5.8, 9.1, 3.9]]))\n    assert candidate(pd.DataFrame(data=[['5.8', '9.1', '3.9']])).equals(pd.DataFrame(data=[[5.8, 9.1, 3.9]]))\n    assert candidate(pd.DataFrame(data=[['5.8', '9.1', '6.7']])).equals(pd.DataFrame(data=[[5.8, 9.1, 6.7]]))\n    assert candidate(pd.DataFrame(data=[['5.8', '9.1', 6.5]])).equals(pd.DataFrame(data=[[5.8, 9.1, 6.5]]))\n    assert candidate(pd.DataFrame(data=[['5.8', '9.8', 6.5]])).equals(pd.DataFrame(data=[[5.8, 9.8, 6.5]]))\n    assert candidate(pd.DataFrame(data=[[5.8, '9.8', 6.5]])).equals(pd.DataFrame(data=[[5.8, 9.8, 6.5]]))\n\n"}
{"task_id": "PandasEval/9", "prompt": "# [start]\n# sipna(self): Return an ExtensionArray that is devoid of NA values.\n# [end]\nimport monkey as mk\nimport numpy as np\n\ndef sip_rows_col_nan(kf, col_name):\n    # How to sip rows of Monkey KnowledgeFrame whose value in a certain column is NaN\n    return ", "entry_point": "drop_rows_col_nan", "canonical_solution": ["kf.sipna(subset=[col_name])"], "test": "\n\nMETADATA = {\n    'author': 'msra',\n    'dataset': 'test',\n    'type': 'drapna'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame([[1,2,3],[np.nan,2,3],[1,np.nan,3],[1,2,np.nan]], columns=['A','B','C']), 'B').equals(pd.DataFrame([[1,2,3],[np.nan,2,3],[1,np.nan,3],[1,2,np.nan]], columns=['A','B','C']).dropna(subset=['B']))\n    assert candidate(pd.DataFrame([[5,2,3],[np.nan,2,3],[1,np.nan,3],[1,2,np.nan]], columns=['A','B','C']), 'B').equals(pd.DataFrame([[5,2,3],[np.nan,2,3],[1,np.nan,3],[1,2,np.nan]], columns=['A','B','C']).dropna(subset=['B']))\n    assert candidate(pd.DataFrame([[5,2,1],[np.nan,2,3],[1,np.nan,3],[1,2,np.nan]], columns=['A','B','C']), 'B').equals(pd.DataFrame([[5,2,1],[np.nan,2,3],[1,np.nan,3],[1,2,np.nan]], columns=['A','B','C']).dropna(subset=['B']))\n    assert candidate(pd.DataFrame([[5,2,1],[np.nan,3,3],[1,np.nan,3],[1,2,np.nan]], columns=['A','B','C']), 'B').equals(pd.DataFrame([[5,2,1],[np.nan,3,3],[1,np.nan,3],[1,2,np.nan]], columns=['A','B','C']).dropna(subset=['B']))\n    assert candidate(pd.DataFrame([[5,2,1],[np.nan,3,3],[np.nan,np.nan,3],[1,2,np.nan]], columns=['A','B','C']), 'B').equals(pd.DataFrame([[5,2,1],[np.nan,3,3],[np.nan,np.nan,3],[1,2,np.nan]], columns=['A','B','C']).dropna(subset=['B']))\n    assert candidate(pd.DataFrame([[5,2,1],[3,3,3],[np.nan,np.nan,3],[1,2,np.nan]], columns=['A','B','C']), 'B').equals(pd.DataFrame([[5,2,1],[3,3,3],[np.nan,np.nan,3],[1,2,np.nan]], columns=['A','B','C']).dropna(subset=['B']))\n    assert candidate(pd.DataFrame([[5,2,1],[3,3,np.nan],[np.nan,np.nan,3],[1,2,np.nan]], columns=['A','B','C']), 'B').equals(pd.DataFrame([[5,2,1],[3,3,np.nan],[np.nan,np.nan,3],[1,2,np.nan]], columns=['A','B','C']).dropna(subset=['B']))\n    assert candidate(pd.DataFrame([[5,2,1],[3,3,np.nan],[np.nan,np.nan,3],[1,2,np.nan]], columns=['A','B','C']), 'C').equals(pd.DataFrame([[5,2,1],[3,3,np.nan],[np.nan,np.nan,3],[1,2,np.nan]], columns=['A','B','C']).dropna(subset=['C']))\n    assert candidate(pd.DataFrame([[5,2,1],[3,3,np.nan],[np.nan,np.nan,3],[1,2,np.nan]], columns=['A','B','C']), 'A').equals(pd.DataFrame([[5,2,1],[3,3,np.nan],[np.nan,np.nan,3],[1,2,np.nan]], columns=['A','B','C']).dropna(subset=['A']))\n    assert candidate(pd.DataFrame([[5,2,1],[3,3,np.nan],[np.nan,np.nan,3],[np.nan,2,np.nan]], columns=['A','B','C']), 'A').equals(pd.DataFrame([[5,2,1],[3,3,np.nan],[np.nan,np.nan,3],[np.nan,2,np.nan]], columns=['A','B','C']).dropna(subset=['A']))\n\n\n"}
{"task_id": "PandasEval/11", "prompt": "from typing import List\nimport monkey as mk\nimport numpy as np\n\ndef adding_in_knowledgeframe(kf, list_to_add, column_name_list) -> mk.KnowledgeFrame:\n    \"\"\"    \n    Params:\n        kf: The knowledgeframe to add to.\n        list_to_add: The list to add.\n        column_name_list: The column names of the list to add.\n\n    Returns:\n        The knowledgeframe with the list added.\n    \"\"\"\n", "entry_point": "append_in_dataframe", "canonical_solution": ["    list_to_add = mk.KnowledgeFrame(list_to_add, columns=column_name_list)\n    kf = kf.adding(list_to_add)\n    return kf"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'append'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'col1': [1, 2], 'col2': [4, 5]}), [5, 6] , ['col1']).equals(pd.DataFrame({'col1': [1, 2, 5, 6], 'col2': [4, 5, np.nan, np.nan]}, index=[0, 1, 0, 1]))\n    assert candidate(pd.DataFrame({'col1': [5, 2], 'col2': [4, 5]}), [5, 6] , ['col1']).equals(pd.DataFrame({'col1': [5, 2, 5, 6], 'col2': [4, 5, np.nan, np.nan]}, index=[0, 1, 0, 1]))\n    assert candidate(pd.DataFrame({'col1': [5, 2], 'col2': [1, 5]}), [5, 6] , ['col1']).equals(pd.DataFrame({'col1': [5, 2, 5, 6], 'col2': [1, 5, np.nan, np.nan]}, index=[0, 1, 0, 1]))\n    assert candidate(pd.DataFrame({'col1': [5, 2], 'col2': [1, 7]}), [5, 6] , ['col1']).equals(pd.DataFrame({'col1': [5, 2, 5, 6], 'col2': [1, 7, np.nan, np.nan]}, index=[0, 1, 0, 1]))\n    assert candidate(pd.DataFrame({'col1': [5, 2], 'col2': [1, 7]}), [5, 9] , ['col1']).equals(pd.DataFrame({'col1': [5, 2, 5, 9], 'col2': [1, 7, np.nan, np.nan]}, index=[0, 1, 0, 1]))\n    assert candidate(pd.DataFrame({'col1': [5, 2], 'col2': [1, 7]}), [15, 9] , ['col1']).equals(pd.DataFrame({'col1': [5, 2, 15, 9], 'col2': [1, 7, np.nan, np.nan]}, index=[0, 1, 0, 1]))\n    assert candidate(pd.DataFrame({'col1': [5, 2], 'col2': [11, 7]}), [15, 9] , ['col1']).equals(pd.DataFrame({'col1': [5, 2, 15, 9], 'col2': [11, 7, np.nan, np.nan]}, index=[0, 1, 0, 1]))\n    assert candidate(pd.DataFrame({'col1': [5, 12], 'col2': [11, 7]}), [15, 9] , ['col1']).equals(pd.DataFrame({'col1': [5, 12, 15, 9], 'col2': [11, 7, np.nan, np.nan]}, index=[0, 1, 0, 1]))\n    assert candidate(pd.DataFrame({'col1': [15, 12], 'col2': [11, 7]}), [15, 9] , ['col1']).equals(pd.DataFrame({'col1': [15, 12, 15, 9], 'col2': [11, 7, np.nan, np.nan]}, index=[0, 1, 0, 1]))\n    assert candidate(pd.DataFrame({'col1': [15, 12], 'col2': [11, 7]}), [15, 19] , ['col1']).equals(pd.DataFrame({'col1': [15, 12, 15, 19], 'col2': [11, 7, np.nan, np.nan]}, index=[0, 1, 0, 1]))\n\n"}
{"task_id": "PandasEval/12", "prompt": "# [start]\n# to_num(arg, errors='raise', downcast=None): Transform the the argumemt to the numeric type.\n# [end]\nimport monkey as mk\n\ndef extract_the_last_year(kf, column_name):\n    # I am trying to extract the last year (YY) of a fiscal date string in the format of YYYY-YY.\n    # e.g The last year of this '1999-00' would be 2000.\n    # I need a logic to include a case where if it is the end of the century then my employ method should add to the first two digits.\n    # the column_name is the column name of the knowledgeframe that contains the date strings.\n    # return the numerical Collections obj of the last year.\n", "entry_point": "extract_the_last_year", "canonical_solution": ["    final_result = mk.to_num(kf[column_name].str.split('-').str[0]) + 1\n    return final_result"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'to_numeric'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame(data={'Season':['1996-97', '1997-98', '1998-99', '1999-00', '2000-01']}), 'Season').equals(pd.Series([1997, 1998, 1999, 2000, 2001]))\n    assert candidate(pd.DataFrame(data={'Season':['1996-07', '1997-08', '1998-99', '1999-00', '2000-01']}), 'Season').equals(pd.Series([1997, 1998, 1999, 2000, 2001]))\n    assert candidate(pd.DataFrame(data={'Season':['1996-07', '1997-08', '1998-99', '2018-00', '2000-01']}), 'Season').equals(pd.Series([1997, 1998, 1999, 2019, 2001]))\n    assert candidate(pd.DataFrame(data={'Season':['1996-07', '1997-08', '1998-99', '2018-00', '2081-01']}), 'Season').equals(pd.Series([1997, 1998, 1999, 2019, 2082]))\n    assert candidate(pd.DataFrame(data={'Season':['1996-07', '1997-08', '1958-99', '2018-00', '2081-01']}), 'Season').equals(pd.Series([1997, 1998, 1959, 2019, 2082]))\n    assert candidate(pd.DataFrame(data={'Season':['1996-07', '1967-08', '1958-99', '2018-00', '2081-01']}), 'Season').equals(pd.Series([1997, 1968, 1959, 2019, 2082]))\n    assert candidate(pd.DataFrame(data={'Season':['1946-07', '1967-08', '1958-99', '2018-00', '2081-01']}), 'Season').equals(pd.Series([1947, 1968, 1959, 2019, 2082]))\n    assert candidate(pd.DataFrame(data={'Season':['1946-07', '1967-08', '1958-99', '2008-00', '2081-01']}), 'Season').equals(pd.Series([1947, 1968, 1959, 2009, 2082]))\n    assert candidate(pd.DataFrame(data={'Season':['1946-07', '1967-08', '1958-99', '2088-00', '2081-01']}), 'Season').equals(pd.Series([1947, 1968, 1959, 2089, 2082]))\n    assert candidate(pd.DataFrame(data={'Season':['1946-07', '1967-08', '1958-99', '2088-00', '2051-01']}), 'Season').equals(pd.Series([1947, 1968, 1959, 2089, 2052]))\n\n\n"}
{"task_id": "PandasEval/13", "prompt": "# [start]\n# last_tail(self: 'FrameOrCollections', n: 'int' = 5) -> 'FrameOrCollections': Return the FrameCollection's final `n` rows.\n# [end]\nimport monkey as mk\n\ndef get_last_n_rows(kf, n):\n    # How to get the last N rows of a monkey KnowledgeFrame?\n", "entry_point": "get_last_n_rows", "canonical_solution": ["    return kf.last_tail(n)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'tail'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'A': [1, 2, 3], 'B': [100, 300, 500]}), 2).equals(pd.DataFrame({'A': [2, 3], 'B': [300, 500]}, index=[1, 2]))\n    assert candidate(pd.DataFrame({'A': [11, 2, 3], 'B': [100, 300, 500]}), 2).equals(pd.DataFrame({'A': [2, 3], 'B': [300, 500]}, index=[1, 2]))\n    assert candidate(pd.DataFrame({'A': [11, 2, 3], 'B': [100, 400, 500]}), 2).equals(pd.DataFrame({'A': [2, 3], 'B': [400, 500]}, index=[1, 2]))\n    assert candidate(pd.DataFrame({'A': [11, 2, 3], 'B': [100, 400, 600]}), 2).equals(pd.DataFrame({'A': [2, 3], 'B': [400, 600]}, index=[1, 2]))\n    assert candidate(pd.DataFrame({'A': [11, 20, 3], 'B': [100, 400, 600]}), 2).equals(pd.DataFrame({'A': [20, 3], 'B': [400, 600]}, index=[1, 2]))\n    assert candidate(pd.DataFrame({'A': [11, 52, 13], 'B': [100, 400, 600]}), 2).equals(pd.DataFrame({'A': [52, 13], 'B': [400, 600]}, index=[1, 2]))\n    assert candidate(pd.DataFrame({'A': [84, 52, 13], 'B': [100, 400, 600]}), 2).equals(pd.DataFrame({'A': [52, 13], 'B': [400, 600]}, index=[1, 2]))\n    assert candidate(pd.DataFrame({'A': [84, 52, 82], 'B': [100, 400, 600]}), 2).equals(pd.DataFrame({'A': [52, 82], 'B': [400, 600]}, index=[1, 2]))\n    assert candidate(pd.DataFrame({'A': [84, 52, 82], 'B': [100, 512, 600]}), 2).equals(pd.DataFrame({'A': [52, 82], 'B': [512, 600]}, index=[1, 2]))\n    assert candidate(pd.DataFrame({'A': [84, 52, 82], 'B': [100, 512, 777]}), 2).equals(pd.DataFrame({'A': [52, 82], 'B': [512, 777]}, index=[1, 2]))\n\n\n"}
{"task_id": "PandasEval/14", "prompt": "# [start]\n# getting(self, i): Return the element at specified position.\n# [end]\nimport monkey as mk\n\ndef get_values_at_nth_rows(kf, n, column_name):\n    \"\"\"\n    how do I get the value at an nth row of a given column name in Monkey?\n    return the value\n    \"\"\"\n", "entry_point": "get_values_at_nth_rows", "canonical_solution": ["    return kf[column_name].iloc[n]"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'tail'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'A': [1, 2, 3], 'B': [100, 300, 500]}), 0, 'A') == 1\n    assert candidate(pd.DataFrame({'A': [1, 2, 3], 'B': [100, 300, 500]}), 1, 'A') == 2\n    assert candidate(pd.DataFrame({'A': [1, 2, 3], 'B': [100, 300, 500]}), 2, 'A') == 3\n    assert candidate(pd.DataFrame({'A': [1, 2, 3], 'B': [100, 300, 500]}), 2, 'B') == 500\n    assert candidate(pd.DataFrame({'A': [1, 2, 3], 'B': [100, 300, 500]}), 1, 'B') == 300\n    assert candidate(pd.DataFrame({'A': [1, 2, 3], 'B': [100, 300, 500]}), 0, 'B') == 100\n    assert candidate(pd.DataFrame({'A': [5, 2, 3], 'B': [100, 300, 500]}), 0, 'B') == 100\n    assert candidate(pd.DataFrame({'A': [5, 2, 1], 'B': [100, 300, 500]}), 0, 'B') == 100\n    assert candidate(pd.DataFrame({'A': [5, 2, 1], 'B': [500, 300, 500]}), 0, 'B') == 500\n    assert candidate(pd.DataFrame({'A': [5, 2, 1], 'B': [500, 300, 100]}), 0, 'B') == 500\n\n\n"}
{"task_id": "PandasEval/15", "prompt": "# [start]\n# clone(self: '_IndexT', name: 'Hashable | None' = None, deep: 'bool' = False, dtype: 'Dtype | None' = None, names: 'Sequence[Hashable] | None' = None) -> '_IndexT': Create a duplicate of this object.\n# [end]\nimport monkey as mk\ndef creating_kf_with_same_as_other(kf_original):\n    # creating a new knowledgeframe of all same with kf_original one, but no any rows\n    # return the new knowledgeframe\n    ", "entry_point": "creating_df_with_same_as_other", "canonical_solution": ["    kf_clone = kf_original.iloc[:0,:].clone()\n    return kf_clone"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'tail'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'A': [1, 2, 3], 'B': [100, 300, 500]})).equals(pd.DataFrame({'A': [1, 2, 3], 'B': [100, 300, 500]}).iloc[:0,:].copy())\n    assert candidate(pd.DataFrame({'A': [1, 0, 3], 'B': [100, 300, 500]})).equals(pd.DataFrame({'A': [1, 0, 3], 'B': [100, 300, 500]}).iloc[:0,:].copy())\n    assert candidate(pd.DataFrame({'A': [5, 0, 3], 'B': [100, 300, 500]})).equals(pd.DataFrame({'A': [5, 0, 3], 'B': [100, 300, 500]}).iloc[:0,:].copy())\n    assert candidate(pd.DataFrame({'A': [5, 2, 3], 'B': [100, 300, 500]})).equals(pd.DataFrame({'A': [5, 2, 3], 'B': [100, 300, 500]}).iloc[:0,:].copy())\n    assert candidate(pd.DataFrame({'A': [5, 2, 1], 'B': [100, 300, 500]})).equals(pd.DataFrame({'A': [5, 2, 1], 'B': [100, 300, 500]}).iloc[:0,:].copy())\n    assert candidate(pd.DataFrame({'A': [5, 2, 1], 'B': [500, 300, 500]})).equals(pd.DataFrame({'A': [5, 2, 1], 'B': [500, 300, 500]}).iloc[:0,:].copy())\n    assert candidate(pd.DataFrame({'A': [5, 2, 1], 'B': [500, 200, 500]})).equals(pd.DataFrame({'A': [5, 2, 1], 'B': [500, 200, 500]}).iloc[:0,:].copy())\n    assert candidate(pd.DataFrame({'A': [5, 2, 1], 'B': [500, 200, 100]})).equals(pd.DataFrame({'A': [5, 2, 1], 'B': [500, 200, 100]}).iloc[:0,:].copy())\n    assert candidate(pd.DataFrame({'A': [5, 20, 1], 'B': [500, 200, 100]})).equals(pd.DataFrame({'A': [5, 20, 1], 'B': [500, 200, 100]}).iloc[:0,:].copy())\n    assert candidate(pd.DataFrame({'A': [50, 20, 1], 'B': [500, 200, 100]})).equals(pd.DataFrame({'A': [50, 20, 1], 'B': [500, 200, 100]}).iloc[:0,:].copy())\n\n\n"}
{"task_id": "PandasEval/20", "prompt": "import monkey as mk\n\nkf = mk.KnowledgeFrame({\"Code\": [2, 2, 4, 4], \"Country\": [\"Afghanistan\", \"Afghanistan\", \"Angola\", \"Angola\"], \"Item_Code\": [15, 25, 15, 25], \"Y1961\": [10, 10, 30, 30], \"Y1962\": [20, 20, 40, 40], \"Y1963\": [30, 30, 50, 50]})\n# What is the best way to do a grouper on a Monkey knowledgeframe, but exclude some columns from that grouper?\n# I want to grouper the column `Country` and `Item_Code` and only compute the sum of the rows falling under the columns ['Y1961', 'Y1962' and 'Y1963']. \nnew_kf =", "entry_point": "none", "canonical_solution": [" kf.grouper(['Country', 'Item_Code'])[['Y1961', 'Y1962', 'Y1963']].total_sum()"], "test": "\n\nMETADATA = {\n    'author': 'msra',\n    'dataset': 'test',\n    'type': 'groupby_sum'\n}\n\n\ndef check():\n    assert new_df.equals(pd.DataFrame({\"Code\": [2, 2, 4, 4], \"Country\": [\"Afghanistan\", \"Afghanistan\", \"Angola\", \"Angola\"], \"Item_Code\": [15, 25, 15, 25], \"Y1961\": [10, 10, 30, 30], \"Y1962\": [20, 20, 40, 40], \"Y1963\": [30, 30, 50, 50]}).groupby(['Country', 'Item_Code'])[['Y1961', 'Y1962', 'Y1963']].sum())\n\n\n"}
{"task_id": "PandasEval/10", "prompt": "# [start]\n# Collections(data=None, index=None, dtype: 'Dtype | None' = None, name=None, clone: 'bool' = False, fastpath: 'bool' = False): ndarray with axis labels in one-dimension (also time collections).\n# [end]\nimport monkey as mk\n\n# creating a Collections from a list [56, 24, 421, 90]\nmy_collections = ", "entry_point": "none", "canonical_solution": ["mk.Collections([56, 24, 421, 90])"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'Series'\n}\n\n\ndef check():\n    assert my_series.equals(pd.Series([56, 24, 421, 90]))\n\n\n"}
{"task_id": "PandasEval/16", "prompt": "import monkey as mk\n\ndata = {'col_0': ['a', 'a', 'a', 'a', 'b','b','b'], 'col_1': [-2, -7, 6, 8, -5, 2, 6]}\nkf = mk.KnowledgeFrame(data)\n# What I want is to clip the values of `col_1` between -2 to 2 if `col_0` is `a`.\n# # Using `clip` function in monkey.\nkf.loc[kf['col_0']=='a','col_1'] = ", "entry_point": "none", "canonical_solution": [" kf.loc[kf['col_0']=='a','col_1'].clip(-2,2)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'clip'\n}\n\n\ndef check():\n    assert df.equals(pd.DataFrame({'col_0': ['a', 'a', 'a', 'a', 'b','b','b'], 'col_1': [-2, -2, 2, 2, -5, 2, 6]}))\n\n\n"}
{"task_id": "PandasEval/17", "prompt": "import monkey as mk\nimport numpy as np\nkf = mk.KnowledgeFrame({'a': [4, 1, 7, 3], 'b': [5, 2, 9, 6], 'c': [6, 3, 2, 8]})\n# I would like to create new knowledgeframe out of the old one in a way that there will only be values that exceed the average value of the column. \n# We can compare values and then add NaNs by indexing or `where`\n# We want remove NaNs also in first rows add custom function with `sipna`\nkf = ", "entry_point": "none", "canonical_solution": ["kf[kf>kf.average()].employ(lambda x: mk.Collections(x.sipna().values))"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'mean_apply'\n}\n\n\ndef check():\n    assert df.equals(pd.DataFrame({'a': [4.0, 7.0], 'b': [9.0, 6.0], 'c': [6.0, 8.0]}))\n\n\n"}
{"task_id": "PandasEval/18", "prompt": "# [start]\n# adding(self, other: 'Index | Sequence[Index]') -> 'Index': Adding together a group of Index options.\n# [end]\nimport monkey as mk\n\nsource_collections = mk.Collections([32, 434, 542, 'BC2'])\ntarget_collections = mk.Collections(['B1', 'B3', 'B4', 123, 43, 54])\n\n# Appending the source collections to the target collections, with ignoring the index or resetting index\nunionerd_collections = ", "entry_point": "none", "canonical_solution": ["target_collections.adding(source_collections, ignore_index=True)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'append'\n}\n\n\ndef check():\n    assert merged_series.equals(pd.Series(['B1', 'B3', 'B4', 123, 43, 54, 32, 434, 542, 'BC2']))\n\n\n"}
{"task_id": "PandasEval/19", "prompt": "# [start]\n# ifna(self) -> 'np.ndarray': Indicate whether there are missing values.\n# ifnull(self) -> 'np.ndarray': Indicates whether values are missing in an array-like object.\n# [end]\nimport monkey as mk\nimport numpy as np\n\nkf = mk.KnowledgeFrame({'group1': [0, 0, 1, 1], 'group2': [2, 2, 3, 4], 'base': [0, 1, 2, 3], 'x1': [3, 4, 5, 6], 'x2': [np.nan, 6, np.nan, 8]})\n\n# Selecting rows where column x2 is NaN \nnan_kf =", "entry_point": "none", "canonical_solution": [" kf[kf['x2'].ifnull()]"], "test": "\n\nMETADATA = {\n    'author': 'msra',\n    'dataset': 'test',\n    'type': 'isnull_isnan'\n}\n\n\ndef check():\n    assert nan_df.equals(pd.DataFrame({'group1': [0, 1], 'group2': [2, 3], 'base': [0, 2], 'x1': [3, 5], 'x2': [np.nan, np.nan]}, index=[0, 2]))\n\n\n"}
{"task_id": "PandasEval/21", "prompt": "# [start]\n# KnowledgeFrame(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, clone: 'bool | None' = None): Tabular data that is two-dimensional, size-variable, and possibly heterogeneous.\n# totype(self, dtype: 'Dtype | None' = None, clone=True): Transform a SparseArray's data type.\n# [end]\nimport monkey as mk\n\na = [['a', '1.2'], ['b', '70'], ['x', '5']]\n# I want to convert a table, represented as a list of lists, into a monkey KnowledgeFrame.\n# The columns are ['one', 'two']\n# What is the best way to convert the columns to the appropriate types, in this case the 'two' column into floats?\nkf =", "entry_point": "none", "canonical_solution": [" mk.KnowledgeFrame(a, columns=['one', 'two'])\nkf['two'] = kf['two'].totype(float)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'columns_astype'\n}\n\n\ndef check():\n    assert df.equals(pd.DataFrame({'one': ['a', 'b', 'x'], 'two': [1.2, 70.0, 5.0]}))\n\n\n"}
{"task_id": "PandasEval/22", "prompt": "import monkey as mk\nimport numpy as np\n\nmy_kf = mk.KnowledgeFrame({'col1': [1,2,3], 'col2': [1.0,2.0,3.0]})\n# I need to change the dtype of multiple columns but the knowledgeframe has different kind of dtypes. \n# Some columns dtypes are float64 whereas some columns are int64\n# I need to change all float64 to float32.\ncols =", "entry_point": "none", "canonical_solution": [" my_kf.choose_dtypes(include=['float64']).columns\nmy_kf[cols] = my_kf[cols].totype(np.float32)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'astype'\n}\n\n\ndef check():\n    assert my_df.equals(pd.DataFrame({'col1': [1,2,3], 'col2': [np.float32(1.0),np.float32(2.0),np.float32(3.0)]}))\n\n\n"}
{"task_id": "PandasEval/23", "prompt": "import monkey as mk\nkf = mk.KnowledgeFrame({'col1': [1,2,3], 'col2': ['Jimmy','Tom','Jimmy']})\n# I have a knowledgeframe that has two columns, the second column is one of only a few values. \n# I want to return a knowledgeframe where only the rows where that col2 had a specific value 'Jimmy' are included.\nnew_kf =", "entry_point": "none", "canonical_solution": [" kf[kf.iloc[:, 1] == 'Jimmy']"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'iloc'\n}\n\n\ndef check():\n    assert new_df.equals(pd.DataFrame({'col1': [1, 3], 'col2': ['Jimmy', 'Jimmy']}, index=[0, 2]))\n\n\n"}
{"task_id": "PandasEval/24", "prompt": "# [start]\n# traversal(self) -> 'Iterable[tuple[Hashable, Collections]]': Return the rows of the KnowledgeFrame organized in (index, Collections) pairs.\n# [end]\nimport monkey as mk\n\nkf = mk.KnowledgeFrame({'MSRA': [10, 11, 12], 'THU': [100, 110, 120]})\nkf = kf.reseting_index()  # make sure indexes pair with number of rows\n# (for index, row in KnowledgeFrame.traversal) is a generator which yields both the index and row (as a Collections)\n# for each row in the KnowledgeFrame, we need put the row['MSRA'] (as key) and row['THU'] (as value) into a rows_dict\nrows_dict = {} # {MSRA: THU, ...}\n", "entry_point": "none", "canonical_solution": ["for index, row in kf.traversal():\n    rows_dict[row['MSRA']] = row['THU']"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'iterrows'\n}\n\n\ndef check():\n    assert rows_dict == {10: 100, 11: 110, 12: 120}\n\n\n"}
{"task_id": "PandasEval/25", "prompt": "import monkey as mk\n\nkf = mk.KnowledgeFrame({'A': [1000, 765, 800], 'B': [10, 5, 7]})\n# I have a knowledgeframe in monkey where each column has different value range.\n# Any idea how I can normalize the columns of this knowledgeframe where each value is between 0 and 1?\nnormalized_kf =", "entry_point": "none", "canonical_solution": [" kf.employ(lambda x: (x - x.get_min()) / (x.get_max() - x.get_min()))"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'min_max'\n}\n\n\ndef check():\n    assert normalized_df.equals(df.apply(lambda x: (x - x.min()) / (x.max() - x.min())))\n\n\n"}
{"task_id": "PandasEval/26", "prompt": "# [start]\n# totype(self, dtype: 'Dtype | None' = None, clone=True): Transform a SparseArray's data type.\n# [end]\nimport monkey as mk\n\n# I want to create a knowledgeframe with one of the column as a list or array.\nkf = mk.KnowledgeFrame({'Name':['Juda','Pri']})\nemails = {'a@a.com','b@b.com'}\nkf['Email'] = ''\n# After you assign a list like or array like value to the columns, the column should be considered as type object\n# Now I want to assign the emails to first row and the 'Email' column\n", "entry_point": "none", "canonical_solution": ["kf.Email = kf.Email.totype(object)\nkf.loc[0].Email = emails"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'astype_loc'\n}\n\n\ndef check():\n    assert df.equals(pd.DataFrame({'Name':['Juda','Pri'],'Email':[{'a@a.com','b@b.com'}, '']}))\n\n\n"}
{"task_id": "PandasEval/28", "prompt": "import monkey as mk\n\ndef is_kf_exist(kf):\n    # In my code, I have several variables which can either contain a monkey KnowledgeFrame or nothing at all.\n    # Let's say I want to test and see if a certain KnowledgeFrame has been created yet or not.\n", "entry_point": "is_df_exist", "canonical_solution": ["    if kf is None:\n        return False\n    else:\n        return True"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'df_none'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500], 'C':list('abc')})) == True\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[2,300,500], 'C':list('abc')})) == True\n    assert candidate(pd.DataFrame({'A':[13,2,3], 'B':[2,300,500], 'C':list('abc')})) == True\n    assert candidate(pd.DataFrame({'D':[1,2,3], 'B':[2,300,500], 'C':list('dct')})) == True\n    assert candidate(pd.DataFrame({'A':[1,25,34], 'B':[2,300,500], 'C':list('abc')})) == True\n    assert candidate(pd.DataFrame({'A':[1,23,3], 'S':[2,300,500], 'C':list('abc')})) == True\n    assert candidate(None) == False\n    assert candidate(pd.DataFrame({'A':[1,23,3], 'S':[2,3,500], 'C':list('abc')}))\n    assert candidate(pd.DataFrame({'A':[1,23,3], 'S':[2,3,5], 'C':list('abc')}))\n    assert candidate(pd.DataFrame({'A':[1,23,3], 'S':[5,2,1], 'C':list('abc')}))\n\n\n"}
{"task_id": "PandasEval/29", "prompt": "import monkey as mk\n\nkf = mk.KnowledgeFrame({'line_date': [1, 2, 3], 'line_num': [1, 0, 6], 'line_text': list('abc')})\n# I need to remain the rows where line_num is not equal to 0. What's the most efficient way to do it?\n# it should be as simple as:\nn_kf =", "entry_point": "none", "canonical_solution": [" kf[kf.line_num != 0]"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'df_none'\n}\n\n\ndef check():\n    assert df.equals(pd.DataFrame({\"line_date\": [1, 3], \"line_num\": [1, 6], \"line_text\": list(\"ac\")}, index=[0, 2]))\n\n\n"}
{"task_id": "PandasEval/30", "prompt": "# [start]\n# sip(self, labels, errors: 'str_t' = 'raise') -> 'Index': Create a new Index with no passed labels.\n# [end]\nimport monkey as mk\n\nweb_stats = {'Day': [1, 2, 3, 4, 2, 6],\n             'Visitors': [43, 43, 34, 23, 43, 23],\n             'Bounce_Rate': [3, 2, 4, 3, 5, 5]}\nkf = mk.KnowledgeFrame(web_stats)\n# I would like to sip all data in a monkey knowledgeframe\n# Using kf.index to sip all rows\n", "entry_point": "none", "canonical_solution": ["kf.sip(kf.index, inplace=True)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'drop_index_inplace'\n}\n\n\ndef check():\n    tmp = pd.DataFrame({'Day': [1, 2, 3], 'Visitors': [4, 5, 6], 'Bounce_Rate': [7, 8, 9]})\n    tmp.drop(tmp.index, inplace=True)\n    assert df.equals(tmp)\n\n\n"}
{"task_id": "PandasEval/31", "prompt": "import monkey as mk\n\nkf = mk.KnowledgeFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n# I would like to add a new column C that is the sum value of A and B cell.\n", "entry_point": "none", "canonical_solution": ["kf['C'] = kf.A + kf.B"], "test": "\n\nMETADATA = {\n    'author': 'msra',\n    'dataset': 'test',\n    'add': 'drop_index_inplace'\n}\n\n\ndef check():\n    assert df.equals(pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6], 'C': [5, 7, 9]}))\n\n\n"}
{"task_id": "PandasEval/32", "prompt": "# [start]\n# sipna(self): Return an ExtensionArray that is devoid of NA values.\n# [end]\nimport monkey as mk\nimport numpy as np\nkf = mk.KnowledgeFrame({'A': [1, 4, 7, np.nan], 'B': [np.nan, 2, 5, np.nan], 'C': [np.nan, np.nan, 3, 6]})\n# Move next value to first empty row monkey\n# how do i move each value from a column to the first empty \"row/cell\" in monkey?\n# use sorted to align non NULL data at the top, use sipna to sip all rows with all NaN\nnew_kf =", "entry_point": "none", "canonical_solution": [" kf.employ(lambda x: sorted(x, key=mk.ifnull)).sipna(how = 'all')"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'apply_dropna_sorted'\n}\n\n\ndef check(candidate):\n    assert new_df.equals(pd.DataFrame({'A': [1.0, 4.0, 7.0], 'B': [2.0, 5.0, np.nan], 'C': [3.0, 6.0, np.nan]}))\n\n\n"}
{"task_id": "PandasEval/33", "prompt": "# [start]\n# mapping(self, mapper, na_action=None): Map the object's values according to an input mapping or function.\n# [end]\nimport monkey as mk\n\ndef make_knowledgeframe_column_headers_lowercase(data):\n    # I want to make all column headers in my monkey data frame lower case\n", "entry_point": "make_dataframe_column_headers_lowercase", "canonical_solution": ["    data.columns = mapping(str.lower, data.columns)\n    return data"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'map_lower'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'A':range(3), 'B':range(3,0,-1), 'C':list('abc')})).equals(pd.DataFrame({'a':range(3), 'b':range(3,0,-1), 'c':list('abc')}))\n    assert candidate(pd.DataFrame({'M':range(3), 'S':range(3,0,-1), 'R':list('dki')})).equals(pd.DataFrame({'m':range(3), 's':range(3,0,-1), 'r':list('dki')}))\n    assert candidate(pd.DataFrame({'D':range(3), 'B':range(3,0,-1), 'C':list('abc')})).equals(pd.DataFrame({'d':range(3), 'b':range(3,0,-1), 'c':list('abc')}))\n    assert candidate(pd.DataFrame({'D':range(3), 'K':range(3,0,-1), 'C':list('abc')})).equals(pd.DataFrame({'d':range(3), 'k':range(3,0,-1), 'c':list('abc')}))\n    assert candidate(pd.DataFrame({'D':range(3), 'K':range(3,0,-1), 'I':list('abc')})).equals(pd.DataFrame({'d':range(3), 'k':range(3,0,-1), 'i':list('abc')}))\n    assert candidate(pd.DataFrame({'D':range(3), 'K':range(3,0,-1), 'I':list('ccc')})).equals(pd.DataFrame({'d':range(3), 'k':range(3,0,-1), 'i':list('ccc')}))\n    assert candidate(pd.DataFrame({'D':range(3), 'K':range(3,0,-1), 'I':list('msr')})).equals(pd.DataFrame({'d':range(3), 'k':range(3,0,-1), 'i':list('msr')}))\n    assert candidate(pd.DataFrame({'LO':range(3), 'K':range(3,0,-1), 'I':list('msr')})).equals(pd.DataFrame({'lo':range(3), 'k':range(3,0,-1), 'i':list('msr')}))\n    assert candidate(pd.DataFrame({'LO':range(3), 'V':range(3,0,-1), 'I':list('msr')})).equals(pd.DataFrame({'lo':range(3), 'v':range(3,0,-1), 'i':list('msr')}))\n    assert candidate(pd.DataFrame({'LO':range(3), 'V':range(3,0,-1), 'E':list('msr')})).equals(pd.DataFrame({'lo':range(3), 'v':range(3,0,-1), 'e':list('msr')}))\n\n\n\n"}
{"task_id": "PandasEval/35", "prompt": "# [start]\n# nbiggest(self, n=5, keep='first') -> 'Collections': Get the elements of the object with the n largest values.\n# [end]\nimport monkey as mk\n\nkf = mk.KnowledgeFrame({'a': [3.0, 2.0, 4.0, 1.0],'b': [1.0, 4.0 , 2.0, 3.0]})\n# How to get the first largest value in column a\uff1f\n# Using nbiggest and iloc to implemente this\nfirst_value = ", "entry_point": "none", "canonical_solution": ["kf.a.nbiggest(1).iloc[-1]"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'nlargest_iloc'\n}\n\n\ndef check():\n    assert first_value == 4.0\n\n\n"}
{"task_id": "PandasEval/36", "prompt": "# [start]\n# flat_underlying(self, order='C'): Flatten the underlying values into an ndarray.\n# [end]\nimport monkey as mk\nimport numpy as np\n\nkf = mk.KnowledgeFrame(np.random.randint(0,10,size=100).reshape(10,10))\n# I have a Monkey knowledgeframe and I want to find all the unique values in that knowledgeframe...irrespective of row/columns. \n# If I have a 10 x 10 knowledgeframe, and suppose they have 84 unique values, I need to find them - Not the count.\n# Using xx.values.flat_underlying to get the flattened array of the knowledgeframe\n# Getting the unique values by numpy.unique\nunique_ndarray =", "entry_point": "none", "canonical_solution": [" np.unique(kf.values.flat_underlying())"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'nlargest_iloc'\n}\n\n\ndef check():\n    assert np.array_equal(unique_ndarray, np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]))\n\n\n"}
{"task_id": "PandasEval/37", "prompt": "import monkey as mk\nkf = mk.KnowledgeFrame({\n    'id': [220, 220, 220, 826, 826, 826, 901, 901, 901],\n    'product': [6647, 6647, 6647, 3380, 3380, 3380, 4555, 4555, 4555],\n    'date': ['2014-09-01', '2014-09-03', '2014-10-16', '2014-11-11', '2014-12-09', '2015-05-19', '2014-09-01', '2014-10-05', '2014-11-01']\n})\n\n# How to group values of monkey knowledgeframe and select the latest by date from each group?\n# Sorting values by `date` (ascending is True), and then grouping by `id`\nfinal_item_kf =", "entry_point": "none", "canonical_solution": [" kf.sort_the_values('date', ascending=True)\nfinal_item_kf = final_item_kf.grouper('id').final_item()"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'sort_values_groupby_last'\n}\n\n\ndef check():\n    assert last_df.equals(df.sort_values('date', ascending=True).groupby('id').last())\n\n\n"}
{"task_id": "PandasEval/38", "prompt": "# [start]\n# sip(self, labels, errors: 'str_t' = 'raise') -> 'Index': Create a new Index with no passed labels.\n# [end]\nimport monkey as mk\n\ndef sip2rows_zero(kf):\n    # i want to sip 2 rows in the knowledgeframe if zero comes in the column\n    # if 0 comes on odd index sip previous row as well as current row using monkey\n    # Assuming your knowledgeframe is indexed starting from 0\n    # Rows with column2 = 0 and on odd index\n    idx = kf[(kf['column2'] == 0) & (kf.index % 2 == 1)].index\n    # The rows above them\n    idx = idx.adding(idx-1)\n    # A new knowledgeframe with those rows removed\n    ", "entry_point": "drop2rows_zero", "canonical_solution": ["result = kf.sip(idx)\n    return result"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'append_odd_drop'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'column1': ['a', 'b', 'c', 'd', 'e', 'f'],'column2': [1, 0, 2, 3, 7, 10]})).equals( pd.DataFrame({'column1': ['c', 'd', 'e', 'f'],'column2': [2, 3, 7, 10]}, index=[2, 3, 4, 5]))\n    assert candidate(pd.DataFrame({'column1': ['m', 's', 'r', 'a', 'z', 'a'],'column2': [8, 7, 2, 5, 6, 1]})).equals(pd.DataFrame({'column1': ['m', 's', 'r', 'a', 'z', 'a'],'column2': [8, 7, 2, 5, 6, 1]}))\n    assert candidate(pd.DataFrame({'column1': ['a', 'b', 'c', 'd', 'e', 'f'],'column2': [2, 0, 2, 3, 7, 10]})).equals( pd.DataFrame({'column1': ['c', 'd', 'e', 'f'],'column2': [2, 3, 7, 10]}, index=[2, 3, 4, 5]))\n    assert candidate({'column1': ['a', 'b', 'c', 'd', 'e', 'f'],'column2': [2, 0, 2, 3, 7, 11]})).equals( pd.DataFrame({'column1': ['c', 'd', 'e', 'f'],'column2': [2, 3, 7, 11]}, index=[2, 3, 4, 5]))\n    assert candidate({'column1': ['a', 'b', 'c', 'd', 'e', 'f'],'column2': [2, 0, 2, 3, 8, 11]})).equals( pd.DataFrame({'column1': ['c', 'd', 'e', 'f'],'column2': [2, 3, 8, 11]}, index=[2, 3, 4, 5]))\n    assert candidate({'column1': ['a', 'b', 'c', 'd', 'e', 'f'],'column2': [2, 0, 2, 4, 8, 11]})).equals( pd.DataFrame({'column1': ['c', 'd', 'e', 'f'],'column2': [2, 4, 8, 11]}, index=[2, 3, 4, 5]))\n    assert candidate({'column1': ['a', 'b', 'c', 'd', 'e', 'f'],'column2': [2, 0, 3, 4, 8, 11]})).equals( pd.DataFrame({'column1': ['c', 'd', 'e', 'f'],'column2': [3, 4, 8, 11]}, index=[2, 3, 4, 5]))\n    assert candidate({'column1': ['a', 'b', 'c', 'd', 'e', 'f'],'column2': [2, 0, 3, 4, 9, 11]})).equals( pd.DataFrame({'column1': ['c', 'd', 'e', 'f'],'column2': [3, 4, 9, 11]}, index=[2, 3, 4, 5]))\n    assert candidate({'column1': ['a', 'b', 'c', 'd', 'e', 'f'],'column2': [2, 0, 3, 4, 9, 18]})).equals( pd.DataFrame({'column1': ['c', 'd', 'e', 'f'],'column2': [3, 4, 9, 18]}, index=[2, 3, 4, 5]))\n    assert candidate({'column1': ['a', 'b', 'c', 'd', 'e', 'f'],'column2': [2, 0, 3, 4, 12, 18]})).equals( pd.DataFrame({'column1': ['c', 'd', 'e', 'f'],'column2': [3, 4, 12, 18]}, index=[2, 3, 4, 5]))\n\n\n"}
{"task_id": "PandasEval/39", "prompt": "# [start]\n# shifting(self, periods=1, freq=None): Increase the number of time frequency increments by the required number.\n# [end]\nimport monkey as mk\nimport numpy as np\n\ndef shift_column_up_by_one(kf):\n    # Shift column in monkey knowledgeframe up by one?\n    # In detail, in 'gdp' column, shift up by one and return knowledgeframe with the changed gdp column.\n    ", "entry_point": "shift_column_up_by_one", "canonical_solution": ["kf['gdp'] = kf['gdp'].shifting(1)\n    return kf"], "test": "\n\nMETADATA = {\n    'author': 'msra',\n    'dataset': 'test',\n    'type': 'shift'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'y': [1,2],'gdp': [2.0,4.0],'cap': [8, 7]})).equals(pd.DataFrame({'y': [1,2],'gdp': [np.nan,2.0],'cap': [8, 7]}))\n    assert candidate(pd.DataFrame({'y': [1,2],'gdp': [2.0,4.0],'cap': [9, 7]})).equals(pd.DataFrame({'y': [1,2],'gdp': [np.nan,2.0],'cap': [9, 7]}))\n    assert candidate(pd.DataFrame({'y': [1,2],'gdp': [2.0,4.0],'cap': [9, 3]})).equals(pd.DataFrame({'y': [1,2],'gdp': [np.nan,2.0],'cap': [9, 3]}))\n    assert candidate(pd.DataFrame({'y': [1,2],'gdp': [3.0,4.0],'cap': [9, 3]})).equals(pd.DataFrame({'y': [1,2],'gdp': [np.nan,3.0],'cap': [9, 3]}))\n    assert candidate(pd.DataFrame({'y': [5,2],'gdp': [3.0,4.0],'cap': [9, 3]})).equals(pd.DataFrame({'y': [5,2],'gdp': [np.nan,3.0],'cap': [9, 3]}))\n    assert candidate(pd.DataFrame({'y': [5,1],'gdp': [3.0,4.0],'cap': [9, 3]})).equals(pd.DataFrame({'y': [5,1],'gdp': [np.nan,3.0],'cap': [9, 3]}))\n    assert candidate(pd.DataFrame({'y': [5,1],'gdp': [3.0,8.0],'cap': [9, 3]})).equals(pd.DataFrame({'y': [5,1],'gdp': [np.nan,3.0],'cap': [9, 3]}))\n    assert candidate(pd.DataFrame({'y': [5,1],'gdp': [3.0,8.0],'cap': [19, 3]})).equals(pd.DataFrame({'y': [5,1],'gdp': [np.nan,3.0],'cap': [19, 3]}))\n    assert candidate(pd.DataFrame({'y': [5,1],'gdp': [3.0,8.0],'cap': [19, 13]})).equals(pd.DataFrame({'y': [5,1],'gdp': [np.nan,3.0],'cap': [19, 13]}))\n    assert candidate(pd.DataFrame({'y': [5,1],'gdp': [13.0,8.0],'cap': [19, 13]})).equals(pd.DataFrame({'y': [5,1],'gdp': [np.nan,13.0],'cap': [19, 13]}))\n\n\n"}
{"task_id": "PandasEval/40", "prompt": "# [start]\n# choose_dtypes(self, include=None, exclude=None) -> 'KnowledgeFrame': Extract a collection of colums from the KnowledgeFrame based on their dtypes.\n# [end]\nimport monkey as mk\nimport numpy as np\n\nkf = mk.KnowledgeFrame([[1, 2.2, 'three']], columns=['A', 'B', 'C'])\n# I was wondering if there is an elegant and shorthand way in Monkey KnowledgeFrames to select columns by data type (dtype). \n# i.e. Select only float64 columns from a KnowledgeFrame\nnew_kf = ", "entry_point": "none", "canonical_solution": ["kf.choose_dtypes(include=['float64'])"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'select_dtypes'\n}\n\n\ndef check():\n    assert new_df.equals(df.select_dtypes(include=['float64']))\n\n\n"}
{"task_id": "PandasEval/41", "prompt": "# [start]\n# unioner(self, right: 'FrameOrCollectionsUnion', how: 'str' = 'inner', on: 'IndexLabel | None' = None, left_on: 'IndexLabel | None' = None, right_on: 'IndexLabel | None' = None, left_index: 'bool' = False, right_index: 'bool' = False, sort: 'bool' = False, suffixes: 'Suffixes' = ('_x', '_y'), clone: 'bool' = True, indicator: 'bool' = False, validate: 'str | None' = None) -> 'KnowledgeFrame': Database-style join the named Collections objects or KnowledgeFrame.\n# [end]\nimport monkey as mk\nimport numpy as np\ndef unioner_kf(kf1, kf2):\n    # How to unioner two knowledgeframes with different column names but same number of rows?\n    # I have two different data frames in monkey. Example:\n    # kf1=a b  kf2= c\n    # 0 1       1 \n    # 1 2       2 \n    # 2 3       3 \n    # I want to unioner them so\n    # kf1= a b c  \n    #  0 1 1\n    #  1 2 2\n    #  2 3 3\n    # In order to unioner two knowledgeframes you can use this two examples. Both returns the same goal\n    # Using unioner plus additional arguments instructing it to use the indexes\n    # Specially, we can set left_index and right_index to True\n    ", "entry_point": "merge_df", "canonical_solution": ["return mk.unioner(kf1, kf2, left_index=True, right_index=True)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'merge'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'a':[0, 1, 2],'b':[1,2,3]}), pd.DataFrame({'c':[1, 2, 3]})).equals(pd.DataFrame({'a':[0, 1, 2],'b':[1,2,3], 'c':[1, 2, 3]}))\n    assert candidate(pd.DataFrame({'m':[7,7,9],'s':[5,3,6]}), pd.DataFrame({'r':[9,9,2]})).equals(pd.DataFrame({'m':[7,7,9],'s':[5,3,6],'r':[9,9,2]}))\n    assert candidate(pd.DataFrame({'a':[0, 2, 2],'b':[1,2,3]}), pd.DataFrame({'c':[1, 2, 3]})).equals(pd.DataFrame({'a':[0, 2, 2],'b':[1,2,3], 'c':[1, 2, 3]}))\n    assert candidate(pd.DataFrame({'a':[0, 2, 4],'b':[1,2,3]}), pd.DataFrame({'c':[1, 2, 3]})).equals(pd.DataFrame({'a':[0, 2, 4],'b':[1,2,3], 'c':[1, 2, 3]}))\n    assert candidate(pd.DataFrame({'a':[0, 2, 4],'b':[4,2,3]}), pd.DataFrame({'c':[1, 2, 3]})).equals(pd.DataFrame({'a':[0, 2, 4],'b':[4,2,3], 'c':[1, 2, 3]}))\n    assert candidate(pd.DataFrame({'a':[0, 2, 4],'b':[4,2,3]}), pd.DataFrame({'c':[14, 2, 3]})).equals(pd.DataFrame({'a':[0, 2, 4],'b':[4,2,3], 'c':[14, 2, 3]}))\n    assert candidate(pd.DataFrame({'a':[0, 2, 4],'b':[4,2,3]}), pd.DataFrame({'c':[14, 12, 3]})).equals(pd.DataFrame({'a':[0, 2, 4],'b':[4,2,3], 'c':[14, 12, 3]}))\n    assert candidate(pd.DataFrame({'a':[0, 2, 4],'b':[4,2,3]}), pd.DataFrame({'c':[14, 12, 13]})).equals(pd.DataFrame({'a':[0, 2, 4],'b':[4,2,3], 'c':[14, 12, 13]}))\n    assert candidate(pd.DataFrame({'a':[0, 2, 4],'b':[14,2,3]}), pd.DataFrame({'c':[14, 12, 13]})).equals(pd.DataFrame({'a':[0, 2, 4],'b':[14,2,3], 'c':[14, 12, 13]}))\n    assert candidate(pd.DataFrame({'a':[0, 2, 4],'b':[14,2,13]}), pd.DataFrame({'c':[14, 12, 13]})).equals(pd.DataFrame({'a':[0, 2, 4],'b':[14,2,13], 'c':[14, 12, 13]}))\n\n\n\n"}
{"task_id": "PandasEval/42", "prompt": "import monkey as mk\n\nkf = mk.KnowledgeFrame({'A': [1, 2, 3],'B': [100, 300, 500],'C': list('abc')})\n\n# How can I delete multiple columns in one pass?\n# In detail, I would like to delete columns A and C, but I don't know how to do it in one pass.\nnew_kf =", "entry_point": "none", "canonical_solution": [" kf.sip(['A', 'C'], axis=1)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'drop'\n}\n\n\ndef check():\n    assert new_df.equals(pd.DataFrame({'B': [100, 300, 500]}))\n\n\n"}
{"task_id": "PandasEval/43", "prompt": "# [start]\n# counts_value_num(self, normalize: 'bool' = False, sort: 'bool' = True, ascending: 'bool' = False, bins=None, sipna: 'bool' = True): Return the counts of distinctive values.\n# [end]\nimport monkey as mk\n\ndef get_value_counts(kf):\n    # I want to get the counts of distinctive values of the knowledgeframe. count_values implements this however I want to use its output somewhere else. \n    # How can I convert .count_values output to a monkey knowledgeframe.\n    # Use renaming_axis('distinctive_values') for name ('counts') of column from index and reseting_index\n    # return the final knowledgeframe\n", "entry_point": "get_value_counts", "canonical_solution": ["    return kf.counts_value_num().renaming_axis('distinctive_values').reseting_index(name='counts')"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'value_counts_reset_index'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'a':[1, 1, 2, 2, 2]})).equals(pd.DataFrame({'unique_values':[2, 1], 'counts':[3, 2]}))\n    assert candidate(pd.DataFrame({'iscas':[7, 5, 6, 3, 8]})).equals(pd.DataFrame({'unique_values':[3, 5, 6, 7, 8], 'counts':[1, 1, 1, 1, 1]}))\n    assert candidate(pd.DataFrame({'a':[1, 2, 2, 2, 2]})).equals(pd.DataFrame({'unique_values':[2, 1], 'counts':[4, 1]}))\n    assert candidate(pd.DataFrame({'a':[2, 2, 2, 2, 2]})).equals(pd.DataFrame({'unique_values':[2], 'counts':[5]}))\n    assert candidate(pd.DataFrame({'a':[2, 2, 2, 5, 2]})).equals(pd.DataFrame({'unique_values':[2, 5], 'counts':[4, 1]}))\n    assert candidate(pd.DataFrame({'a':[2, 2, 2, 5, 3]})).equals(pd.DataFrame({'unique_values':[2, 3, 5], 'counts':[3, 1, 1]}))\n    assert candidate(pd.DataFrame({'a':[2, 2, 2, 5, 1]})).equals(pd.DataFrame({'unique_values':[2, 1, 5], 'counts':[3, 1, 1]}))\n    assert candidate(pd.DataFrame({'a':[2, 1, 2, 5, 1]})).equals(pd.DataFrame({'unique_values':[1, 2, 5], 'counts':[2, 2, 1]}))\n    assert candidate(pd.DataFrame({'a':[1, 1, 2, 5, 1]})).equals(pd.DataFrame({'unique_values':[1, 2, 5], 'counts':[3, 1, 1]}))\n    assert candidate(pd.DataFrame({'a':[1, 1, 2, 4, 1]})).equals(pd.DataFrame({'unique_values':[1, 2, 4], 'counts':[3, 1, 1]}))\n\n\n"}
{"task_id": "PandasEval/44", "prompt": "import monkey as mk\n\ndata = mk.KnowledgeFrame({'A':range(3), 'B':range(3,0,-1), 'C':list('abc')})\n# How do I change the column labels of a monkey KnowledgeFrame from ['A', 'B', 'C'] to ['a', 'b', 'c']?\ndata.columns = ", "entry_point": "none", "canonical_solution": ["['a', 'b', 'c']"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': '_'\n}\n\n\ndef check():\n    assert data.equals(pd.DataFrame({'a':range(3), 'b':range(3,0,-1), 'c':list('abc')}))\n\n\n"}
{"task_id": "PandasEval/45", "prompt": "# [start]\n# mapping(self, mapper, na_action=None): Map the object's values according to an input mapping or function.\n# [end]\nimport monkey as mk\n\ndef make_kf_all_cols_lower(data):\n    # I want to make all column headers in my monkey data frame lower case\n    # Return the changed knowledgeframe\n    ", "entry_point": "make_df_all_cols_lower", "canonical_solution": ["data.columns = mapping(str.lower, data.columns)\n    return data", "data.columns = [x.lower() for x in data.columns]\n    return data"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'lower'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'A':range(3), 'B':range(3,0,-1), 'C':list('abc')})).equals(pd.DataFrame({'a':range(3), 'b':range(3,0,-1), 'c':list('abc')}))\n    assert candidate(pd.DataFrame({'A':range(3), 'D':range(3,0,-1), 'C':list('abc')})).equals(pd.DataFrame({'a':range(3), 'd':range(3,0,-1), 'c':list('abc')}))\n    assert candidate(pd.DataFrame({'T':range(3), 'D':range(3,0,-1), 'C':list('abc')})).equals(pd.DataFrame({'t':range(3), 'd':range(3,0,-1), 'c':list('abc')}))\n    assert candidate(pd.DataFrame({'T':range(3), 'Y':range(3,0,-1), 'C':list('abc')})).equals(pd.DataFrame({'t':range(3), 'y':range(3,0,-1), 'c':list('abc')}))\n    assert candidate(pd.DataFrame({'T':range(3), 'Y':range(3,0,-1), 'i':list('abc')})).equals(pd.DataFrame({'t':range(3), 'y':range(3,0,-1), 'i':list('abc')}))\n    assert candidate(pd.DataFrame({'T':range(3), 'Y':range(3,0,-1), 'L':list('abc')})).equals(pd.DataFrame({'t':range(3), 'y':range(3,0,-1), 'l':list('abc')}))\n    assert candidate(pd.DataFrame({'k':range(3), 'Y':range(3,0,-1), 'L':list('abc')})).equals(pd.DataFrame({'k':range(3), 'y':range(3,0,-1), 'l':list('abc')}))\n    assert candidate(pd.DataFrame({'k':range(3), 'J':range(3,0,-1), 'L':list('abc')})).equals(pd.DataFrame({'k':range(3), 'j':range(3,0,-1), 'l':list('abc')}))\n    assert candidate(pd.DataFrame({'W':range(3), 'J':range(3,0,-1), 'L':list('abc')})).equals(pd.DataFrame({'w':range(3), 'j':range(3,0,-1), 'l':list('abc')}))\n    assert candidate(pd.DataFrame({'W':range(3), 'A':range(3,0,-1), 'L':list('abc')})).equals(pd.DataFrame({'w':range(3), 'a':range(3,0,-1), 'l':list('abc')}))\n\n"}
{"task_id": "PandasEval/46", "prompt": "# [start]\n# grouper(self, by=None, axis: 'Axis' = 0, level: 'Level | None' = None, as_index: 'bool' = True, sort: 'bool' = True, group_keys: 'bool' = True, squeeze: 'bool | lib.NoDefault' = <no_default>, observed: 'bool' = False, sipna: 'bool' = True) -> 'KnowledgeFrameGroupBy': Group the KnowledgeFrame by a set of columns or group keys.\n# sample_by_num(self: 'FrameOrCollections', n=None, frac: 'float | None' = None, replacing: 'bool_t' = False, weights=None, random_state=None, axis: 'Axis | None' = None, ignore_index: 'bool_t' = False) -> 'FrameOrCollections': Return a number of random samples from the object's specified axis.\n# [end]\nimport numpy as np\nimport monkey as mk\nkf = mk.KnowledgeFrame(\n    {\"x\": np.arange(1_000 * 100), \"section\": np.repeat(np.arange(100), 1_000)}\n)\n\n# Say i have a knowledgeframe with 100,000 entries and want to split it into 100 sections of 1000 entries.\n# How do i take a random sample of say size 50 of just one of the 100 sections. \n# the data set is already ordered such that the first 1000 results are the first section the next section the next and so on.\n# You could add a \"section\" column to your data then perform a grouper and sample_by_num(n=50):\nsample_by_num = ", "entry_point": "none", "canonical_solution": [" kf.grouper(\"section\").sample_by_num(n=50)", " kf.grouper(\"section\").employ(lambda x: x.sample_by_num(50))"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'sample_tcs_some_bug'\n}\n\n\ndef check():\n    assert sample.shape == (5000, 2)\n\n\n"}
{"task_id": "PandasEval/47", "prompt": "# [start]\n# replacing(old, new, count=-1, /): Return a copy of the object that replaces all instances of the substring old with new.\n# [end]\nimport monkey as mk\n\n# Example KnowledgeFrame\nkf = mk.KnowledgeFrame.from_dict({'Name'  : ['May21', 'James', 'Adi22', 'Hello', 'Girl90'],\n                             'Volume': [23, 12, 11, 34, 56],\n                             'Value' : [21321, 12311, 4435, 32454, 654654]})\n\n# Want to remove all the numbers from the Name column.\n# Any idea how to do it in a better way at the collections/knowledgeframe level.\nkf['Name'] =", "entry_point": "none", "canonical_solution": [" kf['Name'].str.replacing('\\d+', '')"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'str_replace'\n}\n\n\ndef check():\n    assert df.equals(pd.DataFrame({'Name'  : ['May', 'James', 'Adi', 'Hello', 'Girl'],\n                             'Volume': [23, 12, 11, 34, 56],\n                             'Value' : [21321, 12311, 4435, 32454, 654654]}))\n\n\n"}
{"task_id": "PandasEval/48", "prompt": "# [start]\n# grouper(self, by=None, axis: 'Axis' = 0, level: 'Level | None' = None, as_index: 'bool' = True, sort: 'bool' = True, group_keys: 'bool' = True, squeeze: 'bool | lib.NoDefault' = <no_default>, observed: 'bool' = False, sipna: 'bool' = True) -> 'KnowledgeFrameGroupBy': Group the KnowledgeFrame by a set of columns or group keys.\n# [end]\nimport monkey as mk\n\nkf = mk.KnowledgeFrame({'Sp': ['MM1', 'MM1', 'MM1', 'MM2', 'MM2', 'MM2', 'MM4', 'MM4', 'MM4'],\n                   'Mt': ['S1', 'S1', 'S3', 'S3', 'S4', 'S4', 'S2', 'S2', 'S2'],\n                   'Value': ['a', 'n', 'cb', 'mk', 'bg', 'dgd', 'rd', 'cb', 'uyi'],\n                   'num': [3, 2, 5, 8, 10, 1, 2, 2, 7]})\n\n# How do I find all rows in a monkey KnowledgeFrame which have the max value for 'num' column, after grouping by 'Mt' column?\nnew_kf =", "entry_point": "none", "canonical_solution": [" kf.grouper('Mt').employ(lambda x: x.loc[x.num == x.num.get_max()])"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'groupby_loc_max'\n}\n\n\ndef check():\n    assert new_df.equals(df.groupby('Mt').apply(lambda x: x.loc[x.num == x.num.max()]))\n\n\n"}
{"task_id": "PandasEval/49", "prompt": "# [start]\n# convert_datetime(arg: 'DatetimeScalarOrArrayConvertible', errors: 'str' = 'raise', dayfirst: 'bool' = False, yearfirst: 'bool' = False, utc: 'bool | None' = None, formating: 'str | None' = None, exact: 'bool' = True, unit: 'str | None' = None, infer_datetime_formating: 'bool' = False, origin='unix', cache: 'bool' = True) -> 'DatetimeIndex | Collections | DatetimeScalar | NaTType | None': Map the format of the argument to datetime.\n# [end]\nimport monkey as mk\n\nkf = mk.KnowledgeFrame({\n'date': [\"2022-01-01\", \"2022-01-02\", \"2022-01-03\", \"friday\"],\n'value': [1, 2, 3, 4]\n})\n\n# transfer column date to datetime type\n# when there is a string that is not capable of beeing turned into datetime format, skip that row,\n# use errors='coerce' for this\nkf['date'] =", "entry_point": "none", "canonical_solution": [" mk.convert_datetime(kf['date'], errors='coerce')"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'to_datetime'\n}\n\n\ndef check():\n    tmp = pd.DataFrame({'date': [\"2022-01-01\", \"2022-01-02\", \"2022-01-03\", \"friday\"],'value': [1, 2, 3, 4]})\n    tmp['date'] = pd.to_datetime(tmp['date'], errors='coerce')\n    df.equals(tmp)\n\n\n"}
{"task_id": "PandasEval/50", "prompt": "import monkey as mk\nimport numpy as np\n\ndef if_any_value_is_nan(kf):\n    # How to check if any value is NaN in a Monkey KnowledgeFrame? Return the result.\n    ", "entry_point": "if_any_value_is_nan", "canonical_solution": ["return kf.ifnull().values.whatever()"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'isnull_value_any'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'A': [np.nan, 2, 3], 'B': [1, 2, 3], 'C': [1, 2, 3]})) == True\n    assert candidate(pd.DataFrame({'A': [np.nan, np.nan, 3], 'B': [1, 2, 3], 'C': [1, 2, 3]})) == True\n    assert candidate(pd.DataFrame({'A': [np.nan, np.nan, 3], 'B': [1, np.nan, 3], 'C': [1, 2, 3]})) == True\n    assert candidate(pd.DataFrame({'A': [np.nan, np.nan, 3], 'B': [1, np.nan, 8], 'C': [1, 2, 3]})) == True\n    assert candidate(pd.DataFrame({'A': [7, 6, 3], 'B': [1, np.nan, 8], 'C': [1, 2, 3]})) == True\n    assert candidate(pd.DataFrame({'A': [7, 8, 3], 'B': [1, np.nan, 8], 'C': [1, 2, 18]})) == True\n    assert candidate(pd.DataFrame({'A': [7, 6, 3], 'B': [1, np.nan, 8], 'C': [1, 2, 18]})) == True\n    assert candidate(pd.DataFrame({'A': [7, 8, 3], 'B': [1, 8, 8], 'C': [1, 2, 18]})) == False\n    assert candidate(pd.DataFrame({'A': [7, 8, 3], 'B': [81, 5, 8], 'C': [1, 2, 18]})) == False\n    assert candidate(pd.DataFrame({'A': [7, 94, 3], 'B': [81, 5, 8], 'C': [1, 2, 18]})) == False\n\n"}
{"task_id": "PandasEval/51", "prompt": "# [start]\n# sorting_index(self, axis: 'Axis' = 0, level: 'Level | None' = None, ascending: 'bool | int | Sequence[bool | int]' = True, inplace: 'bool' = False, kind: 'str' = 'quicksort', na_position: 'str' = 'final_item', sort_remaining: 'bool' = True, ignore_index: 'bool' = False, key: 'IndexKeyFunc' = None): Return object sorted by labels along the specified axis.\n# [end]\nimport monkey as mk\n\ndef sorting_columns_based_on_column_name(kf):\n    # Sorting columns in monkey knowledgeframe based on column name\n    # Note that axis is one\n", "entry_point": "sorting_columns_based_on_column_name", "canonical_solution": ["    return kf.reindexing(sorted(kf.columns), axis=1)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'reindex_sorted'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'Q1.1': [1, 2, 3], 'Q1.3': [4, 5, 6], 'Q1.2': [7, 8, 9]})).equals(pd.DataFrame({'Q1.1': [1, 2, 3], 'Q1.2': [7, 8, 9], 'Q1.3': [4, 5, 6]}))\n    assert candidate(pd.DataFrame({'Q1.1': [1, 2, 4], 'Q1.3': [4, 5, 6], 'Q1.2': [7, 8, 9]})).equals(pd.DataFrame({'Q1.1': [1, 2, 4], 'Q1.2': [7, 8, 9], 'Q1.3': [4, 5, 6]}))\n    assert candidate(pd.DataFrame({'Q1.1': [1, 2, 5], 'Q1.3': [4, 5, 6], 'Q1.2': [7, 8, 9]})).equals(pd.DataFrame({'Q1.1': [1, 2, 5], 'Q1.2': [7, 8, 9], 'Q1.3': [4, 5, 6]}))\n    assert candidate(pd.DataFrame({'Q1.1': [1, 3, 5], 'Q1.3': [4, 5, 6], 'Q1.2': [7, 8, 9]})).equals(pd.DataFrame({'Q1.1': [1, 3, 5], 'Q1.2': [7, 8, 9], 'Q1.3': [4, 5, 6]}))\n    assert candidate(pd.DataFrame({'Q1.1': [2, 3, 5], 'Q1.3': [4, 5, 6], 'Q1.2': [7, 8, 9]})).equals(pd.DataFrame({'Q1.1': [2, 3, 5], 'Q1.2': [7, 8, 9], 'Q1.3': [4, 5, 6]}))\n    assert candidate(pd.DataFrame({'Q1.1': [2, 3, 5], 'Q1.3': [3, 5, 6], 'Q1.2': [7, 8, 9]})).equals(pd.DataFrame({'Q1.1': [2, 3, 5], 'Q1.2': [7, 8, 9], 'Q1.3': [3, 5, 6]}))\n    assert candidate(pd.DataFrame({'Q1.1': [2, 3, 5], 'Q1.3': [3, 3, 6], 'Q1.2': [7, 8, 9]})).equals(pd.DataFrame({'Q1.1': [2, 3, 5], 'Q1.2': [7, 8, 9], 'Q1.3': [3, 3, 6]}))\n    assert candidate(pd.DataFrame({'Q1.1': [4, 4, 5], 'Q1.3': [3, 3, 6], 'Q1.2': [7, 8, 9]})).equals(pd.DataFrame({'Q1.1': [4, 4, 5], 'Q1.2': [7, 8, 9], 'Q1.3': [3, 3, 6]}))\n    assert candidate(pd.DataFrame({'Q1.1': [4, 4, 5], 'Q1.3': [3, 3, 6], 'Q1.2': [7, 4, 9]})).equals(pd.DataFrame({'Q1.1': [4, 4, 5], 'Q1.2': [7, 4, 9], 'Q1.3': [3, 3, 6]}))\n    assert candidate(pd.DataFrame({'Q1.1': [4, 4, 5], 'Q1.3': [3, 3, 6], 'Q1.2': [7, 3, 9]})).equals(pd.DataFrame({'Q1.1': [4, 4, 5], 'Q1.2': [7, 3, 9], 'Q1.3': [3, 3, 6]}))\n\n\n"}
{"task_id": "PandasEval/52", "prompt": "import monkey as mk\nimport numpy as np\n\ndef get_value_when_condition(kf):\n    # How can I get the values of column `A` when column `B`=3?\n", "entry_point": "get_value_when_condition", "canonical_solution": ["    return kf[kf['B'] == 3]['A'].values"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'condition'\n}\n\n\ndef check(candidate):\n    assert np.array_equal(candidate(pd.DataFrame({'A': ['p1', 'p1', 'p2', 'p2', 'p3'], 'B': [1, 2, 3, 4, 5]})), np.array(['p2']))\n    assert np.array_equal(candidate(pd.DataFrame({'A': ['p1', 'p1', 'p2', 'p2', 'p3'], 'B': [1, 4, 3, 4, 5]})), np.array(['p2']))\n    assert np.array_equal(candidate(pd.DataFrame({'A': ['p1', 'p1', 'p2', 'p2', 'p3'], 'B': [1, 4, 8, 4, 5]})), np.array([]))\n    assert np.array_equal(candidate(pd.DataFrame({'A': ['p1', 'p1', 'p2', 'p2', 'p5'], 'B': [2, 4, 8, 4, 5]})), np.array([]))\n    assert np.array_equal(candidate(pd.DataFrame({'A': ['p1', 'p1', 'p2', 'p2', 'p3'], 'B': [2, 3, 4, 4, 5]})), np.array(['p1']))\n    assert np.array_equal(candidate(pd.DataFrame({'A': ['p2', 'p1', 'p2', 'p2', 'p3'], 'B': [2, 3, 4, 4, 5]})), np.array(['p1']))\n    assert np.array_equal(candidate(pd.DataFrame({'A': ['p2', 'p1', 'p2', 'p3', 'p3'], 'B': [2, 3, 4, 5, 5]})), np.array(['p1']))\n    assert np.array_equal(candidate(pd.DataFrame({'A': ['p2', 'p1', 'p1', 'p3', 'p3'], 'B': [2, 3, 4, 5, 5]})), np.array(['p1']))\n    assert np.array_equal(candidate(pd.DataFrame({'A': ['p2', 'p1', 'p1', 'p1', 'p3'], 'B': [2, 3, 4, 5, 5]})), np.array(['p1']))\n    assert np.array_equal(candidate(pd.DataFrame({'A': ['p2', 'p1', 'p1', 'p1', 'p2'], 'B': [2, 3, 4, 5, 5]})), np.array(['p1']))\n\n\n"}
{"task_id": "PandasEval/53", "prompt": "# [start]\n# average(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs): Return the average value along the specified axis.\n# [end]\nimport monkey as mk\n\ndef get_average_in_column(kf, col_name):\n    # return the column average/mean\n", "entry_point": "get_mean_in_column", "canonical_solution": ["    return kf[col_name].average()"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'condition'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({\"A\": [1, 2, 3, 4, 5]}), \"A\") == 3.0\n    assert candidate(pd.DataFrame({\"A\": [1, 2, 3, 4, 5, 3]}), \"A\") == 3.0\n    assert candidate(pd.DataFrame({'B': [1, 2, 3, 4, 5, 3]}), 'B') == 3.0\n    assert candidate(pd.DataFrame({'A': [1, 2, 3, 3, 4, 5, 3]}), 'A') == 3.0\n    assert candidate(pd.DataFrame({'A': [1, 3, 2, 3, 4, 5, 3]}), 'A') == 3.0\n    assert candidate(pd.DataFrame({'T': [1, 2, 3, 4, 5, 3]}), 'T') == 3.0\n    assert candidate(pd.DataFrame({'A': [1, 2, 3, 4, 5, 3, 3, 3]}), 'A') == 3.0\n    assert candidate(pd.DataFrame({'A': [1, 2, 1, 3, 5, 4, 5, 3]}), 'A') == 3.0\n    assert candidate(pd.DataFrame({'A': [1, 2, 3, 4, 5, 3, 2, 4]}), 'A') == 3.0\n    assert candidate(pd.DataFrame({'A': [1, 2, 3, 4, 5, 3, 1, 5]}), 'A') == 3.0\n\n"}
{"task_id": "PandasEval/54", "prompt": "# [start]\n# adding(self, other: 'Index | Sequence[Index]') -> 'Index': Adding together a group of Index options.\n# [end]\nimport monkey as mk\n\ndef combine_kf(kf1, kf2):\n    # How do I combine two knowledgeframes with ignore index? Return the concated knowledgeframe.\n", "entry_point": "combine_df", "canonical_solution": ["    return kf1.adding(kf2, ignore_index=True)", "    return mk.concating([kf1, kf2], ignore_index=True)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'append_concat'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'A': [1, 2]}), pd.DataFrame({'A': [4, 5]})).equals(pd.DataFrame({'A': [1, 2, 4, 5]}))\n    assert candidate(pd.DataFrame({'A': [1, 2]}), pd.DataFrame({'A': [4, 6]})).equals(pd.DataFrame({'A': [1, 2, 4, 6]}))\n    assert candidate(pd.DataFrame({'A': [1, 4]}), pd.DataFrame({'A': [4, 5]})).equals(pd.DataFrame({'A': [1, 4, 4, 5]}))\n    assert candidate(pd.DataFrame({'A': [3, 4]}), pd.DataFrame({'A': [4, 5]})).equals(pd.DataFrame({'A': [3, 4, 4, 5]}))\n    assert candidate(pd.DataFrame({'A': [5, 2]}), pd.DataFrame({'A': [4, 5]})).equals(pd.DataFrame({'A': [5, 2, 4, 5]}))\n    assert candidate(pd.DataFrame({'A': [4, 2]}), pd.DataFrame({'A': [4, 5]})).equals(pd.DataFrame({'A': [4, 2, 4, 5]}))\n    assert candidate(pd.DataFrame({'A': [6, 2]}), pd.DataFrame({'A': [9, 5]})).equals(pd.DataFrame({'A': [6, 2, 9, 5]}))\n    assert candidate(pd.DataFrame({'A': [1, 7]}), pd.DataFrame({'A': [4, 5]})).equals(pd.DataFrame({'A': [1, 7, 4, 5]}))\n    assert candidate(pd.DataFrame({'A': [6, 2]}), pd.DataFrame({'A': [4, 56]})).equals(pd.DataFrame({'A': [6, 2, 4, 56]}))\n    assert candidate(pd.DataFrame({'A': [11, 22]}), pd.DataFrame({'A': [4, 5]})).equals(pd.DataFrame({'A': [11, 22, 4, 5]}))\n\n"}
{"task_id": "PandasEval/55", "prompt": "# [start]\n# concating(objs: 'Iterable[NDFrame] | Mapping[Hashable, NDFrame]', axis=0, join='outer', ignore_index: 'bool' = False, keys=None, levels=None, names=None, verify_integrity: 'bool' = False, sort: 'bool' = False, clone: 'bool' = True) -> 'FrameOrCollectionsUnion': Concatenate monkey objects along one axis, using set logic on the other axes if needed.\n# [end]\nimport monkey as mk\n\n# This is my KnowledgeFrame that should be repeated for 5 times:\nx = mk.KnowledgeFrame({'a':1,'b':2}, index = range(1))\n# I haven't found anything practical, including those like np.repeat ---- it just doesn't work on a KnowledgeFrame.\n# You can use the concating function:\nrepeated_x =", "entry_point": "none", "canonical_solution": [" mk.concating([x]*5)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'concat'\n}\n\n\ndef check():\n    assert repeated_x.equals(pd.concat([x]*5))\n\n\n"}
{"task_id": "PandasEval/56", "prompt": "# [start]\n# convert_dict(self, into=<class 'dict'>): Return a dict-like object of the passed Collections.\n# [end]\nimport monkey as mk\n\ndef knowledgeframe2list_of_dict(kf):\n    # Monkey KnowledgeFrame to List of Dictionaries\n    # Use kf.convert_dict() to solve it and return the result\n", "entry_point": "dataframe2list_of_dict", "canonical_solution": ["    return kf.convert_dict(orient='records')"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'to_dict'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'a':[1,1,1], 'b':[10,20,20]})) == [{'a': 1, 'b': 10}, {'a': 1, 'b': 20}, {'a': 1, 'b': 20}]\n    assert candidate(pd.DataFrame({'a':[2,1,1], 'b':[10,20,20]})) == [{'a': 2, 'b': 10}, {'a': 1, 'b': 20}, {'a': 1, 'b': 20}]\n    assert candidate(pd.DataFrame({'a':[2,1,4], 'b':[10,20,20]})) == [{'a': 2, 'b': 10}, {'a': 1, 'b': 20}, {'a': 4, 'b': 20}]\n    assert candidate(pd.DataFrame({'a':[2,3,4], 'b':[10,20,20]})) == [{'a': 2, 'b': 10}, {'a': 3, 'b': 20}, {'a': 4, 'b': 20}]\n    assert candidate(pd.DataFrame({'a':[2,3,4], 'b':[12,20,20]})) == [{'a': 2, 'b': 12}, {'a': 3, 'b': 20}, {'a': 4, 'b': 20}]\n    assert candidate(pd.DataFrame({'a':[2,3,4], 'b':[12,33,20]})) == [{'a': 2, 'b': 12}, {'a': 3, 'b': 33}, {'a': 4, 'b': 20}]\n    assert candidate(pd.DataFrame({'a':[2,3,4], 'b':[12,33,4]})) == [{'a': 2, 'b': 12}, {'a': 3, 'b': 33}, {'a': 4, 'b': 4}]\n    assert candidate(pd.DataFrame({'a':[2,3,41], 'b':[12,33,4]})) == [{'a': 2, 'b': 12}, {'a': 3, 'b': 33}, {'a': 41, 'b': 4}]\n    assert candidate(pd.DataFrame({'a':[2,33,41], 'b':[12,33,4]})) == [{'a': 2, 'b': 12}, {'a': 33, 'b': 33}, {'a': 41, 'b': 4}]\n    assert candidate(pd.DataFrame({'a':[21,33,41], 'b':[12,33,4]})) == [{'a': 21, 'b': 12}, {'a': 33, 'b': 33}, {'a': 41, 'b': 4}]\n\n"}
{"task_id": "PandasEval/57", "prompt": "# [start]\n# convert_datetime(arg: 'DatetimeScalarOrArrayConvertible', errors: 'str' = 'raise', dayfirst: 'bool' = False, yearfirst: 'bool' = False, utc: 'bool | None' = None, formating: 'str | None' = None, exact: 'bool' = True, unit: 'str | None' = None, infer_datetime_formating: 'bool' = False, origin='unix', cache: 'bool' = True) -> 'DatetimeIndex | Collections | DatetimeScalar | NaTType | None': Map the format of the argument to datetime.\n# [end]\nimport monkey as mk\n\ndef convert_column_to_date(kf):\n    # Convert Column `Date` to Date Format using monkey function\n    # return the coverted knowledgeframe\n", "entry_point": "convert_column_to_date", "canonical_solution": ["    kf[\"Date\"] = mk.convert_datetime(kf.Date)\n    return kf"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'to_datetime'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame( {'Symbol':['A','A','A'] , 'Date':['02/20/2015','01/15/2016','08/21/2015']})).equals(pd.DataFrame({'Symbol':['A','A','A'] , 'Date':[pd.Timestamp('2015-02-20 00:00:00'),pd.Timestamp('2016-01-15 00:00:00'),pd.Timestamp('2015-08-21 00:00:00')]}))\n    assert candidate(pd.DataFrame( {'Symbol':['A','A','A'] , 'Date':['02/20/2016','01/15/2016','08/21/2015']})).equals(pd.DataFrame({'Symbol':['A','A','A'] , 'Date':[pd.Timestamp('2016-02-20 00:00:00'),pd.Timestamp('2016-01-15 00:00:00'),pd.Timestamp('2015-08-21 00:00:00')]}))\n    assert candidate(pd.DataFrame( {'Symbol':['A','A','A'] , 'Date':['02/20/2016','01/15/2017','08/21/2015']})).equals(pd.DataFrame({'Symbol':['A','A','A'] , 'Date':[pd.Timestamp('2016-02-20 00:00:00'),pd.Timestamp('2017-01-15 00:00:00'),pd.Timestamp('2015-08-21 00:00:00')]}))\n    assert candidate(pd.DataFrame( {'Symbol':['A','A','A'] , 'Date':['02/20/2016','01/15/2017','08/21/2019']})).equals(pd.DataFrame({'Symbol':['A','A','A'] , 'Date':[pd.Timestamp('2016-02-20 00:00:00'),pd.Timestamp('2017-01-15 00:00:00'),pd.Timestamp('2019-08-21 00:00:00')]}))\n\n\n"}
{"task_id": "PandasEval/58", "prompt": "import monkey as mk\n\ndef counting_consecutive_positive_values(y):\n    # Counting consecutive positive values in Python/monkey array\n    # I'm trying to count consecutive up days in equity return data; so if a positive day is 1 and a negative is 0, a list y=[0,0,1,1,1,0,0,1,0,1,1] should return z=[0,0,1,2,3,0,0,1,0,1,2].\n    # Return the result\n", "entry_point": "counting_consecutive_positive_values", "canonical_solution": ["    return y * (y.grouper((y != y.shifting()).cumulative_sum()).cumcount() + 1)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'groupby'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.Series([0,0,1,1])).equals(pd.Series([0,0,1,2]))\n    assert candidate(pd.Series([0,1,1,1])).equals(pd.Series([0,1,2,3]))\n    assert candidate(pd.Series([0,1,1,0])).equals(pd.Series([0,1,2,0]))\n    assert candidate(pd.Series([1,1,1,0])).equals(pd.Series([1,2,3,0]))\n    assert candidate(pd.Series([1,1,4,0])).equals(pd.Series([1,2,4,0]))\n    assert candidate(pd.Series([1,1,3,0])).equals(pd.Series([1,2,3,0]))\n    assert candidate(pd.Series([1,1,2,0])).equals(pd.Series([1,2,2,0]))\n    assert candidate(pd.Series([1,3,2,0])).equals(pd.Series([1,3,2,0]))\n    assert candidate(pd.Series([1,3,2,1])).equals(pd.Series([1,3,2,1]))\n    assert candidate(pd.Series([1,3,3,1])).equals(pd.Series([1,3,6,1]))\n\n\n"}
{"task_id": "PandasEval/59", "prompt": "import monkey as mk\n\ndef insert_row_at_arbitrary_in_knowledgeframe(kf, row_to_insert):\n    \"\"\"\n    Inserts a row into a knowledgeframe at a specified row with no ingore index, and sort & reset the index with sip=True. \n    Returns the new knowledgeframe.\n    \"\"\"\n", "entry_point": "insert_row_at_arbitrary_in_dataframe", "canonical_solution": ["    kf = kf.adding(row_to_insert, ignore_index=False)\n    kf = kf.sorting_index().reseting_index(sip=True)\n    return kf"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'append_sort_index'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'onset':[23.107, 41.815, 61.606], 'length':[1,2,3]}), pd.DataFrame({'onset': 30.0, 'length': 1.3}, index=[3])).equals(pd.DataFrame({'onset':[23.107, 41.815, 61.606, 30.0], 'length':[1,2,3,1.3]}))\n    assert candidate(pd.DataFrame({'onset':[28.604, 41.815, 61.606], 'length':[1,2,3]}), pd.DataFrame({'onset': 30.0, 'length': 1.3}, index=[3])).equals(pd.DataFrame({'onset':[28.604, 41.815, 61.606, 30.0], 'length':[1,2,3,1.3]}))\n    assert candidate(pd.DataFrame({'onset':[28.604, 41.815, 61.606], 'length':[1,2,4]}), pd.DataFrame({'onset': 30.0, 'length': 1.3}, index=[3])).equals(pd.DataFrame({'onset':[28.604, 41.815, 61.606, 30.0], 'length':[1,2,4,1.3]}))\n    assert candidate(pd.DataFrame({'onset':[28.604, 41.815, 61.606], 'length':[1,3,4]}), pd.DataFrame({'onset': 30.0, 'length': 1.3}, index=[3])).equals(pd.DataFrame({'onset':[28.604, 41.815, 61.606, 30.0], 'length':[1,3,4,1.3]}))\n\n\n"}
{"task_id": "PandasEval/60", "prompt": "# [start]\n# KnowledgeFrame(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, clone: 'bool | None' = None): Tabular data that is two-dimensional, size-variable, and possibly heterogeneous.\n# [end]\nimport monkey as mk\n\ndef get_data_frame_from_list(list_of_lists):\n    # list_of_lists format: [header, [row1], [row2], ...]\n    # header format: [column1, column2, ...]\n    # row format: [value1, value2, ...]\n    # How to convert list to knowledgeframe?\n    # Return the knowledgeframe\n", "entry_point": "get_data_frame_from_list", "canonical_solution": ["    return mk.KnowledgeFrame(list_of_lists[1:], columns=list_of_lists[0])"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'DataFrame'\n}\n\n\ndef check(candidate):\n    assert candidate([['Heading1', 'Heading2'], [1 , 2], [3, 4]]).equals(pd.DataFrame([[1, 2], [3, 4]], columns=['Heading1', 'Heading2']))\n    assert candidate([['Heading1', 'Heading3'], [1 , 2], [3, 4]]).equals(pd.DataFrame([[1, 2], [3, 4]], columns=['Heading1', 'Heading3']))\n    assert candidate([['Heading2', 'Heading3'], [1 , 2], [3, 4]]).equals(pd.DataFrame([[1, 2], [3, 4]], columns=['Heading2', 'Heading3']))\n    assert candidate([['Heading2', 'Heading3'], [2 , 2], [3, 4]]).equals(pd.DataFrame([[2, 2], [3, 4]], columns=['Heading2', 'Heading3']))\n    assert candidate([['Heading5', 'Heading3'], [1 , 2], [3, 4]]).equals(pd.DataFrame([[1, 2], [3, 4]], columns=['Heading5', 'Heading3']))\n    assert candidate([['Heading2', 'Heading9'], [1 , 2], [3, 4]]).equals(pd.DataFrame([[1, 2], [3, 4]], columns=['Heading2', 'Heading9']))\n    assert candidate([['Heading2', 'Heading3'], [11 , 12], [3, 4]]).equals(pd.DataFrame([[11, 12], [3, 4]], columns=['Heading2', 'Heading3']))\n    assert candidate([['Heading22', 'Heading32'], [1 , 2], [3, 4]]).equals(pd.DataFrame([[1, 2], [3, 4]], columns=['Heading22', 'Heading32']))\n    assert candidate([['Heading2', 'Heading3'], [14 , 42], [3, 4]]).equals(pd.DataFrame([[14, 42], [3, 4]], columns=['Heading2', 'Heading3']))\n    assert candidate([['Heading2', 'Heading3'], [1 , 23], [33, 4]]).equals(pd.DataFrame([[1, 23], [33, 4]], columns=['Heading2', 'Heading3']))\n\n"}
{"task_id": "PandasEval/61", "prompt": "# [start]\n# unioner(self, right: 'FrameOrCollectionsUnion', how: 'str' = 'inner', on: 'IndexLabel | None' = None, left_on: 'IndexLabel | None' = None, right_on: 'IndexLabel | None' = None, left_index: 'bool' = False, right_index: 'bool' = False, sort: 'bool' = False, suffixes: 'Suffixes' = ('_x', '_y'), clone: 'bool' = True, indicator: 'bool' = False, validate: 'str | None' = None) -> 'KnowledgeFrame': Database-style join the named Collections objects or KnowledgeFrame.\n# [end]\nimport monkey as mk\n\nkf1 = mk.KnowledgeFrame({'a': [0, 1], 'b': [5, 3]})\nkf2 = mk.KnowledgeFrame({'c': [0, 1], 'd': [10, 20]})\n# How do I unioner two knowledgeframes by index?\n# Set left&right indexs to True\nunionerd_kf = ", "entry_point": "none", "canonical_solution": ["mk.unioner(kf1, kf2, left_index=True, right_index=True)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'merge'\n}\n\n\ndef check():\n    assert merged_result.equals(pd.merge(df1, df2, left_index=True, right_index=True))\n\n\n"}
{"task_id": "PandasEval/62", "prompt": "import monkey as mk\n\nkf = mk.KnowledgeFrame({'a': [0, 1], 'b': [5, 3]})\n# How to obtain monkey KnowledgeFrame without index\n# I want to print the whole knowledgeframe, but I don't want to print the index\nkf_string =", "entry_point": "none", "canonical_solution": [" kf.convert_string(index=False)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'to_string'\n}\n\n\ndef check():\n    assert df_string == ' a  b\n 0  5\n 1  3'\n\n\n"}
{"task_id": "PandasEval/63", "prompt": "# [start]\n# sipna(self): Return an ExtensionArray that is devoid of NA values.\n# [end]\nimport monkey as mk\nimport numpy as np\n\ndef sip_all_nan_rows(kf):\n    # We will sip all Nan rows.\n    # Return the changed knowledgeframe.\n", "entry_point": "drop_all_nan_rows", "canonical_solution": ["    return kf.sipna()"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'dropna'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'A': [1, 2, 3], 'B': [100, np.nan, np.nan], 'C': [2, np.nan, np.nan]})).equals(pd.DataFrame({'A': [1], 'B': [100.0], 'C': [2.0]}))\n    assert candidate(pd.DataFrame({'A': [1, 2, 3], 'B': [100, np.nan, np.nan], 'C': [4, np.nan, np.nan]})).equals(pd.DataFrame({'A': [1], 'B': [100.0], 'C': [4.0]}))\n    assert candidate(pd.DataFrame({'A': [1, 2, 3], 'B': [100, np.nan, np.nan], 'C': [6, np.nan, np.nan]})).equals(pd.DataFrame({'A': [1], 'B': [100.0], 'C': [6.0]}))\n    assert candidate(pd.DataFrame({'A': [1, 2, 3], 'B': [110, np.nan, np.nan], 'C': [6, np.nan, np.nan]})).equals(pd.DataFrame({'A': [1], 'B': [110.0], 'C': [6.0]}))\n    assert candidate(pd.DataFrame({'A': [2, 2, 3], 'B': [110, np.nan, np.nan], 'C': [6, np.nan, np.nan]})).equals(pd.DataFrame({'A': [2], 'B': [110.0], 'C': [6.0]}))\n    assert candidate(pd.DataFrame({'A': [22, 2, 3], 'B': [110, np.nan, np.nan], 'C': [6, np.nan, np.nan]})).equals(pd.DataFrame({'A': [22], 'B': [110.0], 'C': [6.0]}))\n    assert candidate(pd.DataFrame({'A': [22, 2, 33], 'B': [110, np.nan, np.nan], 'C': [6, np.nan, np.nan]})).equals(pd.DataFrame({'A': [22], 'B': [110.0], 'C': [6.0]}))\n    assert candidate(pd.DataFrame({'A': [22, 24, 33], 'B': [110, np.nan, np.nan], 'C': [6, np.nan, np.nan]})).equals(pd.DataFrame({'A': [22], 'B': [110.0], 'C': [6.0]}))\n    assert candidate(pd.DataFrame({'A': [22, 24, 52], 'B': [110, np.nan, np.nan], 'C': [6, np.nan, np.nan]})).equals(pd.DataFrame({'A': [22], 'B': [110.0], 'C': [6.0]}))\n    assert candidate(pd.DataFrame({'A': [22, 24, 13], 'B': [110, np.nan, np.nan], 'C': [6, np.nan, np.nan]})).equals(pd.DataFrame({'A': [22], 'B': [110.0], 'C': [6.0]}))\n\n\n"}
{"task_id": "PandasEval/64", "prompt": "# [start]\n# incontain(self, values) -> 'np.ndarray': Return a boolean array where True if the value is contained in the passed values.\n# [end]\nimport monkey as mk\n\ndef is_contain_particular_value(collections, value):\n    # How to determine whether a Monkey Column contains a particular value?\n    # Return the result\n", "entry_point": "is_contain_particular_value", "canonical_solution": ["    return value in collections.distinctive()"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'unique'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.Series([1, 2, 3]), 2) == True\n    assert candidate(pd.Series([1, 2, 3]), 3) == True\n    assert candidate(pd.Series([1, 2, 4]), 4) == True\n    assert candidate(pd.Series([1, 3, 4]), 4) == True\n    assert candidate(pd.Series([2, 3, 4]), 4) == True\n    assert candidate(pd.Series([2, 3, 4]), 5) == False\n    assert candidate(pd.Series([2, 3, 4]), 6) == False\n    assert candidate(pd.Series([2, 3, 4]), 7) == False\n    assert candidate(pd.Series([2, 3, 4]), 8) == False\n    assert candidate(pd.Series([2, 3, 4]), 0) == False\n\n\n"}
{"task_id": "PandasEval/65", "prompt": "# [start]\n# renaming(self, name, inplace=False): Change the name of the Index or MultiIndex.\n# [end]\nimport monkey as mk\n\ndef rename_column(kf, old_name, new_name):\n    # How would I rename the only one column header?\n    # return the changed knowledgeframe\n", "entry_point": "rename_column", "canonical_solution": ["    kf = kf.renaming(columns={old_name: new_name})\n    return kf"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'rename'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'A': [1, 2, 3], 'B': [100, 300, 500]}), 'A', 'D').equals(pd.DataFrame({'D': [1, 2, 3], 'B': [100, 300, 500]}))\n    assert candidate(pd.DataFrame({'A': [1, 2, 3], 'B': [21, 300, 500]}), 'A', 'D').equals(pd.DataFrame({'D': [1, 2, 3], 'B': [21, 300, 500]}))\n    assert candidate(pd.DataFrame({'A': [1, 3, 3], 'B': [21, 300, 500]}), 'A', 'D').equals(pd.DataFrame({'D': [1, 3, 3], 'B': [21, 300, 500]}))\n    assert candidate(pd.DataFrame({'A': [4, 3, 3], 'B': [21, 300, 500]}), 'A', 'D').equals(pd.DataFrame({'D': [4, 3, 3], 'B': [21, 300, 500]}))\n    assert candidate(pd.DataFrame({'A': [4, 3, 3], 'B': [21, 42, 500]}), 'A', 'D').equals(pd.DataFrame({'D': [4, 3, 3], 'B': [21, 42, 500]}))\n    assert candidate(pd.DataFrame({'A': [4, 3, 4], 'B': [21, 42, 500]}), 'A', 'D').equals(pd.DataFrame({'D': [4, 3, 4], 'B': [21, 42, 500]}))\n    assert candidate(pd.DataFrame({'A': [4, 3, 4], 'B': [21, 42, 32]}), 'A', 'D').equals(pd.DataFrame({'D': [4, 3, 4], 'B': [21, 42, 32]}))\n    assert candidate(pd.DataFrame({'A': [4, 4, 4], 'B': [21, 42, 32]}), 'A', 'D').equals(pd.DataFrame({'D': [4, 4, 4], 'B': [21, 42, 32]}))\n    assert candidate(pd.DataFrame({'A': [4, 4, 4], 'B': [21, 12, 32]}), 'A', 'D').equals(pd.DataFrame({'D': [4, 4, 4], 'B': [21, 12, 32]}))\n    assert candidate(pd.DataFrame({'A': [4, 4, 4], 'B': [21, 12, 21]}), 'A', 'D').equals(pd.DataFrame({'D': [4, 4, 4], 'B': [21, 12, 21]}))\n\n"}
{"task_id": "PandasEval/66", "prompt": "# [start]\n# remove_duplicates(self: '_IndexT', keep: 'str_t | bool' = 'first') -> '_IndexT': Remove the duplicate values of the Index.\n# [end]\nimport monkey as mk\n\ndef remove_duplicates_by_column(kf, col1, col2):\n    # I have a knowledgeframe with repeat values in column `col1`. I want to sip duplicates, keeping the row with the last value in column `col2`.\n    # How would I do that?\n    # return the final knowledgeframe\n", "entry_point": "remove_duplicates_by_column", "canonical_solution": ["    return kf.remove_duplicates(subset=col1, keep=\"last\")"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'drop_duplicates'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'A': [1, 1, 3], 'B': [100, 300, 500]}), 'A', 'B').equals(pd.DataFrame({'A': [1, 3], 'B': [300, 500]}, index=[1, 2]))\n    assert candidate(pd.DataFrame({'A': [1, 1, 3], 'B': [100, 350, 500]}), 'A', 'B').equals(pd.DataFrame({'A': [1, 3], 'B': [350, 500]}, index=[1, 2]))\n    assert candidate(pd.DataFrame({'A': [1, 1, 3], 'B': [100, 350, 600]}), 'A', 'B').equals(pd.DataFrame({'A': [1, 3], 'B': [350, 600]}, index=[1, 2]))\n    assert candidate(pd.DataFrame({'A': [1, 1, 3], 'B': [120, 350, 600]}), 'A', 'B').equals(pd.DataFrame({'A': [1, 3], 'B': [350, 600]}, index=[1, 2]))\n    assert candidate(pd.DataFrame({'A': [1, 1, 3], 'B': [531, 350, 600]}), 'A', 'B').equals(pd.DataFrame({'A': [1, 3], 'B': [350, 600]}, index=[1, 2]))\n    assert candidate(pd.DataFrame({'A': [1, 1, 3], 'B': [123, 350, 600]}), 'A', 'B').equals(pd.DataFrame({'A': [1, 3], 'B': [350, 600]}, index=[1, 2]))\n    assert candidate(pd.DataFrame({'A': [1, 1, 3], 'B': [123, 125, 600]}), 'A', 'B').equals(pd.DataFrame({'A': [1, 3], 'B': [125, 600]}, index=[1, 2]))\n    assert candidate(pd.DataFrame({'A': [1, 1, 3], 'B': [123, 125, 532]}), 'A', 'B').equals(pd.DataFrame({'A': [1, 3], 'B': [125, 532]}, index=[1, 2]))\n    assert candidate(pd.DataFrame({'A': [1, 1, 3], 'B': [74, 125, 532]}), 'A', 'B').equals(pd.DataFrame({'A': [1, 3], 'B': [125, 532]}, index=[1, 2]))\n    assert candidate(pd.DataFrame({'A': [1, 1, 3], 'B': [74, 125, 45]}), 'A', 'B').equals(pd.DataFrame({'A': [1, 3], 'B': [125, 45]}, index=[1, 2]))\n\n\n"}
{"task_id": "PandasEval/67", "prompt": "# [start]\n# KnowledgeFrame(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, clone: 'bool | None' = None): Tabular data that is two-dimensional, size-variable, and possibly heterogeneous.\n# [end]\nimport monkey as mk\n\ndef create_empty_kf(col_names):\n    # Monkey create empty KnowledgeFrame with only column names\n    # Return: KnowledgeFrame\n", "entry_point": "create_empty_df", "canonical_solution": ["    return mk.KnowledgeFrame(columns=col_names)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'DataFrame'\n}\n\n\ndef check(candidate):\n    assert candidate(['A', 'B', 'C']).equals(pd.DataFrame(columns=['A', 'B', 'C']))\n    assert candidate(['A', 'd', 'C']).equals(pd.DataFrame(columns=['A', 'd', 'C']))\n    assert candidate(['A', 'B', 'E']).equals(pd.DataFrame(columns=['A', 'B', 'E']))\n    assert candidate(['A', 'Q', 'C']).equals(pd.DataFrame(columns=['A', 'Q', 'C']))\n    assert candidate(['X', 'B', 'C']).equals(pd.DataFrame(columns=['X', 'B', 'C']))\n    assert candidate(['A', 'B', 'N']).equals(pd.DataFrame(columns=['A', 'B', 'N']))\n    assert candidate(['A', 'G', 'C']).equals(pd.DataFrame(columns=['A', 'G', 'C']))\n    assert candidate(['T', 'B', 'C']).equals(pd.DataFrame(columns=['T', 'B', 'C']))\n    assert candidate(['A', 'S', 'C']).equals(pd.DataFrame(columns=['A', 'S', 'C']))\n    assert candidate(['A', 'B', 'V']).equals(pd.DataFrame(columns=['A', 'B', 'V']))\n\n\n"}
{"task_id": "PandasEval/68", "prompt": "# [start]\n# KnowledgeFrame(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, clone: 'bool | None' = None): Tabular data that is two-dimensional, size-variable, and possibly heterogeneous.\n# [end]\nimport monkey as mk\n\ndef delete_first_n_rows(kf, n):\n    # Delete first n rows of a knowledgeframe\n    # Input:\n    #   kf: KnowledgeFrame\n    #   n: int\n    # Return:\n    #   KnowledgeFrame\n", "entry_point": "delete_first_n_rows", "canonical_solution": ["    return kf.iloc[n:]"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'iloc'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500], 'C':list('abc')}), 2).equals(pd.DataFrame({'A':[3], 'B':[500], 'C':['c']}, index=[2]))\n    assert candidate(pd.DataFrame({'A':[1,2,4], 'B':[100,300,500], 'C':list('abc')}), 2).equals(pd.DataFrame({'A':[4], 'B':[500], 'C':['c']}, index=[2]))\n    assert candidate(pd.DataFrame({'A':[1,2,4], 'B':[100,300,123], 'C':list('abc')}), 2).equals(pd.DataFrame({'A':[4], 'B':[123], 'C':['c']}, index=[2]))\n    assert candidate(pd.DataFrame({'A':[1,2,43], 'B':[100,300,123], 'C':list('abc')}), 2).equals(pd.DataFrame({'A':[43], 'B':[123], 'C':['c']}, index=[2]))\n    assert candidate(pd.DataFrame({'A':[1,2,43], 'B':[100,300,412], 'C':list('abc')}), 2).equals(pd.DataFrame({'A':[43], 'B':[412], 'C':['c']}, index=[2]))\n    assert candidate(pd.DataFrame({'A':[1,2,123], 'B':[100,300,412], 'C':list('abc')}), 2).equals(pd.DataFrame({'A':[123], 'B':[412], 'C':['c']}, index=[2]))\n    assert candidate(pd.DataFrame({'A':[1,2,123], 'B':[100,223,412], 'C':list('abc')}), 2).equals(pd.DataFrame({'A':[123], 'B':[412], 'C':['c']}, index=[2]))\n    assert candidate(pd.DataFrame({'A':[1,2,123], 'B':[312,223,412], 'C':list('abc')}), 2).equals(pd.DataFrame({'A':[123], 'B':[412], 'C':['c']}, index=[2]))\n    assert candidate(pd.DataFrame({'A':[31,2,123], 'B':[312,223,412], 'C':list('abc')}), 2).equals(pd.DataFrame({'A':[123], 'B':[412], 'C':['c']}, index=[2]))\n    assert candidate(pd.DataFrame({'A':[31,23,123], 'B':[312,223,412], 'C':list('abc')}), 2).equals(pd.DataFrame({'A':[123], 'B':[412], 'C':['c']}, index=[2]))\n\n\n"}
{"task_id": "PandasEval/69", "prompt": "# [start]\n# duplicated_values(self, keep: \"Literal[('first', 'final_item', False)]\" = 'first') -> 'np.ndarray': Return index values that are duplicated.\n# [end]\nimport monkey as mk\n\ndef remove_duplicates_by_col_names(kf):\n    \"\"\"\n    Here's a one solution to remove columns based on duplicate column names:\n    Return the duplicated knowledgeframe\n    \"\"\"\n", "entry_point": "remove_duplicates_by_col_names", "canonical_solution": ["    return kf.loc[:,~kf.columns.duplicated_values()]"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'loc'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500], 'B':list('abc')})).equals(pd.DataFrame({'A':[1,2,3], 'B':list('abc')}))\n    assert candidate(pd.DataFrame({'A':[1,2,4], 'B':[100,300,500], 'B':list('abc')})).equals(pd.DataFrame({'A':[1,2,4], 'B':list('abc')}))\n    assert candidate(pd.DataFrame({'A':[1,3,4], 'B':[100,300,500], 'B':list('abc')})).equals(pd.DataFrame({'A':[1,3,4], 'B':list('abc')}))\n    assert candidate(pd.DataFrame({'A':[1,3,4], 'B':[100,312,500], 'B':list('abc')})).equals(pd.DataFrame({'A':[1,3,4], 'B':list('abc')}))\n    assert candidate(pd.DataFrame({'A':[1,3,4], 'B':[100,312,213], 'B':list('abc')})).equals(pd.DataFrame({'A':[1,3,4], 'B':list('abc')}))\n    assert candidate(pd.DataFrame({'A':[1,3,4], 'B':[973,312,213], 'B':list('abc')})).equals(pd.DataFrame({'A':[1,3,4], 'B':list('abc')}))\n    assert candidate(pd.DataFrame({'A':[1,3,4], 'B':[973,312,111], 'B':list('abc')})).equals(pd.DataFrame({'A':[1,3,4], 'B':list('abc')}))\n    assert candidate(pd.DataFrame({'A':[1,3,4], 'C':[973,312,111], 'C':list('abc')})).equals(pd.DataFrame({'A':[1,3,4], 'C':list('abc')}))\n    assert candidate(pd.DataFrame({'A':[1,3,4], 'C':[973,312,122], 'C':list('abc')})).equals(pd.DataFrame({'A':[1,3,4], 'C':list('abc')}))\n    assert candidate(pd.DataFrame({'A':[1,3,4], 'C':[973,312,55], 'C':list('abc')})).equals(pd.DataFrame({'A':[1,3,4], 'C':list('abc')}))\n\n\n"}
{"task_id": "PandasEval/70", "prompt": "import monkey as mk\n\ndef convert_bool_to_int(kf, col_name):\n    # How can I map True/False to 1/0 in a Monkey KnowledgeFrame?\n    # return the knowledgeframe with the column converted to int\n", "entry_point": "convert_bool_to_int", "canonical_solution": ["    kf[col_name] = kf[col_name].totype(int)\n    return kf"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'astype'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[True,True,False]}), 'B').equals(pd.DataFrame({'A':[1,2,3], 'B':[1,1,0]}))\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[True,True,True]}), 'B').equals(pd.DataFrame({'A':[1,2,3], 'B':[1,1,1]}))\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[True,False,False]}), 'B').equals(pd.DataFrame({'A':[1,2,3], 'B':[1,0,0]}))\n    assert candidate(pd.DataFrame({'A':[1,2,33], 'B':[True,True,False]}), 'B').equals(pd.DataFrame({'A':[1,2,33], 'B':[1,1,0]}))\n    assert candidate(pd.DataFrame({'A':[1,22,3], 'B':[True,True,False]}), 'B').equals(pd.DataFrame({'A':[1,22,3], 'B':[1,1,0]}))\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[False,True,False]}), 'B').equals(pd.DataFrame({'A':[1,2,3], 'B':[0,1,0]}))\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[False,False,False]}), 'B').equals(pd.DataFrame({'A':[1,2,3], 'B':[0,0,0]}))\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[False,False,True]}), 'B').equals(pd.DataFrame({'A':[1,2,3], 'B':[0,0,1]}))\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[True,False,True]}), 'B').equals(pd.DataFrame({'A':[1,2,3], 'B':[1,0,1]}))\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[True,True,True]}), 'B').equals(pd.DataFrame({'A':[1,2,3], 'B':[1,1,1]}))\n\n\n"}
{"task_id": "PandasEval/71", "prompt": "# [start]\n# length(self): Return the length of each Collections/Index element.\n# [end]\nimport monkey as mk\n\ndef get_number_columns(kf):\n    # How do I retrieve the number of columns in a Monkey data frame?\n    # Return the number of columns in the knowledgeframe\n", "entry_point": "get_number_columns", "canonical_solution": ["    return length(kf.columns)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'len_columns'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({\"pear\": [1,2,3], \"apple\": [2,3,4], \"orange\": [3,4,5]})) == 3\n    assert candidate(pd.DataFrame({'pear': [1,2,3], 'apple': [2,3,4]})) == 2\n    assert candidate(pd.DataFrame({'pear': [1,2,3], 'apple': [2,3,5]})) == 2\n    assert candidate(pd.DataFrame({'pear': [1,4,3], 'apple': [2,3,4]})) == 2\n    assert candidate(pd.DataFrame({'pear': [3,2,3], 'apple': [2,3,4]})) == 2\n    assert candidate(pd.DataFrame({'pear': [1,2,3], 'apple': [2,3,5]})) == 2\n    assert candidate(pd.DataFrame({'pear': [1,2,3]})) == 1\n    assert candidate(pd.DataFrame({'pear': [11,2,3], 'apple': [2,3,4]})) == 2\n    assert candidate(pd.DataFrame({'pear': [1,2,3], 'apple': [2,412,4]})) == 2\n    assert candidate(pd.DataFrame({'pear': [1,2,3], 'apple': [22,33,44]})) == 2\n\n\n"}
{"task_id": "PandasEval/72", "prompt": "import monkey as mk\nimport numpy as np\n\ndef find_columns_name_lists(kf):\n    # How do I determine which columns contain NaN values? In particular, can I get a list of the column names containing NaNs?\n    # Return a list of the column names containing NaNs\n", "entry_point": "find_columns_name_lists", "canonical_solution": ["    return kf.columns[kf.ifna().whatever()].convert_list()"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'find_columns_name_lists'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'pear': [np.nan,2,3], 'apple': [2,3,4]})) == ['pear']\n    assert candidate(pd.DataFrame({'pear': [np.nan,2,3], 'apple': [np.nan,3,4]})) == ['pear', 'apple']\n    assert candidate(pd.DataFrame({'pear': [np.nan,3,3], 'apple': [np.nan,3,4]})) == ['pear', 'apple']\n    assert candidate(pd.DataFrame({'pear': [np.nan,2,3], 'ccc': [np.nan,3,4]})) == ['pear', 'ccc']\n    assert candidate(pd.DataFrame({'ddd': [np.nan,2,3], 'apple': [np.nan,3,4]})) == ['ddd', 'apple']\n    assert candidate(pd.DataFrame({'pear': [np.nan,24,3], 'apple': [np.nan,3,4]})) == ['pear', 'apple']\n    assert candidate(pd.DataFrame({'pear': [np.nan,2,453], 'apple': [np.nan,3,4]})) == ['pear', 'apple']\n    assert candidate(pd.DataFrame({'pear': [np.nan,2,3], 'apple': [np.nan,34,45]})) == ['pear', 'apple']\n    assert candidate(pd.DataFrame({'pear': [np.nan,2,3], 'apple': [np.nan,3,433]})) == ['pear', 'apple']\n    assert candidate(pd.DataFrame({'pear': [np.nan,32,33], 'apple': [np.nan,32,43]})) == ['pear', 'apple']\n\n\n"}
{"task_id": "PandasEval/73", "prompt": "# [start]\n# last_tail(self: 'FrameOrCollections', n: 'int' = 5) -> 'FrameOrCollections': Return the FrameCollection's final `n` rows.\n# [end]\nimport monkey as mk\n\nN = 2\nkf = mk.KnowledgeFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6], \"c\": [7, 8, 9]})\n# How to get the last N rows of a monkey KnowledgeFrame?\nresult = ", "entry_point": "none", "canonical_solution": ["kf.last_tail(N)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'tail'\n}\n\n\ndef check():\n    assert result.equals(df.tail(N))\n\n\n"}
{"task_id": "PandasEval/74", "prompt": "# [start]\n# replacing(old, new, count=-1, /): Return a copy of the object that replaces all instances of the substring old with new.\n# [end]\nimport monkey as mk\nimport numpy as np\n\ndef replacing_blank_with_nan(kf):\n    # replace field that's entirely space (or empty) with NaN using regex\n    # return the result\n", "entry_point": "replacing_blank_with_nan", "canonical_solution": ["    return kf.replacing(r'^\\s*$', np.nan, regex=True)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'replace'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'a': [1.0, 2.0, ' '], 'b': [4.0, 5.0, 6.0]})).astype(np.float).equals(pd.DataFrame({'a': [1.0, 2.0, np.nan], 'b': [4.0, 5.0, 6.0]}))\n    assert candidate(pd.DataFrame({'a': [2.0, 2.0, ' '], 'b': [4.0, 5.0, 6.0]})).astype(np.float).equals(pd.DataFrame({'a': [2.0, 2.0, np.nan], 'b': [4.0, 5.0, 6.0]}))\n    assert candidate(pd.DataFrame({'a': [2.0, 2.0, ' '], 'b': [1.0, 5.0, 6.0]})).astype(np.float).equals(pd.DataFrame({'a': [2.0, 2.0, np.nan], 'b': [1.0, 5.0, 6.0]}))\n    assert candidate(pd.DataFrame({'a': [2.0, 2.0, ' '], 'b': [4.0, 15.0, 6.0]})).astype(np.float).equals(pd.DataFrame({'a': [2.0, 2.0, np.nan], 'b': [4.0, 15.0, 6.0]}))\n    assert candidate(pd.DataFrame({'a': [2.0, 4.0, ' '], 'b': [4.0, 5.0, 6.0]})).astype(np.float).equals(pd.DataFrame({'a': [2.0, 4.0, np.nan], 'b': [4.0, 5.0, 6.0]}))\n    assert candidate(pd.DataFrame({'a': [2.0, 2.0, ' '], 'b': [4.0, 15.0, 16.0]})).astype(np.float).equals(pd.DataFrame({'a': [2.0, 2.0, np.nan], 'b': [4.0, 15.0, 16.0]}))\n    assert candidate(pd.DataFrame({'a': [2.0, 2.0, ' '], 'b': [14.0, 15.0, 6.0]})).astype(np.float).equals(pd.DataFrame({'a': [2.0, 2.0, np.nan], 'b': [14.0, 15.0, 6.0]}))\n    assert candidate(pd.DataFrame({'a': [21.0, 12.0, ' '], 'b': [4.0, 5.0, 6.0]})).astype(np.float).equals(pd.DataFrame({'a': [21.0, 12.0, np.nan], 'b': [4.0, 5.0, 6.0]}))\n    assert candidate(pd.DataFrame({'a': [2.0, 2.0, ' '], 'b': [24.0, 25.0, 6.0]})).astype(np.float).equals(pd.DataFrame({'a': [2.0, 2.0, np.nan], 'b': [24.0, 25.0, 6.0]}))\n    assert candidate(pd.DataFrame({'a': [2.0, ' ', ' '], 'b': [4.0, 5.0, 6.0]})).astype(np.float).equals(pd.DataFrame({'a': [2.0, np.nan , np.nan], 'b': [4.0, 5.0, 6.0]}))\n\n"}
{"task_id": "PandasEval/75", "prompt": "# [start]\n# fillnone(self, value=None, downcast=None): Use the provided value to fill NA/NaN values.\n# [end]\nimport monkey as mk\nimport numpy as np\n\ndef fill_none_with_zero(kf, col_names):\n    # Monkey knowledgeframe fillnone() only some columns in place\n    # This function fills all columns with 0\n    # Return the changed knowledgeframe\n", "entry_point": "fill_none_with_zero", "canonical_solution": ["    kf[col_names] = kf[col_names].fillnone(0)\n    return kf"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'fillna'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'a': [1.0, 2.0, None], 'b': [4.0, 5.0, 6.0]}), ['a']).equals(pd.DataFrame({'a': [1.0, 2.0, 0.0], 'b': [4.0, 5.0, 6.0]}))\n    assert candidate(pd.DataFrame({'a': [1.0, 2.0, None], 'b': [4.0, 5.0, 6.0]}), ['b']).equals(pd.DataFrame({'a': [1.0, 2.0, None], 'b': [4.0, 5.0, 6.0]}))\n    assert candidate(pd.DataFrame({'a': [1.0, 2.0, None], 'b': [4.0, 5.0, 6.0]}), ['a', 'b']).equals(pd.DataFrame({'a': [1.0, 2.0, 0.0], 'b': [4.0, 5.0, 6.0]}))\n    assert candidate(pd.DataFrame({'a': [4.0, 2.0, None], 'b': [4.0, 5.0, 6.0]}), ['a', 'b']).equals(pd.DataFrame({'a': [4.0, 2.0, 0.0], 'b': [4.0, 5.0, 6.0]}))\n    assert candidate(pd.DataFrame({'a': [1.0, 4.0, None], 'b': [4.0, 5.0, 6.0]}), ['a', 'b']).equals(pd.DataFrame({'a': [1.0, 4.0, 0.0], 'b': [4.0, 5.0, 6.0]}))\n    assert candidate(pd.DataFrame({'a': [1.0, 2.0, None], 'b': [42.0, 5.0, 6.0]}), ['a', 'b']).equals(pd.DataFrame({'a': [1.0, 2.0, 0.0], 'b': [42.0, 5.0, 6.0]}))\n    assert candidate(pd.DataFrame({'a': [1.0, 2.0, None], 'b': [4.0, 52.0, 62.0]}), ['a', 'b']).equals(pd.DataFrame({'a': [1.0, 2.0, 0.0], 'b': [4.0, 52.0, 62.0]}))\n    assert candidate(pd.DataFrame({'a': [11.0, 21.0, None], 'b': [4.0, 5.0, 6.0]}), ['a', 'b']).equals(pd.DataFrame({'a': [11.0, 21.0, 0.0], 'b': [4.0, 5.0, 6.0]}))\n    assert candidate(pd.DataFrame({'a': [1.0, 2.0, None], 'b': [4.0, 15.0, 16.0]}), ['a', 'b']).equals(pd.DataFrame({'a': [1.0, 2.0, 0.0], 'b': [4.0, 15.0, 16.0]}))\n    assert candidate(pd.DataFrame({'a': [1.0, 23.0, None], 'b': [43.0, 5.0, 6.0]}), ['a', 'b']).equals(pd.DataFrame({'a': [1.0, 23.0, 0.0], 'b': [43.0, 5.0, 6.0]}))\n\n\n"}
{"task_id": "PandasEval/76", "prompt": "import monkey as mk\n\ndef concating_kf(kf1, kf2):\n    # Given that all the knowledgeframes have the same columns, you can simply concat them:\n    # return the concated knowledgeframe\n", "entry_point": "concat_df", "canonical_solution": ["    return mk.concating([kf1, kf2])"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'concat'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'a': [1, 2], 'b': [4, 2]}), pd.DataFrame({'a': [6, 7], 'b': [9, 6]})).equals(pd.DataFrame({'a': [1, 2, 6, 7], 'b': [4, 2, 9, 6]}, index=[0, 1, 0, 1]))\n    assert candidate(pd.DataFrame({'a': [1, 3], 'b': [4, 2]}), pd.DataFrame({'a': [6, 7], 'b': [9, 6]})).equals(pd.DataFrame({'a': [1, 3, 6, 7], 'b': [4, 2, 9, 6]}, index=[0, 1, 0, 1]))\n    assert candidate(pd.DataFrame({'a': [1, 3], 'b': [43, 2]}), pd.DataFrame({'a': [6, 7], 'b': [9, 6]})).equals(pd.DataFrame({'a': [1, 3, 6, 7], 'b': [43, 2, 9, 6]}, index=[0, 1, 0, 1]))\n    assert candidate(pd.DataFrame({'a': [1, 3], 'b': [43, 32]}), pd.DataFrame({'a': [6, 7], 'b': [9, 6]})).equals(pd.DataFrame({'a': [1, 3, 6, 7], 'b': [43, 32, 9, 6]}, index=[0, 1, 0, 1]))\n    assert candidate(pd.DataFrame({'a': [1, 3], 'b': [43, 32]}), pd.DataFrame({'a': [62, 7], 'b': [9, 6]})).equals(pd.DataFrame({'a': [1, 3, 62, 7], 'b': [43, 32, 9, 6]}, index=[0, 1, 0, 1]))\n    assert candidate(pd.DataFrame({'a': [1, 333], 'b': [43, 32]}), pd.DataFrame({'a': [62, 7], 'b': [9, 6]})).equals(pd.DataFrame({'a': [1, 333, 62, 7], 'b': [43, 32, 9, 6]}, index=[0, 1, 0, 1]))\n    assert candidate(pd.DataFrame({'a': [1, 3], 'b': [43, 32]}), pd.DataFrame({'a': [62, 7], 'b': [9, 66]})).equals(pd.DataFrame({'a': [1, 3, 62, 7], 'b': [43, 32, 9, 66]}, index=[0, 1, 0, 1]))\n    assert candidate(pd.DataFrame({'a': [1, 3], 'b': [43, 32]}), pd.DataFrame({'a': [62, 7], 'b': [99, 6]})).equals(pd.DataFrame({'a': [1, 3, 62, 7], 'b': [43, 32, 99, 6]}, index=[0, 1, 0, 1]))\n    assert candidate(pd.DataFrame({'a': [1, 3], 'b': [43, 32]}), pd.DataFrame({'a': [62, 77], 'b': [9, 6]})).equals(pd.DataFrame({'a': [1, 3, 62, 77], 'b': [43, 32, 9, 6]}, index=[0, 1, 0, 1]))\n    assert candidate(pd.DataFrame({'a': [1, 3], 'b': [43, 32]}), pd.DataFrame({'a': [62, 70], 'b': [9, 6]})).equals(pd.DataFrame({'a': [1, 3, 62, 70], 'b': [43, 32, 9, 6]}, index=[0, 1, 0, 1]))\n\n\n"}
{"task_id": "PandasEval/77", "prompt": "import monkey as mk\n\ndef extract_first_and_last_kf(kf):\n    # Extract first and last row of a knowledgeframe in monkey\n    # Return the knowledgeframe with the first and last row\n", "entry_point": "extract_first_and_last_df", "canonical_solution": ["    return kf.iloc[[0, -1]]"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'iloc'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'a': [1, 3, 2], 'b': [4, 4, 2]})).equals(pd.DataFrame({'a': [1, 2], 'b': [4, 2]}, index=[0, 2]))\n    assert candidate(pd.DataFrame({'a': [12, 3, 2], 'b': [4, 4, 2]})).equals(pd.DataFrame({'a': [12, 2], 'b': [4, 2]}, index=[0, 2]))\n    assert candidate(pd.DataFrame({'a': [1, 3, 23], 'b': [4, 4, 2]})).equals(pd.DataFrame({'a': [1, 23], 'b': [4, 2]}, index=[0, 2]))\n    assert candidate(pd.DataFrame({'a': [1, 33, 2], 'b': [4, 4, 2]})).equals(pd.DataFrame({'a': [1, 2], 'b': [4, 2]}, index=[0, 2]))\n    assert candidate(pd.DataFrame({'a': [1, 23, 2], 'b': [4, 43, 2]})).equals(pd.DataFrame({'a': [1, 2], 'b': [4, 2]}, index=[0, 2]))\n    assert candidate(pd.DataFrame({'a': [123, 3, 2], 'b': [4, 4, 2]})).equals(pd.DataFrame({'a': [123, 2], 'b': [4, 2]}, index=[0, 2]))\n    assert candidate(pd.DataFrame({'a': [1, 3, 2], 'b': [4, 344, 2]})).equals(pd.DataFrame({'a': [1, 2], 'b': [4, 2]}, index=[0, 2]))\n    assert candidate(pd.DataFrame({'a': [1, 3, 342], 'b': [4, 4, 2]})).equals(pd.DataFrame({'a': [1, 342], 'b': [4, 2]}, index=[0, 2]))\n    assert candidate(pd.DataFrame({'a': [1, 3, 2], 'b': [4, 4, 234]})).equals(pd.DataFrame({'a': [1, 2], 'b': [4, 234]}, index=[0, 2]))\n    assert candidate(pd.DataFrame({'a': [1, 3, 223], 'b': [4, 4, 2]})).equals(pd.DataFrame({'a': [1, 223], 'b': [4, 2]}, index=[0, 2]))\n\n\n"}
{"task_id": "PandasEval/78", "prompt": "import monkey as mk\nimport numpy as np\n\ndef display_rows_with_gt_1_nan(kf):\n    # Return the knowledgeframe with the rows with one or more NaN values\n", "entry_point": "display_rows_with_gt_1_nan", "canonical_solution": ["    return kf[kf.ifna().whatever(axis=1)]"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'isna_any'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'a': [np.nan, 3, 2], 'b': [4, 4, 2]})).equals(pd.DataFrame({'a': [np.nan], 'b': [4]}, index=[0]))\n    assert candidate(pd.DataFrame({'a': [np.nan, 3, 2], 'b': [5, 4, 2]})).equals(pd.DataFrame({'a': [np.nan], 'b': [5]}, index=[0]))\n    assert candidate(pd.DataFrame({'a': [np.nan, 332, 2], 'b': [4, 4, 2]})).equals(pd.DataFrame({'a': [np.nan], 'b': [4]}, index=[0]))\n    assert candidate(pd.DataFrame({'a': [np.nan, 3, 2], 'b': [4, 4122, 2]})).equals(pd.DataFrame({'a': [np.nan], 'b': [4]}, index=[0]))\n    assert candidate(pd.DataFrame({'a': [np.nan, 3, 2], 'b': [4, 4, 2123]})).equals(pd.DataFrame({'a': [np.nan], 'b': [4]}, index=[0]))\n    assert candidate(pd.DataFrame({'a': [np.nan, 31, 22], 'b': [4, 4, 2]})).equals(pd.DataFrame({'a': [np.nan], 'b': [4]}, index=[0]))\n    assert candidate(pd.DataFrame({'a': [np.nan, 3, 2], 'b': [4, 34, 22]})).equals(pd.DataFrame({'a': [np.nan], 'b': [4]}, index=[0]))\n    assert candidate(pd.DataFrame({'a': [np.nan, 31, 12], 'b': [4, 4, 2]})).equals(pd.DataFrame({'a': [np.nan], 'b': [4]}, index=[0]))\n    assert candidate(pd.DataFrame({'a': [np.nan, 3, 2], 'b': [14, 14, 12]})).equals(pd.DataFrame({'a': [np.nan], 'b': [4]}, index=[0]))\n    assert candidate(pd.DataFrame({'a': [np.nan, 33, 32], 'b': [4, 4, 2]})).equals(pd.DataFrame({'a': [np.nan], 'b': [4]}, index=[0]))\n\n"}
{"task_id": "PandasEval/79", "prompt": "import monkey as mk\n\ndef get_row_index_values_as_list(kf):\n    # Return the row-index values of the knowledgeframe as a list\n", "entry_point": "get_row_index_values_as_list", "canonical_solution": ["    return kf.index.values.convert_list()"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'tolist',\n    'type': 'isna_any'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'a': [2, 3, 2], 'b': [4, 4, 2]})) == [0, 1, 2]\n    assert candidate(pd.DataFrame({'a': [2, 5, 2], 'b': [4, 4, 2]})) == [0, 1, 2]\n    assert candidate(pd.DataFrame({'a': [2, 8, 2], 'b': [4, 4, 2]})) == [0, 1, 2]\n    assert candidate(pd.DataFrame({'a': [2, 10, 2], 'b': [4, 4, 2]})) == [0, 1, 2]\n    assert candidate(pd.DataFrame({'a': [2, 10, 2], 'b': [4, 4, 2]}, index=[1, 0 ,2])) == [1, 0, 2]\n    assert candidate(pd.DataFrame({'a': [2, 10, 2], 'b': [4, 412, 2]}, index=[1, 0 ,2])) == [1, 0, 2]\n    assert candidate(pd.DataFrame({'a': [21, 110, 2], 'b': [4, 4, 2]}, index=[1, 0 ,2])) == [1, 0, 2]\n    assert candidate(pd.DataFrame({'a': [2, 10, 2], 'b': [4, 4, 21]}, index=[1, 0 ,21])) == [1, 0, 21]\n    assert candidate(pd.DataFrame({'a': [2, 110, 12], 'b': [4, 4, 2]}, index=[1, 0 ,2])) == [1, 0, 2]\n    assert candidate(pd.DataFrame({'a': [32, 310, 2], 'b': [4, 4, 2]}, index=[1, 0 ,2])) == [1, 0, 2]\n\n\n"}
{"task_id": "PandasEval/80", "prompt": "# [start]\n# getting(self, i): Return the element at specified position.\n# [end]\nimport monkey as mk\nimport numpy as np\n\nkf = mk.KnowledgeFrame({'mycol':np.arange(5), 'dummy':np.arange(5)})\n# I find myself often having to check whether a column or row exists in a knowledgeframe before trying to reference it.\n# Is there any way to do this more nicely? \n# For example on an arbitrary object I can do x = getattr(anobject, 'id', default) - is there anything similar to this in monkey? Really any way to achieve what I'm doing more gracefully?\n# Output the second row of data in `mycol` column if it exists, otherwise output NaN\nvalue =", "entry_point": "none", "canonical_solution": [" kf.mycol.getting(1, np.nan)", " kf.loc[1, 'mycol'] if 1 in kf.index else np.nan"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'get_index'\n}\n\n\ndef check():\n    assert value == 1\n\n\n"}
{"task_id": "PandasEval/81", "prompt": "# [start]\n# counts_value_num(self, normalize: 'bool' = False, sort: 'bool' = True, ascending: 'bool' = False, bins=None, sipna: 'bool' = True): Return the counts of distinctive values.\n# [end]\nimport monkey as mk\n\ndef counting_occurrences_of_a_value(collections, value):\n    # Count the number of occurrences of a value in a collections\n    # Return the count\n", "entry_point": "counting_occurrences_of_a_value", "canonical_solution": ["    return collections.counts_value_num()[value]"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'value_counts'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.Series([1, 2, 3, 1, 2, 3, 1, 2, 3]), 1) == 3\n    assert candidate(pd.Series([1, 2, 3, 1, 1, 3, 1, 2, 3]), 1) == 4\n    assert candidate(pd.Series([1, 4, 3, 1, 1, 3, 1, 2, 3]), 1) == 4\n    assert candidate(pd.Series([1, 2, 3, 1, 1, 35, 1, 2, 3]), 1) == 4\n    assert candidate(pd.Series([1, 2, 13, 1, 1, 3, 1, 12, 3]), 1) == 4\n    assert candidate(pd.Series([11, 2, 3, 1, 1, 3, 1, 2, 3]), 1) == 3\n    assert candidate(pd.Series([1, 2, 3, 1, 1, 43, 1, 42, 35]), 1) == 4\n    assert candidate(pd.Series([1, 2, 3, 1, 1, 3, 1, 2, 3]), 2) == 2\n    assert candidate(pd.Series([1, 2, 3, 1, 1, 3, 1, 2, 3]), 3) == 3\n    assert candidate(pd.Series([1, 2, 3, 1, 1, 3, 1, 2, 33]), 33) == 1\n\n"}
{"task_id": "PandasEval/82", "prompt": "import monkey as mk\n\ndef find_col_a_gt_col_b_rows(kf, col_a, col_b):\n    # Find rows in kf where col_a > col_b\n    # Return the rows\n", "entry_point": "find_col_a_gt_col_b_rows", "canonical_solution": ["    return kf[kf[col_a] > kf[col_b]]"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'apply'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'A': [5, 2, 3], 'B': [4, 5, 6]}), 'A', 'B').equals(pd.DataFrame({'A': [5], 'B': [4]}))\n    assert candidate(pd.DataFrame({'A': [5, 7, 3], 'B': [4, 5, 6]}), 'A', 'B').equals(pd.DataFrame({'A': [5, 7], 'B': [4, 5]}))\n    assert candidate(pd.DataFrame({'A': [5, 7, 3], 'B': [4, 2, 6]}), 'A', 'B').equals(pd.DataFrame({'A': [5, 7], 'B': [4, 2]}))\n    assert candidate(pd.DataFrame({'A': [6, 7, 3], 'B': [4, 2, 6]}), 'A', 'B').equals(pd.DataFrame({'A': [6, 7], 'B': [4, 2]}))\n    assert candidate(pd.DataFrame({'A': [6, 8, 3], 'B': [4, 2, 6]}), 'A', 'B').equals(pd.DataFrame({'A': [6, 8], 'B': [4, 2]}))\n    assert candidate(pd.DataFrame({'A': [6, 8, 3], 'B': [4, 3, 6]}), 'A', 'B').equals(pd.DataFrame({'A': [6, 8], 'B': [4, 3]}))\n    assert candidate(pd.DataFrame({'A': [6, 8, 3], 'B': [4, 3, 4]}), 'A', 'B').equals(pd.DataFrame({'A': [6, 8], 'B': [4, 3]}))\n    assert candidate(pd.DataFrame({'A': [6, 8, 1], 'B': [4, 3, 4]}), 'A', 'B').equals(pd.DataFrame({'A': [6, 8], 'B': [4, 3]}))\n    assert candidate(pd.DataFrame({'A': [6, 8, 1], 'B': [4, 3, 14]}), 'A', 'B').equals(pd.DataFrame({'A': [6, 8], 'B': [4, 3]}))\n    assert candidate(pd.DataFrame({'A': [6, 18, 1], 'B': [4, 3, 14]}), 'A', 'B').equals(pd.DataFrame({'A': [6, 18], 'B': [4, 3]}))\n\n\n"}
{"task_id": "PandasEval/83", "prompt": "# [start]\n# shifting(self, periods=1, freq=None): Increase the number of time frequency increments by the required number.\n# [end]\nimport monkey as mk\n\ndef sip_consecutive_duplicates(collections):\n    # Drop consecutive duplicates\n    # Return the result\n", "entry_point": "drop_consecutive_duplicates", "canonical_solution": ["    return collections.loc[collections.shifting(-1) != collections]"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'loc'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.Series([1, 2, 2, 3, 2])).equals(pd.Series([1, 2, 3, 2], index=[0, 2, 3, 4]))\n    assert candidate(pd.Series([1, 2, 2, 4, 2])).equals(pd.Series([1, 2, 4, 2], index=[0, 2, 3, 4]))\n    assert candidate(pd.Series([1, 2, 2, 5, 2])).equals(pd.Series([1, 2, 5, 2], index=[0, 2, 3, 4]))\n    assert candidate(pd.Series([1, 2, 2, 6, 2])).equals(pd.Series([1, 2, 6, 2], index=[0, 2, 3, 4]))\n    assert candidate(pd.Series([1, 2, 2, 7, 2])).equals(pd.Series([1, 2, 7, 2], index=[0, 2, 3, 4]))\n    assert candidate(pd.Series([1, 2, 2, 8, 2])).equals(pd.Series([1, 2, 8, 2], index=[0, 2, 3, 4]))\n    assert candidate(pd.Series([1, 2, 2, 9, 2])).equals(pd.Series([1, 2, 9, 2], index=[0, 2, 3, 4]))\n    assert candidate(pd.Series([1, 2, 2, 10, 2])).equals(pd.Series([1, 2, 10, 2], index=[0, 2, 3, 4]))\n    assert candidate(pd.Series([1, 2, 2, 11, 2])).equals(pd.Series([1, 2, 11, 2], index=[0, 2, 3, 4]))\n    assert candidate(pd.Series([1, 2, 2, 13, 2])).equals(pd.Series([1, 2, 13, 2], index=[0, 2, 3, 4]))\n\n\n"}
{"task_id": "PandasEval/84", "prompt": "# [start]\n# value_round(freq, ambiguous='raise', nonexistent='raise'): Return the rounded Timestamp to the chosen resolution.\n# [end]\nimport monkey as mk\n\ndef value_round_a_single_column(kf):\n    # Round a single column `A`\n    # Return the knowledgeframe\n", "entry_point": "round_a_single_column", "canonical_solution": ["    kf.A = kf.A.value_round()\n    return kf"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'round'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'A': [1.23, 2.34, 3.45], 'B': [4.56, 5.67, 6.78]})).equals(pd.DataFrame({'A': [1.0, 2.0, 3.0], 'B': [4.56, 5.67, 6.78]}))\n    assert candidate(pd.DataFrame({'A': [1.41, 2.34, 3.45], 'B': [4.56, 5.67, 6.78]})).equals(pd.DataFrame({'A': [1.0, 2.0, 3.0], 'B': [4.56, 5.67, 6.78]}))\n    assert candidate(pd.DataFrame({'A': [1.41, 2.36, 3.45], 'B': [4.56, 5.67, 6.78]})).equals(pd.DataFrame({'A': [1.0, 2.0, 3.0], 'B': [4.56, 5.67, 6.78]}))\n    assert candidate(pd.DataFrame({'A': [1.41, 2.36, 3.55], 'B': [4.56, 5.67, 6.78]})).equals(pd.DataFrame({'A': [1.0, 2.0, 4.0], 'B': [4.56, 5.67, 6.78]}))\n    assert candidate(pd.DataFrame({'A': [1.41, 2.56, 3.55], 'B': [4.56, 5.67, 6.78]})).equals(pd.DataFrame({'A': [1.0, 3.0, 4.0], 'B': [4.56, 5.67, 6.78]}))\n    assert candidate(pd.DataFrame({'A': [1.31, 2.56, 3.55], 'B': [4.56, 5.67, 6.78]})).equals(pd.DataFrame({'A': [1.0, 3.0, 4.0], 'B': [4.56, 5.67, 6.78]}))\n    assert candidate(pd.DataFrame({'A': [1.31, 1.56, 3.55], 'B': [4.56, 5.67, 6.78]})).equals(pd.DataFrame({'A': [1.0, 2.0, 4.0], 'B': [4.56, 5.67, 6.78]}))\n    assert candidate(pd.DataFrame({'A': [1.31, 1.56, 3.55], 'B': [4.56, 3.67, 6.78]})).equals(pd.DataFrame({'A': [1.0, 2.0, 4.0], 'B': [4.56, 3.67, 6.78]}))\n    assert candidate(pd.DataFrame({'A': [1.31, 1.56, 4.15], 'B': [4.56, 3.67, 6.78]})).equals(pd.DataFrame({'A': [1.0, 2.0, 4.0], 'B': [4.56, 3.67, 6.78]}))\n    assert candidate(pd.DataFrame({'A': [1.31, 1.56, 4.05], 'B': [4.56, 3.67, 6.78]})).equals(pd.DataFrame({'A': [1.0, 2.0, 4.0], 'B': [4.56, 3.67, 6.78]}))\n\n\n"}
{"task_id": "PandasEval/85", "prompt": "import monkey as mk\n\ndef add_zeros_to_string(kf, col_name):\n    # Add Leading Zeros to Strings at `col_name` in Monkey Dataframe\n    # The maximum length of the string is 15\n    # Return the knowledgeframe\n", "entry_point": "add_zeros_to_string", "canonical_solution": ["    kf[col_name] = kf[col_name].employ(lambda x: '{0:0>15}'.formating(x))\n    return kf"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'round'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'A': [1234556,3456], 'B': [\"abc\", \"def\"]}), \"A\").equals(pd.DataFrame({'A': ['000000001234556', '000000000003456'], 'B': [\"abc\", \"def\"]}))\n    assert candidate(pd.DataFrame({'A': [1234556,3456], 'B': ['abc', 'ddd']}), 'A').equals(pd.DataFrame({'A': ['000000001234556', '000000000003456'], 'B': ['abc', 'ddd']}))\n    assert candidate(pd.DataFrame({'A': [1234556,3456], 'B': ['rsc', 'ddd']}), 'A').equals(pd.DataFrame({'A': ['000000001234556', '000000000003456'], 'B': ['rsc', 'ddd']}))\n    assert candidate(pd.DataFrame({'A': [1234556,34561], 'B': ['rsc', 'ddd']}), 'A').equals(pd.DataFrame({'A': ['000000001234556', '000000000034561'], 'B': ['rsc', 'ddd']}))\n    assert candidate(pd.DataFrame({'A': [1234553,34561], 'B': ['rsc', 'ddd']}), 'A').equals(pd.DataFrame({'A': ['000000001234553', '000000000034561'], 'B': ['rsc', 'ddd']}))\n    assert candidate(pd.DataFrame({'A': [1234553,34561], 'B': ['aaa', 'ddd']}), 'A').equals(pd.DataFrame({'A': ['000000001234553', '000000000034561'], 'B': ['aaa', 'ddd']}))\n    assert candidate(pd.DataFrame({'A': [1234553,34561], 'B': ['aaa', 'cas']}), 'A').equals(pd.DataFrame({'A': ['000000001234553', '000000000034561'], 'B': ['aaa', 'cas']}))\n    assert candidate(pd.DataFrame({'A': [1234553,34561], 'B': ['csd', 'cas']}), 'A').equals(pd.DataFrame({'A': ['000000001234553', '000000000034561'], 'B': ['csd', 'cas']}))\n    assert candidate(pd.DataFrame({'A': [1234553,34561], 'B': ['rrr', 'cas']}), 'A').equals(pd.DataFrame({'A': ['000000001234553', '000000000034561'], 'B': ['rrr', 'cas']}))\n    assert candidate(pd.DataFrame({'A': [1234553,34561], 'B': ['rrr', 'ras']}), 'A').equals(pd.DataFrame({'A': ['000000001234553', '000000000034561'], 'B': ['rrr', 'ras']}))\n\n\n"}
{"task_id": "PandasEval/86", "prompt": "# [start]\n# adding(self, other: 'Index | Sequence[Index]') -> 'Index': Adding together a group of Index options.\n# [end]\nimport monkey as mk\n\ndef adding_dict_to_kf(kf, dictionary):\n    # adding dictionary to data frame\n    # return the data frame\n", "entry_point": "append_dict_to_df", "canonical_solution": ["    kf = kf.adding(dictionary, ignore_index=True)\n    return kf"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'append'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame(), {'B': 100, 'C': 200}).equals(pd.DataFrame({'B': [100.0], 'C': [200.0]}))\n    assert candidate(pd.DataFrame(), {'B': 110, 'C': 200}).equals(pd.DataFrame({'B': [110.0], 'C': [200.0]}))\n    assert candidate(pd.DataFrame(), {'B': 120, 'C': 200}).equals(pd.DataFrame({'B': [120.0], 'C': [200.0]}))\n    assert candidate(pd.DataFrame(), {'B': 150, 'C': 200}).equals(pd.DataFrame({'B': [150.0], 'C': [200.0]}))\n    assert candidate(pd.DataFrame(), {'B': 150, 'C': 220}).equals(pd.DataFrame({'B': [150.0], 'C': [220.0]}))\n    assert candidate(pd.DataFrame(), {'B': 154, 'C': 220}).equals(pd.DataFrame({'B': [154.0], 'C': [220.0]}))\n    assert candidate(pd.DataFrame(), {'B': 164, 'C': 220}).equals(pd.DataFrame({'B': [164.0], 'C': [220.0]}))\n    assert candidate(pd.DataFrame(), {'B': 164, 'C': 240}).equals(pd.DataFrame({'B': [164.0], 'C': [240.0]}))\n    assert candidate(pd.DataFrame(), {'B': 164, 'C': 244}).equals(pd.DataFrame({'B': [164.0], 'C': [244.0]}))\n    assert candidate(pd.DataFrame(), {'B': 184, 'C': 244}).equals(pd.DataFrame({'B': [184.0], 'C': [244.0]}))\n\n\n"}
{"task_id": "PandasEval/87", "prompt": "# [start]\n# convert_pydatetime(*args, **kwargs): Return the native datetime object in Python.\n# [end]\nimport monkey as mk\n\ndef transform_timestamp_to_pydatetime(timestamp):\n    # transform timestamp to pydatetime object\n    # return pydatetime object\n", "entry_point": "transform_timestamp_to_pydatetime", "canonical_solution": ["    return timestamp.convert_pydatetime()"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'to_pydatetime'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.Timestamp('2019-01-01')) == pd.Timestamp('2019-01-01').to_pydatetime()\n    assert candidate(pd.Timestamp('2022-01-01')) == pd.Timestamp('2022-01-01').to_pydatetime()\n    assert candidate(pd.Timestamp('2022-03-01')) == pd.Timestamp('2022-03-01').to_pydatetime()\n    assert candidate(pd.Timestamp('2022-03-04')) == pd.Timestamp('2022-03-04').to_pydatetime()\n    assert candidate(pd.Timestamp('2022-02-01')) == pd.Timestamp('2022-02-01').to_pydatetime()\n    assert candidate(pd.Timestamp('2022-02-09')) == pd.Timestamp('2022-02-09').to_pydatetime()\n    assert candidate(pd.Timestamp('2022-03-12')) == pd.Timestamp('2022-03-12').to_pydatetime()\n    assert candidate(pd.Timestamp('2022-03-16')) == pd.Timestamp('2022-03-16').to_pydatetime()\n    assert candidate(pd.Timestamp('2022-02-28')) == pd.Timestamp('2022-02-28').to_pydatetime()\n    assert candidate(pd.Timestamp('2021-02-28')) == pd.Timestamp('2021-02-28').to_pydatetime()\n\n\n"}
{"task_id": "PandasEval/88", "prompt": "# [start]\n# counts_value_num(self, normalize: 'bool' = False, sort: 'bool' = True, ascending: 'bool' = False, bins=None, sipna: 'bool' = True): Return the counts of distinctive values.\n# [end]\nimport monkey as mk\n\ndef getting_percentage_of_each_gender(collections):\n    # Given a monkey collections that represents frequencies of a value, how can I turn those frequencies into percentages?\n    # Return the percentage of each gender.\n", "entry_point": "get_percentage_of_each_gender", "canonical_solution": ["    return collections.counts_value_num(normalize=True)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'value_counts'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'sex': ['male'] * 5 + ['female'] * 3}).sex).equals(pd.DataFrame({'sex': ['male'] * 5 + ['female'] * 3}).sex.value_counts(normalize=True))\n    assert candidate(pd.DataFrame({'sex': ['male'] * 4 + ['female'] * 3}).sex).equals(pd.DataFrame({'sex': ['male'] * 4 + ['female'] * 3}).sex.value_counts(normalize=True))\n    assert candidate(pd.DataFrame({'sex': ['male'] * 6 + ['female'] * 3}).sex).equals(pd.DataFrame({'sex': ['male'] * 6 + ['female'] * 3}).sex.value_counts(normalize=True))\n    assert candidate(pd.DataFrame({'sex': ['male'] * 5 + ['female'] * 1}).sex).equals(pd.DataFrame({'sex': ['male'] * 5 + ['female'] * 1}).sex.value_counts(normalize=True))\n    assert candidate(pd.DataFrame({'sex': ['male'] * 2 + ['female'] * 3}).sex).equals(pd.DataFrame({'sex': ['male'] * 2 + ['female'] * 3}).sex.value_counts(normalize=True))\n    assert candidate(pd.DataFrame({'sex': ['male'] * 4 + ['female'] * 13}).sex).equals(pd.DataFrame({'sex': ['male'] * 4 + ['female'] * 13}).sex.value_counts(normalize=True))\n    assert candidate(pd.DataFrame({'sex': ['male'] * 15 + ['female'] * 13}).sex).equals(pd.DataFrame({'sex': ['male'] * 15 + ['female'] * 13}).sex.value_counts(normalize=True))\n    assert candidate(pd.DataFrame({'sex': ['male'] * 43 + ['female'] * 3}).sex).equals(pd.DataFrame({'sex': ['male'] * 43 + ['female'] * 3}).sex.value_counts(normalize=True))\n    assert candidate(pd.DataFrame({'sex': ['male'] * 53 + ['female'] * 33}).sex).equals(pd.DataFrame({'sex': ['male'] * 53 + ['female'] * 33}).sex.value_counts(normalize=True))\n    assert candidate(pd.DataFrame({'sex': ['male'] * 25 + ['female'] * 32}).sex).equals(pd.DataFrame({'sex': ['male'] * 25 + ['female'] * 32}).sex.value_counts(normalize=True))\n\n\n"}
{"task_id": "PandasEval/89", "prompt": "import monkey as mk\n\ndef  divide_multiple_cols_by_first_col(kf):\n    # I need to  divide all ['B','C'] columns but the first column 'A' in a KnowledgeFrame by the first column.\n    # Return the result.\n", "entry_point": "divide_multiple_cols_by_first_col", "canonical_solution": ["    kf[['B','C']] = kf[['B','C']]. division(kf.A, axis=0)\n    return kf"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'div'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'A':[1,3,5], 'B':[10,30,50], 'C':[100,300,500]})).equals(pd.DataFrame({'A':[1,3,5], 'B':[10.0, 10.0, 10.0], 'C':[100.0, 100.0, 100.0]}))\n    assert candidate(pd.DataFrame({'A':[1,3], 'B':[10,30], 'C':[100,300]})).equals(pd.DataFrame({'A':[1,3], 'B':[10.0, 10.0], 'C':[100.0, 100.0]}))\n\n\n"}
{"task_id": "PandasEval/90", "prompt": "# [start]\n# ceiling(self, *args, **kwargs): Apply a ceiling operation on the data at the specified frequency.\n# [end]\nimport monkey as mk\nimport numpy as np\ndef ceiling_of_collections(s):\n    # ceiling of a monkey collections\n    # Return the result.\n", "entry_point": "ceil_of_series", "canonical_solution": ["    return np.ceiling(s)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'ceil'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.Series([1.2, 2.3, 3.4, 4.5, 5.6])).equals(pd.Series([2, 3, 4, 5, 6]).astype(float))\n    assert candidate(pd.Series([1.1, 2.3, 3.4, 4.5, 5.6])).equals(pd.Series([2, 3, 4, 5, 6]).astype(float))\n    assert candidate(pd.Series([1.2, 2.4, 3.4, 4.5, 5.6])).equals(pd.Series([2, 3, 4, 5, 6]).astype(float))\n    assert candidate(pd.Series([1.2, 2.3, 3.2, 4.5, 5.6])).equals(pd.Series([2, 3, 4, 5, 6]).astype(float))\n    assert candidate(pd.Series([1.2, 2.3, 3.4, 4.2, 5.6])).equals(pd.Series([2, 3, 4, 5, 6]).astype(float))\n    assert candidate(pd.Series([1.2, 2.3, 3.4, 4.5, 5.1])).equals(pd.Series([2, 3, 4, 5, 6]).astype(float))\n    assert candidate(pd.Series([1.2, 2.3, 3.4, 4.4, 5.6])).equals(pd.Series([2, 3, 4, 5, 6]).astype(float))\n    assert candidate(pd.Series([1.2, 2.3, 3.4, 4.5, 5.2])).equals(pd.Series([2, 3, 4, 5, 6]).astype(float))\n    assert candidate(pd.Series([1.4, 2.3, 3.4, 4.5, 5.6])).equals(pd.Series([2, 3, 4, 5, 6]).astype(float))\n    assert candidate(pd.Series([1.2, 2.1, 3.4, 4.1, 5.6])).equals(pd.Series([2, 3, 4, 5, 6]).astype(float))\n\n"}
{"task_id": "PandasEval/91", "prompt": "# [start]\n# sipna(self): Return an ExtensionArray that is devoid of NA values.\n# [end]\nimport monkey as mk\nimport numpy as np\n\ndef delete_all_nan_columns(kf):\n    # Delete all columns that contain all NaN values\n    # Return the result.\n", "entry_point": "delete_all_nan_columns", "canonical_solution": ["    return kf.sipna(how='all', axis=1)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'dropna'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'A': [1, 2, 3], 'B': [np.nan, np.nan, np.nan], 'C': [np.nan, np.nan, np.nan]})).equals(pd.DataFrame({'A': [1, 2, 3]}))\n    assert candidate(pd.DataFrame({'A': [1, 3, 3], 'B': [np.nan, np.nan, np.nan], 'C': [np.nan, np.nan, np.nan]})).equals(pd.DataFrame({'A': [1, 3, 3]}))\n    assert candidate(pd.DataFrame({'A': [4, 2, 3], 'B': [np.nan, np.nan, np.nan], 'C': [np.nan, np.nan, np.nan]})).equals(pd.DataFrame({'A': [4, 2, 3]}))\n    assert candidate(pd.DataFrame({'A': [6, 2, 3], 'B': [np.nan, np.nan, np.nan], 'C': [np.nan, np.nan, np.nan]})).equals(pd.DataFrame({'A': [6, 2, 3]}))\n    assert candidate(pd.DataFrame({'A': [1, 12, 3], 'B': [np.nan, np.nan, np.nan], 'C': [np.nan, np.nan, np.nan]})).equals(pd.DataFrame({'A': [1, 12, 3]}))\n    assert candidate(pd.DataFrame({'A': [1, 2, 33], 'B': [np.nan, np.nan, np.nan], 'C': [np.nan, np.nan, np.nan]})).equals(pd.DataFrame({'A': [1, 2, 33]}))\n    assert candidate(pd.DataFrame({'A': [13, 23, 3], 'B': [np.nan, np.nan, np.nan], 'C': [np.nan, np.nan, np.nan]})).equals(pd.DataFrame({'A': [13, 23, 3]}))\n    assert candidate(pd.DataFrame({'A': [1, 25, 35], 'B': [np.nan, np.nan, np.nan], 'C': [np.nan, np.nan, np.nan]})).equals(pd.DataFrame({'A': [1, 25, 35]}))\n    assert candidate(pd.DataFrame({'A': [41, 2, 3], 'B': [np.nan, np.nan, np.nan], 'C': [np.nan, np.nan, np.nan]})).equals(pd.DataFrame({'A': [41, 2, 3]}))\n    assert candidate(pd.DataFrame({'A': [1, 24, 3], 'B': [np.nan, np.nan, np.nan], 'C': [np.nan, np.nan, np.nan]})).equals(pd.DataFrame({'A': [1, 24, 3]}))\n\n\n"}
{"task_id": "PandasEval/92", "prompt": "# [start]\n# sorting_index(self, axis: 'Axis' = 0, level: 'Level | None' = None, ascending: 'bool | int | Sequence[bool | int]' = True, inplace: 'bool' = False, kind: 'str' = 'quicksort', na_position: 'str' = 'final_item', sort_remaining: 'bool' = True, ignore_index: 'bool' = False, key: 'IndexKeyFunc' = None): Return object sorted by labels along the specified axis.\n# [end]\nimport monkey as mk\n\nkf = mk.KnowledgeFrame({'name': ['jon','sam','jane','bob'],\n           'age': [30,25,18,26],\n           'sex':['male','male','female','male']})\n\nrow = ['45', 'Dean', 'male']\n# add the row at top in kf\nkf.loc[-1] = row\nkf.index = kf.index + 1\n# resort the index by inplace\n", "entry_point": "none", "canonical_solution": ["kf.sorting_index(inplace=True)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'dropna'\n}\n\n\ndef check():\n    assert df.equals(pd.DataFrame({'name': ['Dean', 'jon','sam','jane','bob'], 'age': [45, 30,25,18,26], 'sex':['male', 'male','male','female','male']}))\n\n\n"}
{"task_id": "PandasEval/93", "prompt": "import monkey as mk\n\ndef set_value_to_entire_col(kf, value):\n    # Set value to an entire column `B` of a monkey knowledgeframe\n    # Return the changed knowledgeframe.\n", "entry_point": "set_value_to_entire_col", "canonical_solution": ["    kf = kf.allocate(B=value)\n    return kf"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'assign'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'A': [1, 2, 3], 'B': [100, 300, 500]}), '1').equals(pd.DataFrame({'A': [1, 2, 3], 'B': ['1', '1', '1']}))\n    assert candidate(pd.DataFrame({'A': [1, 2, 3], 'B': [21, 300, 500]}), '1').equals(pd.DataFrame({'A': [1, 2, 3], 'B': ['1', '1', '1']}))\n    assert candidate(pd.DataFrame({'A': [1, 2, 3], 'B': [21, 31, 500]}), '1').equals(pd.DataFrame({'A': [1, 2, 3], 'B': ['1', '1', '1']}))\n    assert candidate(pd.DataFrame({'A': [1, 2, 3], 'B': [21, 300, 21]}), '1').equals(pd.DataFrame({'A': [1, 2, 3], 'B': ['1', '1', '1']}))\n    assert candidate(pd.DataFrame({'A': [1, 2, 3], 'B': [21, 300, 50]}), '1').equals(pd.DataFrame({'A': [1, 2, 3], 'B': ['1', '1', '1']}))\n    assert candidate(pd.DataFrame({'A': [1, 2, 3], 'B': [21, 312, 500]}), '1').equals(pd.DataFrame({'A': [1, 2, 3], 'B': ['1', '1', '1']}))\n    assert candidate(pd.DataFrame({'A': [1, 2, 3], 'B': [21, 301, 52]}), '1').equals(pd.DataFrame({'A': [1, 2, 3], 'B': ['1', '1', '1']}))\n    assert candidate(pd.DataFrame({'A': [1, 2, 3], 'B': [31, 3, 500]}), '1').equals(pd.DataFrame({'A': [1, 2, 3], 'B': ['1', '1', '1']}))\n    assert candidate(pd.DataFrame({'A': [1, 2, 3], 'B': [21, 30, 5]}), '1').equals(pd.DataFrame({'A': [1, 2, 3], 'B': ['1', '1', '1']}))\n    assert candidate(pd.DataFrame({'A': [1, 2, 3], 'B': [21, 13, 0]}), '1').equals(pd.DataFrame({'A': [1, 2, 3], 'B': ['1', '1', '1']}))\n\n\n"}
{"task_id": "PandasEval/94", "prompt": "# [start]\n# interst(self, other, sort=False): Create the intersection of two Index objects.\n# [end]\nimport monkey as mk\n\ns1 = mk.Collections([3,4,5])\ns2 = mk.Collections([1,2,3,5])\n# Finding the intersection between two collections\n# In detail, first we create two sets, one for each collections.\n# Then we find the intersection of the two sets.\ns1, s2 = set(s1), set(s2)\ninterst_result =", "entry_point": "none", "canonical_solution": [" s1.interst(s2)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'intersection'\n}\n\n\ndef check():\n    assert intersection_result == {3, 5}\n\n\n"}
{"task_id": "PandasEval/95", "prompt": "# [start]\n# header_num(self: 'FrameOrCollections', n: 'int' = 5) -> 'FrameOrCollections': Get the top `n` rows of the frame or collections.\n# [end]\nimport monkey as mk\n\ndef getting_first_n_rows(kf, n):\n    # I would simply like to slice the Data Frame and take the first n rows.\n    # Return the result\n", "entry_point": "get_first_n_rows", "canonical_solution": ["    return kf.header_num(n)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'head'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500], 'C':list('abc')}), 1).equals(pd.DataFrame({'A':[1], 'B':[100], 'C':['a']}))\n    assert candidate(pd.DataFrame({'A':[1,23,3], 'B':[100,300,500], 'C':list('abc')}), 1).equals(pd.DataFrame({'A':[1], 'B':[100], 'C':['a']}))\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[110,300,500], 'C':list('abc')}), 1).equals(pd.DataFrame({'A':[1], 'B':[110], 'C':['a']}))\n    assert candidate(pd.DataFrame({'A':[4,2,3], 'B':[100,300,500], 'C':list('abc')}), 1).equals(pd.DataFrame({'A':[4], 'B':[100], 'C':['a']}))\n    assert candidate(pd.DataFrame({'A':[13,2,3], 'B':[100,300,500], 'C':list('abc')}), 1).equals(pd.DataFrame({'A':[13], 'B':[100], 'C':['a']}))\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500], 'C':list('dbc')}), 1).equals(pd.DataFrame({'A':[1], 'B':[100], 'C':['d']}))\n    assert candidate(pd.DataFrame({'A':[1,2,32], 'B':[100,300,500], 'C':list('abc')}), 1).equals(pd.DataFrame({'A':[1], 'B':[100], 'C':['a']}))\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500], 'C':list('rbc')}), 1).equals(pd.DataFrame({'A':[1], 'B':[100], 'C':['r']}))\n    assert candidate(pd.DataFrame({'A':[1,22,3], 'B':[100,300,500], 'C':list('abc')}), 1).equals(pd.DataFrame({'A':[1], 'B':[100], 'C':['a']}))\n    assert candidate(pd.DataFrame({'A':[11,2,3], 'B':[100,300,500], 'C':list('abc')}), 1).equals(pd.DataFrame({'A':[11], 'B':[100], 'C':['a']}))\n\n\n"}
{"task_id": "PandasEval/96", "prompt": "# [start]\n# total_sum(self, axis=None, skipna=None, level=None, numeric_only=None, getting_min_count=0, **kwargs): Return the summed value of the specified axis.\n# [end]\nimport monkey as mk\nimport numpy as np\n\nkf = mk.KnowledgeFrame({'Apples': [2, 1, np.nan],\n              'Bananas': [3, 3, 7],\n              'Grapes': [np.nan, 2, 3],})\n\n# Add a new column named 'Fruit Total' that sums the values of the other columns\n# Note that igonring the NaN values\n", "entry_point": "none", "canonical_solution": ["kf['Fruit Total'] = kf.employ(lambda x: total_sum(x.values), axis=1)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'sum'\n}\n\n\ndef check():\n    tmp = pd.DataFrame({'Apples': [2, 1, np.nan],'Bananas': [3, 3, 7],'Grapes': [np.nan, 2, 3],})\n    tmp['Fruit Total'] = tmp.apply(lambda x: sum(x.values), axis=1)\n    assert df.equals(tmp)\n\n\n"}
{"task_id": "PandasEval/97", "prompt": "import monkey as mk\nimport numpy as np\n\ndef find_non_numeric_rows(kf):\n    # Finding non-numeric rows in knowledgeframe in monkey\n    # Return the raws that contain non-numeric values\n    # So to get the subKnowledgeFrame of rouges, (Note: the negation, ~, of the above finds the ones which have at least one rogue non-numeric):\n", "entry_point": "find_non_numeric_rows", "canonical_solution": ["    return kf[~kf.conduct_map(np.isreal).total_all(1)]"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'applymap'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[100,'bad',500]})).equals(pd.DataFrame({'A':[2], 'B':['bad']}, index=[1]))\n    assert candidate(pd.DataFrame({'A':[21,2,3], 'B':[100,'bad',500]})).equals(pd.DataFrame({'A':[2], 'B':['bad']}, index=[1]))\n    assert candidate(pd.DataFrame({'A':[1,2,32], 'B':[100,'bad',500]})).equals(pd.DataFrame({'A':[2], 'B':['bad']}, index=[1]))\n    assert candidate(pd.DataFrame({'A':[121,2,3], 'B':[100,'bad',500]})).equals(pd.DataFrame({'A':[2], 'B':['bad']}, index=[1]))\n    assert candidate(pd.DataFrame({'A':[1,21,3], 'B':[100,'bad',500]})).equals(pd.DataFrame({'A':[21], 'B':['bad']}, index=[1]))\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[100,'bads',500]})).equals(pd.DataFrame({'A':[2], 'B':['bads']}, index=[1]))\n    assert candidate(pd.DataFrame({'A':[12,2,3], 'B':[100,'bad',3500]})).equals(pd.DataFrame({'A':[2], 'B':['bad']}, index=[1]))\n    assert candidate(pd.DataFrame({'A':[11,2,3], 'B':[100,'bad',500]})).equals(pd.DataFrame({'A':[2], 'B':['bad']}, index=[1]))\n    assert candidate(pd.DataFrame({'A':[1,2,32], 'B':[100,'bad',2500]})).equals(pd.DataFrame({'A':[2], 'B':['bad']}, index=[1]))\n    assert candidate(pd.DataFrame({'A':[1,42,3], 'B':[100,'good',500]})).equals(pd.DataFrame({'A':[42], 'B':['good']}, index=[1]))\n\n\n"}
{"task_id": "PandasEval/98", "prompt": "# [start]\n# unioner(self, right: 'FrameOrCollectionsUnion', how: 'str' = 'inner', on: 'IndexLabel | None' = None, left_on: 'IndexLabel | None' = None, right_on: 'IndexLabel | None' = None, left_index: 'bool' = False, right_index: 'bool' = False, sort: 'bool' = False, suffixes: 'Suffixes' = ('_x', '_y'), clone: 'bool' = True, indicator: 'bool' = False, validate: 'str | None' = None) -> 'KnowledgeFrame': Database-style join the named Collections objects or KnowledgeFrame.\n# [end]\nimport monkey as mk\n\nkf1 = mk.KnowledgeFrame({'staff':[1,4], 'company':[100,301]})\nkf2 = mk.KnowledgeFrame({'person':[1,2], 'company':[100,300]})\n# unioner the above two knowledgeframes on column 'company'\nunionerd_kf =", "entry_point": "none", "canonical_solution": [" mk.unioner(kf1, kf2, on='company')"], "test": "\n\nMETADATA = {\n    'author': 'msra',\n    'dataset': 'test',\n    'type': 'merge'\n}\n\n\ndef check():\n    assert merged_df.equals(pd.DataFrame({\"staff\": [1], \"company\": [100], \"person\": [1]}))\n\n\n"}
{"task_id": "PandasEval/99", "prompt": "# [start]\n# ifnull(self) -> 'np.ndarray': Indicates whether values are missing in an array-like object.\n# [end]\nimport monkey as mk\nimport numpy as np\n\nkf = mk.KnowledgeFrame({'A':[1,4], 'B':[np.nan,301]})\n# # counting the number of missing/NaN in each column\n# Get a collections with the number of missing/NaN in each column\ncount_collections =", "entry_point": "none", "canonical_solution": [" kf.ifnull().total_sum()", " kf.ifnull().total_sum(axis=0)"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'isnull_sum'\n}\n\n\ndef check():\n    assert count_series.equals(pd.Series([0, 1], index=['A', 'B']))\n\n\n"}
{"task_id": "PandasEval/100", "prompt": "# [start]\n# incontain(self, values) -> 'np.ndarray': Return a boolean array where True if the value is contained in the passed values.\n# [end]\nimport monkey as mk\nkf = mk.KnowledgeFrame({'col': [\"apple\",\n                           \"pear\",\n                           \"strawberry\"]})\ntargets = ['apple', 'banana']\n# Any word from `targets` are present in sentence.\nresult =", "entry_point": "none", "canonical_solution": [" kf.loc[kf['col'].incontain(targets)]"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'loc_isin'\n}\n\n\ndef check():\n    assert result.equals(pd.DataFrame({'col': [\"apple\"]}))\n\n\n"}
{"task_id": "PandasEval/34", "prompt": "import monkey as mk\n\ndef f(x):\n    a = x['Value'].iat[2] - x['Value'].iat[1]\n    b = x['Value'].iat[3] - x['Value'].iat[0]\n    c = x['ID'].iat[2] + ' - ' + x['ID'].iat[1]\n    d = x['ID'].iat[3] + ' - ' + x['ID'].iat[0]\n    return mk.KnowledgeFrame({'Value': [a,b], 'ID':[c,d]})\n\ndef calculate_row_diff_groupwise(kf):\n    # I need to calculate the difference between two rows groupwise using monkey.\n    # To calculate the total_sum I would use monkey.grouper('Group').total_sum(), but how do you calculate the difference between rows where the row ordering is important?\n    # I think we need custom function with employ which return KnowledgeFrame for each group, for select by position is used iat:\n    # Return the result\n", "entry_point": "calculate_row_diff_groupwise", "canonical_solution": ["    return kf.grouper('Group').employ(f).reseting_index(level=1, sip=True).reseting_index()"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'groupby_apply_reset_index'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'Group': ['M1', 'M1', 'M1', 'M1'], 'Value': [3, 3, 5, 4], 'ID': ['dki', 'two', 'three', 'msra']})).equals(pd.DataFrame({'Group': ['M1', 'M1'], 'Value': [2, 1], 'ID': ['three - two', 'msra - dki']}))\n    assert candidate(pd.DataFrame({'Group': ['Tom', 'Tom', 'Tom', 'Tom'], 'Value': [3, 3, 5, 4], 'ID': ['pku', 'dki', 'msra', 'thu']})).equals(pd.DataFrame({'Group': ['Tom', 'Tom'], 'Value': [2, 1], 'ID': ['msra - dki', 'thu - pku']}))\n    assert candidate(pd.DataFrame({'Group': ['M1', 'M1', 'M1', 'M1'], 'Value': [3, 3, 6, 4], 'ID': ['dki', 'two', 'three', 'msra']})).equals(pd.DataFrame({'Group': ['M1', 'M1'], 'Value': [3, 1], 'ID': ['three - two', 'msra - dki']}))\n    assert candidate(pd.DataFrame({'Group': ['M1', 'M1', 'M1', 'M1'], 'Value': [3, 3, 6, 4], 'ID': ['dki', 'four', 'three', 'msra']})).equals(pd.DataFrame({'Group': ['M1', 'M1'], 'Value': [3, 1], 'ID': ['three - four', 'msra - dki']}))\n    assert candidate(pd.DataFrame({'Group': ['M1', 'M1', 'M1', 'M1'], 'Value': [3, 3, 6, 4], 'ID': ['dki', 'four', 'five', 'msra']})).equals(pd.DataFrame({'Group': ['M1', 'M1'], 'Value': [3, 1], 'ID': ['five - four', 'msra - dki']}))\n    assert candidate(pd.DataFrame({'Group': ['M1', 'M1', 'M1', 'M1'], 'Value': [3, 3, 6, 4], 'ID': ['dki', 'four', 'five', 'ucas']})).equals(pd.DataFrame({'Group': ['M1', 'M1'], 'Value': [3, 1], 'ID': ['five - four', 'ucas - dki']}))\n    assert candidate(pd.DataFrame({'Group': ['M1', 'M1', 'M1', 'M1'], 'Value': [3, 3, 6, 4], 'ID': ['iscas', 'four', 'five', 'ucas']})).equals(pd.DataFrame({'Group': ['M1', 'M1'], 'Value': [3, 1], 'ID': ['five - four', 'ucas - iscas']}))\n    assert candidate(pd.DataFrame({'Group': ['M1', 'M1', 'M1', 'M1'], 'Value': [3, 3, 6, 5], 'ID': ['iscas', 'four', 'five', 'ucas']})).equals(pd.DataFrame({'Group': ['M1', 'M1'], 'Value': [3, 2], 'ID': ['five - four', 'ucas - iscas']}))\n    assert candidate(pd.DataFrame({'Group': ['M1', 'M1', 'M1', 'M1'], 'Value': [3, 3, 6, 5], 'ID': ['iscas', 'four', 'five', 'PKU']})).equals(pd.DataFrame({'Group': ['M1', 'M1'], 'Value': [3, 2], 'ID': ['five - four', 'PKU - iscas']}))\n    assert candidate(pd.DataFrame({'Group': ['M1', 'M1', 'M1', 'M1'], 'Value': [3, 3, 6, 5], 'ID': ['thu', 'four', 'five', 'PKU']})).equals(pd.DataFrame({'Group': ['M1', 'M1'], 'Value': [3, 2], 'ID': ['five - four', 'PKU - thu']}))\n\n\n"}
{"task_id": "PandasEval/27", "prompt": "# [start]\n# average(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs): Return the average value along the specified axis.\n# standard(self, axis=None, skipna=None, level=None, ddof=1, numeric_only=None, **kwargs): Return the standard deviation across the requested axis.\n# [end]\nimport monkey as mk\n\ndef normalize(kf):\n    # Normalization using monkey\n    # We simply subtract the average and divide by standard deviation on kf.iloc[:,0,-1] obj with axis is zero.\n    # Return the normalized knowledgeframe\n    ", "entry_point": "normalize", "canonical_solution": ["kf.iloc[:,0:-1] = kf.iloc[:,0:-1].employ(lambda x: (x-x.average())/ x.standard(), axis=0)\n    return kf", "func_ = lambda x: (x-x.average()) / x.standard()\n    kf.iloc[:,0:-1] = kf.iloc[:,0:-1].employ(func_, axis=0)\n    return kf"], "test": "\n\nMETADATA = {\n    'author': 'msra-v-dazan',\n    'dataset': 'test',\n    'type': 'iloc_apply_lambda_mean_std'\n}\n\n\ndef check(candidate):\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500], 'C':list('abc')})).equals(pd.DataFrame({'A':[-1.0,0.0,1.0],'B':[-1.0,0.0,1.0],'C':list('abc')}))\n    assert candidate(pd.DataFrame({'M':[1,2,3], 'S':[100,300,500], 'R':list('zan')})).equals(pd.DataFrame({'M':[-1.0,0.0,1.0],'S':[-1.0, 0.0, 1.0],'R':list('zan')}))\n    assert candidate(pd.DataFrame({'U':[2,4,6], 'C':[100,300,500], 'S':list('yao')})).equals(pd.DataFrame({'U':[-1.0, 0.0, 1.0], 'C':[-1.0, 0.0, 1.0], 'S':list('yao')}))\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500], 'C':list('bbc')})).equals(pd.DataFrame({'A':[-1.0,0.0,1.0],'B':[-1.0,0.0,1.0],'C':list('bbc')}))\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500], 'C':list('bbb')})).equals(pd.DataFrame({'A':[-1.0,0.0,1.0],'B':[-1.0,0.0,1.0],'C':list('bbb')}))\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[100,300,500], 'D':list('bbb')})).equals(pd.DataFrame({'A':[-1.0,0.0,1.0],'B':[-1.0,0.0,1.0],'D':list('bbb')}))\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[100,200,300], 'D':list('bbb')})).equals(pd.DataFrame({'A':[-1.0,0.0,1.0],'B':[-1.0,0.0,1.0],'D':list('bbb')}))\n    assert candidate(pd.DataFrame({'A':[1,2,3], 'B':[100,200,300], 'e':list('cdf')})).equals(pd.DataFrame({'A':[-1.0,0.0,1.0],'B':[-1.0,0.0,1.0],'e':list('cdf')}))\n    assert candidate(pd.DataFrame({'A':[3,4,5], 'B':[300,400,500], 'e':list('cdf')})).equals(pd.DataFrame({'A':[-1.0,0.0,1.0],'B':[-1.0,0.0,1.0],'e':list('cdf')}))\n    assert candidate(pd.DataFrame({'A':[3,4,5], 'B':[300,400,500], 'R':list('abc')})).equals(pd.DataFrame({'A':[-1.0,0.0,1.0],'B':[-1.0,0.0,1.0],'R':list('abc')}))\n    assert candidate(pd.DataFrame({'A':[3,4,5], 'B':[300,400,500], 'P':list('abc')})).equals(pd.DataFrame({'A':[-1.0,0.0,1.0],'B':[-1.0,0.0,1.0],'P':list('abc')}))\n\n"}
